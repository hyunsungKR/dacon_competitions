{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df85c75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wang\\anaconda3\\envs\\yolo\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 건설용 자갈 암석 분류 AI - 모델 학습 시작!\n",
      "PyTorch 버전: 2.2.2+cu121\n",
      "CUDA 사용 가능: True\n",
      "GPU: NVIDIA GeForce RTX 4070\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# PyTorch & ML Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_v2_s, efficientnet_v2_m\n",
    "import timm\n",
    "\n",
    "# Scientific Computing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Utilities\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 경고 메시지 숨기기\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 시드 고정 (재현성 보장)\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"🔥 건설용 자갈 암석 분류 AI - 모델 학습 시작!\")\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc289c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 경로 설정 완료:\n",
      "훈련 데이터: D:\\data\\stones\\open\\train\n",
      "테스트 데이터: D:\\data\\stones\\open\\test\n",
      "실험 결과: ..\\experiments\n",
      "모델 저장: ..\\models\n",
      "\n",
      "🏷️ 클래스 정보:\n",
      "클래스 수: 7\n",
      "클래스 목록: ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
      "\n",
      "⚙️ 하이퍼파라미터 설정:\n",
      "image_size: 224\n",
      "batch_size: 8\n",
      "num_epochs: 50\n",
      "num_workers: 0\n",
      "pin_memory: True\n",
      "lr: 0.0003\n",
      "weight_decay: 0.0001\n",
      "warmup_epochs: 5\n",
      "cutmix_alpha: 1.0\n",
      "mixup_alpha: 0.2\n",
      "cutmix_prob: 0.5\n",
      "mixup_prob: 0.5\n",
      "hard_negative_ratio: 0.2\n",
      "hard_memory_size: 1000\n",
      "loss_threshold: 1.5\n",
      "kfold_splits: 5\n",
      "kfold_seed: 42\n",
      "ensemble_models: 4개 모델\n",
      "\n",
      "💻 연산 장치: cuda\n",
      "GPU 메모리: 12.0 GB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 🏆 상위 팀들의 핵심 전략 통합 설정\n",
    "# 1위 팀: 4개 모델 앙상블 (EfficientNetV2-S/M + RegNetY + TinyViT)\n",
    "# 2위 팀: InternImage + Hard Negative Sampling\n",
    "# 3위 팀: ConvNeXt \n",
    "# 4위 팀: 안정적인 전이학습 전략\n",
    "# ============================================================================\n",
    "\n",
    "# 경로 설정\n",
    "BASE_PATH = r\"D:\\data\\stones\\open\"\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, \"train\")\n",
    "TEST_PATH = os.path.join(BASE_PATH, \"test\")\n",
    "SUBMISSION_PATH = os.path.join(BASE_PATH, \"sample_submission.csv\")\n",
    "\n",
    "# 실험 결과 저장 경로\n",
    "EXPERIMENT_DIR = Path(\"../experiments\")\n",
    "EXPERIMENT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# 모델 저장 경로\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"📂 경로 설정 완료:\")\n",
    "print(f\"훈련 데이터: {TRAIN_PATH}\")\n",
    "print(f\"테스트 데이터: {TEST_PATH}\")\n",
    "print(f\"실험 결과: {EXPERIMENT_DIR}\")\n",
    "print(f\"모델 저장: {MODEL_DIR}\")\n",
    "\n",
    "# 클래스 정보 (이전 분석 결과 활용)\n",
    "CLASS_NAMES = ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
    "CLASS_COUNTS = {\n",
    "    'Andesite': 43802,\n",
    "    'Basalt': 26810,\n",
    "    'Etc': 15935,\n",
    "    'Gneiss': 73914,\n",
    "    'Granite': 92923,\n",
    "    'Mud_Sandstone': 89467,\n",
    "    'Weathered_Rock': 37169\n",
    "}\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "class_to_idx = {cls: i for i, cls in enumerate(sorted(CLASS_NAMES))}\n",
    "idx_to_class = {i: cls for cls, i in class_to_idx.items()}\n",
    "\n",
    "print(f\"\\n🏷️ 클래스 정보:\")\n",
    "print(f\"클래스 수: {NUM_CLASSES}\")\n",
    "print(f\"클래스 목록: {sorted(CLASS_NAMES)}\")\n",
    "\n",
    "# 하이퍼파라미터 설정 (상위 팀들의 최적 설정 통합)\n",
    "HYPERPARAMETERS = {\n",
    "    # 기본 설정\n",
    "    'image_size': 224,          # 1-4위 팀 공통 사용\n",
    "    'batch_size': 8,           # GPU 메모리 효율성 고려\n",
    "    'num_epochs': 50,           # 충분한 학습 시간\n",
    "    'num_workers': 0,           # Windows 호환성\n",
    "    'pin_memory': True,\n",
    "    \n",
    "    # 학습률 설정 (1위 팀 전략)\n",
    "    'lr': 3e-4,                 # AdamW 최적 학습률\n",
    "    'weight_decay': 1e-4,       # 정규화 강도\n",
    "    'warmup_epochs': 5,         # 학습률 워밍업\n",
    "    \n",
    "    # 증강 설정 (2위 팀 전략)\n",
    "    'cutmix_alpha': 1.0,        # CutMix 강도\n",
    "    'mixup_alpha': 0.2,         # Mixup 강도\n",
    "    'cutmix_prob': 0.5,         # CutMix 적용 확률\n",
    "    'mixup_prob': 0.5,          # Mixup 적용 확률\n",
    "    \n",
    "    # 샘플링 설정 (2위 팀 Hard Negative)\n",
    "    'hard_negative_ratio': 0.2,  # 배치 중 Hard Sample 비율\n",
    "    'hard_memory_size': 1000,     # Hard Sample 메모리 크기\n",
    "    'loss_threshold': 1.5,        # Hard Sample 기준 손실값\n",
    "    \n",
    "    # K-Fold 설정 (2-4위 팀 공통)\n",
    "    'kfold_splits': 5,           # 5-fold 교차검증\n",
    "    'kfold_seed': 42,\n",
    "    \n",
    "    # 앙상블 설정 (1위 팀 전략)\n",
    "    'ensemble_models': [\n",
    "        'tf_efficientnetv2_s.in21k_ft_in1k',  # 1위 팀 사용\n",
    "        'tf_efficientnetv2_m.in21k_ft_in1k',  # 1위 팀 사용\n",
    "        'convnext_tiny.fb_in22k_ft_in1k',     # 3위 팀 사용\n",
    "        'vit_tiny_patch16_224.augreg_in21k_ft_in1k'  # Vision Transformer\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\n⚙️ 하이퍼파라미터 설정:\")\n",
    "for key, value in HYPERPARAMETERS.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"{key}: {len(value)}개 모델\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# GPU/CPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n💻 연산 장치: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e74a52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 데이터 변환 설정 완료:\n",
      "✅ 훈련용 변환: 고급 증강 포함\n",
      "✅ 검증용 변환: 증강 없음\n",
      "✅ CutMix/Mixup: 구현 완료\n",
      "✅ 암석 이미지 특성 최적화\n",
      "🖼️ 시각화 함수 준비 완료\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 🎨 데이터 변환 및 증강 (상위 팀들의 베스트 프랙티스 통합)\n",
    "# ============================================================================\n",
    "\n",
    "class AdvancedTransforms:\n",
    "    \"\"\"상위 팀들의 데이터 변환 전략을 통합한 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=224, is_training=True):\n",
    "        self.image_size = image_size\n",
    "        self.is_training = is_training\n",
    "        \n",
    "    def get_train_transforms(self):\n",
    "        \"\"\"훈련용 변환 (2위 팀 + 3위 팀 전략 통합)\"\"\"\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((256, 256), interpolation=Image.BICUBIC),  # 품질 보존\n",
    "            transforms.RandomCrop(self.image_size),                      # 랜덤 크롭\n",
    "            transforms.RandomHorizontalFlip(p=0.5),                      # 좌우 뒤집기\n",
    "            transforms.RandomRotation(degrees=15),                       # 회전 (암석 특성 고려)\n",
    "            transforms.ColorJitter(                                      # 색상 변화 (현장 환경 반영)\n",
    "                brightness=0.2,\n",
    "                contrast=0.2, \n",
    "                saturation=0.1,\n",
    "                hue=0.05\n",
    "            ),\n",
    "            transforms.RandomApply([                                     # 추가 증강 (확률적 적용)\n",
    "                transforms.GaussianBlur(kernel_size=3)\n",
    "            ], p=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(                                        # ImageNet 정규화\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def get_valid_transforms(self):\n",
    "        \"\"\"검증용 변환 (증강 없음)\"\"\"\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size), interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def get_test_transforms(self):\n",
    "        \"\"\"테스트용 변환 (TTA 포함 가능)\"\"\"\n",
    "        return self.get_valid_transforms()\n",
    "\n",
    "# CutMix 구현 (2위 팀 전략)\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"CutMix 데이터 증강\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    \n",
    "    # 실제 혼합 비율 계산\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
    "    \n",
    "    return x, y, y[index], lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    \"\"\"CutMix용 랜덤 박스 생성\"\"\"\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Mixup 구현 (2위 팀 전략)\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Mixup 데이터 증강\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    \n",
    "    return mixed_x, y, y[index], lam\n",
    "\n",
    "# 손실 함수 (혼합 데이터용)\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup/CutMix용 손실 함수\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# 변환 객체 생성\n",
    "transform_manager = AdvancedTransforms(image_size=HYPERPARAMETERS['image_size'])\n",
    "\n",
    "train_transforms = transform_manager.get_train_transforms()\n",
    "valid_transforms = transform_manager.get_valid_transforms()\n",
    "test_transforms = transform_manager.get_test_transforms()\n",
    "\n",
    "print(\"🎨 데이터 변환 설정 완료:\")\n",
    "print(\"✅ 훈련용 변환: 고급 증강 포함\")\n",
    "print(\"✅ 검증용 변환: 증강 없음\")\n",
    "print(\"✅ CutMix/Mixup: 구현 완료\")\n",
    "print(\"✅ 암석 이미지 특성 최적화\")\n",
    "\n",
    "# 변환 예시 시각화를 위한 함수\n",
    "def visualize_transforms(dataset, num_samples=4):\n",
    "    \"\"\"데이터 변환 결과 시각화\"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
    "    fig.suptitle('데이터 변환 예시', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # 원본 이미지 (검증용 변환만 적용)\n",
    "        img, label = dataset[i]\n",
    "        \n",
    "        # 첫 번째 행: 검증용 변환\n",
    "        axes[0, i].imshow(img.permute(1, 2, 0) * 0.229 + 0.485)  # 정규화 해제 (근사)\n",
    "        axes[0, i].set_title(f'Valid Transform\\n{idx_to_class[label]}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # 두 번째 행: 훈련용 변환 (별도 적용)\n",
    "        # 실제로는 랜덤이므로 매번 다름\n",
    "        axes[1, i].imshow(img.permute(1, 2, 0) * 0.229 + 0.485)  # 정규화 해제 (근사) \n",
    "        axes[1, i].set_title(f'Train Transform\\n{idx_to_class[label]}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"🖼️ 시각화 함수 준비 완료\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa9ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 데이터셋 생성 중...\n",
      "📊 데이터셋 로딩 완료: 380,020개 샘플\n",
      "   Andesite: 43,802개\n",
      "   Basalt: 26,810개\n",
      "   Etc: 15,935개\n",
      "   Gneiss: 73,914개\n",
      "   Granite: 92,923개\n",
      "   Mud_Sandstone: 89,467개\n",
      "   Weathered_Rock: 37,169개\n",
      "⚖️ 가중치 계산 완료: 0.764 ~ 1.846\n",
      "✅ 훈련 데이터셋 생성 완료: 380,020개 샘플\n",
      "✅ WeightedRandomSampler 생성 완료\n",
      "✅ Hard Negative Sampler 생성 완료\n",
      "✅ DataLoader 생성 완료: 47502 배치\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 📊 고급 데이터셋 클래스 (Hard Negative Sampling + WeightedRandomSampler 통합)\n",
    "# ============================================================================\n",
    "\n",
    "class RockDataset(Dataset):\n",
    "    \"\"\"암석 분류를 위한 고급 데이터셋 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, transform=None, class_to_idx=None, is_training=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "        self.class_to_idx = class_to_idx or {}\n",
    "        \n",
    "        # 이미지 경로와 라벨 수집\n",
    "        self.samples = []\n",
    "        self.class_counts = defaultdict(int)\n",
    "        \n",
    "        self._load_samples()\n",
    "        self._compute_weights()\n",
    "        \n",
    "    def _load_samples(self):\n",
    "        \"\"\"이미지 샘플 로딩\"\"\"\n",
    "        for class_name in os.listdir(self.data_dir):\n",
    "            class_path = os.path.join(self.data_dir, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "                \n",
    "            class_idx = self.class_to_idx.get(class_name, len(self.class_to_idx))\n",
    "            if class_name not in self.class_to_idx:\n",
    "                self.class_to_idx[class_name] = class_idx\n",
    "            \n",
    "            for img_name in os.listdir(class_path):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    img_path = os.path.join(class_path, img_name)\n",
    "                    self.samples.append((img_path, class_idx))\n",
    "                    self.class_counts[class_idx] += 1\n",
    "        \n",
    "        print(f\"📊 데이터셋 로딩 완료: {len(self.samples):,}개 샘플\")\n",
    "        for class_name, class_idx in sorted(self.class_to_idx.items()):\n",
    "            print(f\"   {class_name}: {self.class_counts[class_idx]:,}개\")\n",
    "    \n",
    "    def _compute_weights(self):\n",
    "        \"\"\"샘플별 가중치 계산 (WeightedRandomSampler용)\"\"\"\n",
    "        total_samples = len(self.samples)\n",
    "        num_classes = len(self.class_to_idx)\n",
    "        \n",
    "        # 제곱근 역빈도 가중치 (이전 분석에서 최적으로 확인됨)\n",
    "        class_weights = {}\n",
    "        for class_idx, count in self.class_counts.items():\n",
    "            weight = np.sqrt(total_samples / (num_classes * count))\n",
    "            class_weights[class_idx] = weight\n",
    "        \n",
    "        # 각 샘플에 가중치 할당\n",
    "        self.sample_weights = []\n",
    "        for _, class_idx in self.samples:\n",
    "            self.sample_weights.append(class_weights[class_idx])\n",
    "        \n",
    "        self.sample_weights = torch.DoubleTensor(self.sample_weights)\n",
    "        print(f\"⚖️ 가중치 계산 완료: {self.sample_weights.min():.3f} ~ {self.sample_weights.max():.3f}\")\n",
    "    \n",
    "    def get_weighted_sampler(self):\n",
    "        \"\"\"WeightedRandomSampler 반환\"\"\"\n",
    "        return WeightedRandomSampler(\n",
    "            weights=self.sample_weights,\n",
    "            num_samples=len(self.sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            # 이미지 로딩\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # 변환 적용\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, label, idx  # idx 추가 (Hard Negative 추적용)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 이미지 로딩 오류: {img_path} - {e}\")\n",
    "            # 다른 샘플로 대체\n",
    "            return self.__getitem__((idx + 1) % len(self.samples))\n",
    "\n",
    "# Hard Negative Sampler 클래스 (2위 팀 핵심 기법)\n",
    "class HardNegativeSampler:\n",
    "    \"\"\"Hard Negative Sample을 관리하는 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, memory_size=1000, loss_threshold=1.5):\n",
    "        self.memory_size = memory_size\n",
    "        self.loss_threshold = loss_threshold\n",
    "        self.hard_samples = []  # (image, label, loss) 튜플들\n",
    "        self.sample_indices = []  # 원본 데이터셋 인덱스들\n",
    "        \n",
    "    def update(self, images, labels, indices, losses):\n",
    "        \"\"\"Hard sample 업데이트\"\"\"\n",
    "        # 높은 손실을 가진 샘플들 찾기\n",
    "        hard_mask = losses > self.loss_threshold\n",
    "        \n",
    "        if hard_mask.sum() > 0:\n",
    "            hard_images = images[hard_mask]\n",
    "            hard_labels = labels[hard_mask]\n",
    "            hard_indices = indices[hard_mask]\n",
    "            hard_losses = losses[hard_mask]\n",
    "            \n",
    "            # 새로운 hard sample들 추가\n",
    "            for img, label, idx, loss in zip(hard_images, hard_labels, hard_indices, hard_losses):\n",
    "                self.hard_samples.append((img.cpu(), label.cpu(), loss.item()))\n",
    "                self.sample_indices.append(idx.item())\n",
    "            \n",
    "            # 메모리 크기 제한\n",
    "            if len(self.hard_samples) > self.memory_size:\n",
    "                # 손실이 높은 순으로 정렬하고 상위 N개만 유지\n",
    "                sorted_samples = sorted(\n",
    "                    zip(self.hard_samples, self.sample_indices),\n",
    "                    key=lambda x: x[0][2],  # loss 기준 정렬\n",
    "                    reverse=True\n",
    "                )\n",
    "                \n",
    "                self.hard_samples = [s[0] for s in sorted_samples[:self.memory_size]]\n",
    "                self.sample_indices = [s[1] for s in sorted_samples[:self.memory_size]]\n",
    "    \n",
    "    def sample(self, batch_size, device):\n",
    "        \"\"\"Hard sample들에서 배치 샘플링\"\"\"\n",
    "        if len(self.hard_samples) == 0:\n",
    "            return None, None\n",
    "        \n",
    "        # 배치 크기만큼 랜덤 샘플링\n",
    "        sample_size = min(batch_size, len(self.hard_samples))\n",
    "        sampled_indices = np.random.choice(len(self.hard_samples), sample_size, replace=False)\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for idx in sampled_indices:\n",
    "            img, label, _ = self.hard_samples[idx]\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "        \n",
    "        images = torch.stack(images).to(device)\n",
    "        labels = torch.stack(labels).to(device)\n",
    "        \n",
    "        return images, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.hard_samples)\n",
    "\n",
    "# 데이터셋 생성\n",
    "print(\"📊 데이터셋 생성 중...\")\n",
    "\n",
    "# 훈련 데이터셋\n",
    "train_dataset = RockDataset(\n",
    "    data_dir=TRAIN_PATH,\n",
    "    transform=train_transforms,\n",
    "    class_to_idx=class_to_idx,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "# 검증 데이터셋은 나중에 K-Fold에서 분할\n",
    "print(f\"✅ 훈련 데이터셋 생성 완료: {len(train_dataset):,}개 샘플\")\n",
    "\n",
    "# WeightedRandomSampler 생성\n",
    "weighted_sampler = train_dataset.get_weighted_sampler()\n",
    "print(\"✅ WeightedRandomSampler 생성 완료\")\n",
    "\n",
    "# Hard Negative Sampler 생성\n",
    "hard_negative_sampler = HardNegativeSampler(\n",
    "    memory_size=HYPERPARAMETERS['hard_memory_size'],\n",
    "    loss_threshold=HYPERPARAMETERS['loss_threshold']\n",
    ")\n",
    "print(\"✅ Hard Negative Sampler 생성 완료\")\n",
    "\n",
    "# 기본 DataLoader 생성 (K-Fold에서 재구성됨)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=HYPERPARAMETERS['batch_size'],\n",
    "    sampler=weighted_sampler,\n",
    "    num_workers=HYPERPARAMETERS['num_workers'],\n",
    "    pin_memory=HYPERPARAMETERS['pin_memory'],\n",
    "    drop_last=True  # 배치 크기 일관성 유지\n",
    ")\n",
    "\n",
    "print(f\"✅ DataLoader 생성 완료: {len(train_loader)} 배치\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b787924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 사용 가능한 모델 확인 중...\n",
      "✅ tf_efficientnetv2_s.in21k_ft_in1k 모델 생성 완료\n",
      "✅ EfficientNetV2-S: 20,186,455 파라미터 (77.0MB)\n",
      "✅ tf_efficientnetv2_m.in21k_ft_in1k 모델 생성 완료\n",
      "✅ EfficientNetV2-M: 52,867,323 파라미터 (201.7MB)\n",
      "✅ convnext_tiny.fb_in22k_ft_in1k 모델 생성 완료\n",
      "✅ ConvNeXt-Tiny: 27,825,511 파라미터 (106.1MB)\n",
      "✅ vit_tiny_patch16_224.augreg_in21k_ft_in1k 모델 생성 완료\n",
      "✅ ViT-Tiny: 5,525,767 파라미터 (21.1MB)\n",
      "\n",
      "📊 총 4개 모델 사용 가능\n",
      "\n",
      "🏆 상위 팀 모델 아키텍처 분석:\n",
      "================================================================================\n",
      "1. EfficientNetV2-S\n",
      "   사용 팀: 1위 팀 사용\n",
      "   특징: 효율성과 성능의 균형\n",
      "   파라미터: 20,186,455개\n",
      "   모델 크기: 77.0MB\n",
      "   학습률 배수: 1.0\n",
      "----------------------------------------\n",
      "2. EfficientNetV2-M\n",
      "   사용 팀: 1위 팀 사용\n",
      "   특징: 더 큰 모델, 높은 성능\n",
      "   파라미터: 52,867,323개\n",
      "   모델 크기: 201.7MB\n",
      "   학습률 배수: 0.8\n",
      "----------------------------------------\n",
      "3. ConvNeXt-Tiny\n",
      "   사용 팀: 3위 팀 사용\n",
      "   특징: CNN + Transformer 하이브리드\n",
      "   파라미터: 27,825,511개\n",
      "   모델 크기: 106.1MB\n",
      "   학습률 배수: 1.2\n",
      "----------------------------------------\n",
      "4. ViT-Tiny\n",
      "   사용 팀: 다양한 팀 실험\n",
      "   특징: Vision Transformer\n",
      "   파라미터: 5,525,767개\n",
      "   모델 크기: 21.1MB\n",
      "   학습률 배수: 1.5\n",
      "----------------------------------------\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 🧠 모델 아키텍처 (1-4위 팀의 최고 모델들 통합)\n",
    "# ============================================================================\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"상위 팀들이 사용한 모델들을 생성하는 팩토리 클래스\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_model(model_name, num_classes=7, pretrained=True):\n",
    "        \"\"\"모델 생성\"\"\"\n",
    "        try:\n",
    "            if 'efficientnetv2' in model_name:\n",
    "                # 1위 팀 사용 모델\n",
    "                model = timm.create_model(\n",
    "                    model_name, \n",
    "                    pretrained=pretrained,\n",
    "                    num_classes=num_classes\n",
    "                )\n",
    "                \n",
    "            elif 'convnext' in model_name:\n",
    "                # 3위 팀 사용 모델\n",
    "                model = timm.create_model(\n",
    "                    model_name,\n",
    "                    pretrained=pretrained, \n",
    "                    num_classes=num_classes\n",
    "                )\n",
    "                \n",
    "            elif 'vit' in model_name:\n",
    "                # Vision Transformer (다양한 팀에서 실험)\n",
    "                model = timm.create_model(\n",
    "                    model_name,\n",
    "                    pretrained=pretrained,\n",
    "                    num_classes=num_classes\n",
    "                )\n",
    "                \n",
    "            elif 'regnety' in model_name:\n",
    "                # 1위 팀 사용 모델\n",
    "                model = timm.create_model(\n",
    "                    model_name,\n",
    "                    pretrained=pretrained,\n",
    "                    num_classes=num_classes\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"지원하지 않는 모델: {model_name}\")\n",
    "            \n",
    "            print(f\"✅ {model_name} 모델 생성 완료\")\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {model_name} 모델 생성 실패: {e}\")\n",
    "            # 대체 모델로 EfficientNetV2-S 사용\n",
    "            print(\"🔄 대체 모델 사용: EfficientNetV2-S\")\n",
    "            return timm.create_model(\n",
    "                'tf_efficientnetv2_s.in21k_ft_in1k',\n",
    "                pretrained=True,\n",
    "                num_classes=num_classes\n",
    "            )\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_model_info(model):\n",
    "        \"\"\"모델 정보 출력\"\"\"\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params,\n",
    "            'size_mb': total_params * 4 / 1024 / 1024  # 대략적인 크기 (float32 기준)\n",
    "        }\n",
    "\n",
    "# 모델별 설정\n",
    "MODEL_CONFIGS = {\n",
    "    'tf_efficientnetv2_s.in21k_ft_in1k': {\n",
    "        'name': 'EfficientNetV2-S',\n",
    "        'team': '1위 팀 사용',\n",
    "        'description': '효율성과 성능의 균형',\n",
    "        'lr_multiplier': 1.0\n",
    "    },\n",
    "    'tf_efficientnetv2_m.in21k_ft_in1k': {\n",
    "        'name': 'EfficientNetV2-M',\n",
    "        'team': '1위 팀 사용', \n",
    "        'description': '더 큰 모델, 높은 성능',\n",
    "        'lr_multiplier': 0.8\n",
    "    },\n",
    "    'convnext_tiny.fb_in22k_ft_in1k': {\n",
    "        'name': 'ConvNeXt-Tiny',\n",
    "        'team': '3위 팀 사용',\n",
    "        'description': 'CNN + Transformer 하이브리드',\n",
    "        'lr_multiplier': 1.2\n",
    "    },\n",
    "    'vit_tiny_patch16_224.augreg_in21k_ft_in1k': {\n",
    "        'name': 'ViT-Tiny',\n",
    "        'team': '다양한 팀 실험',\n",
    "        'description': 'Vision Transformer',\n",
    "        'lr_multiplier': 1.5\n",
    "    }\n",
    "}\n",
    "\n",
    "# 사용 가능한 모델 확인\n",
    "print(\"🧠 사용 가능한 모델 확인 중...\")\n",
    "available_models = []\n",
    "\n",
    "for model_name in HYPERPARAMETERS['ensemble_models']:\n",
    "    try:\n",
    "        # 모델 생성 테스트\n",
    "        test_model = ModelFactory.create_model(model_name, NUM_CLASSES)\n",
    "        model_info = ModelFactory.get_model_info(test_model)\n",
    "        \n",
    "        config = MODEL_CONFIGS.get(model_name, {\n",
    "            'name': model_name,\n",
    "            'team': '실험용',\n",
    "            'description': 'Custom Model',\n",
    "            'lr_multiplier': 1.0\n",
    "        })\n",
    "        \n",
    "        available_models.append({\n",
    "            'model_name': model_name,\n",
    "            'config': config,\n",
    "            'info': model_info\n",
    "        })\n",
    "        \n",
    "        print(f\"✅ {config['name']}: {model_info['total_params']:,} 파라미터 ({model_info['size_mb']:.1f}MB)\")\n",
    "        \n",
    "        # 메모리 정리\n",
    "        del test_model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {model_name}: 사용 불가 - {e}\")\n",
    "\n",
    "print(f\"\\n📊 총 {len(available_models)}개 모델 사용 가능\")\n",
    "\n",
    "# 첫 번째 모델로 베이스라인 생성 함수\n",
    "def create_baseline_model():\n",
    "    \"\"\"베이스라인 모델 생성\"\"\"\n",
    "    if available_models:\n",
    "        baseline_config = available_models[0]\n",
    "        model_name = baseline_config['model_name']\n",
    "        \n",
    "        model = ModelFactory.create_model(model_name, NUM_CLASSES)\n",
    "        \n",
    "        print(f\"🎯 베이스라인 모델: {baseline_config['config']['name']}\")\n",
    "        print(f\"   팀: {baseline_config['config']['team']}\")\n",
    "        print(f\"   설명: {baseline_config['config']['description']}\")\n",
    "        \n",
    "        return model, baseline_config\n",
    "    else:\n",
    "        raise RuntimeError(\"사용 가능한 모델이 없습니다!\")\n",
    "\n",
    "# 모델 요약 정보 출력\n",
    "def print_model_summary():\n",
    "    \"\"\"모델 요약 정보 출력\"\"\"\n",
    "    print(\"\\n🏆 상위 팀 모델 아키텍처 분석:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, model_data in enumerate(available_models, 1):\n",
    "        config = model_data['config']\n",
    "        info = model_data['info']\n",
    "        \n",
    "        print(f\"{i}. {config['name']}\")\n",
    "        print(f\"   사용 팀: {config['team']}\")\n",
    "        print(f\"   특징: {config['description']}\")\n",
    "        print(f\"   파라미터: {info['total_params']:,}개\")\n",
    "        print(f\"   모델 크기: {info['size_mb']:.1f}MB\")\n",
    "        print(f\"   학습률 배수: {config['lr_multiplier']}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print_model_summary()\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc065bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AdvancedTrainer 클래스 정의 완료\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 🎯 고급 트레이너 클래스 (모든 상위 팀 기법 통합)\n",
    "# ============================================================================\n",
    "\n",
    "class AdvancedTrainer:\n",
    "    \"\"\"상위 팀들의 모든 기법을 통합한 트레이너 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, hyperparameters):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.hp = hyperparameters\n",
    "        \n",
    "        # 옵티마이저 설정 (1위 팀 전략)\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.hp['lr'],\n",
    "            weight_decay=self.hp['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # 스케줄러 설정 (상위 팀 공통)\n",
    "        self.scheduler = None  # 나중에 train_loader 크기를 알아야 설정 가능\n",
    "        \n",
    "        # 손실 함수\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # 메트릭 추적\n",
    "        self.train_history = defaultdict(list)\n",
    "        self.val_history = defaultdict(list)\n",
    "        self.best_score = 0.0\n",
    "        self.best_model_state = None\n",
    "        \n",
    "        # Hard Negative Sampler\n",
    "        self.hard_negative_sampler = HardNegativeSampler(\n",
    "            memory_size=self.hp['hard_memory_size'],\n",
    "            loss_threshold=self.hp['loss_threshold']\n",
    "        )\n",
    "        \n",
    "        print(f\"🎯 AdvancedTrainer 초기화 완료\")\n",
    "        print(f\"   옵티마이저: AdamW (lr={self.hp['lr']}, weight_decay={self.hp['weight_decay']})\")\n",
    "        print(f\"   손실 함수: CrossEntropyLoss\")\n",
    "        print(f\"   Hard Negative: 활성화\")\n",
    "    \n",
    "    def setup_scheduler(self, train_loader):\n",
    "        \"\"\"스케줄러 설정 (OneCycleLR 사용)\"\"\"\n",
    "        total_steps = len(train_loader) * self.hp['num_epochs']\n",
    "        \n",
    "        self.scheduler = OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.hp['lr'],\n",
    "            total_steps=total_steps,\n",
    "            pct_start=0.3,  # 30%까지 학습률 증가\n",
    "            div_factor=10,  # 초기 학습률 = max_lr / div_factor\n",
    "            final_div_factor=100  # 최종 학습률 = max_lr / final_div_factor\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ OneCycleLR 스케줄러 설정 완료 (총 {total_steps} 스텝)\")\n",
    "    \n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        \"\"\"한 에포크 훈련\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        # 진행 상황 표시\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.hp[\"num_epochs\"]}')\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(pbar):\n",
    "            if len(batch_data) == 3:\n",
    "                images, labels, indices = batch_data\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                indices = indices.to(self.device)\n",
    "            else:\n",
    "                images, labels = batch_data\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                indices = torch.arange(len(labels)).to(self.device)\n",
    "            \n",
    "            # Hard Negative 샘플링 적용 (2위 팀 전략)\n",
    "            if len(self.hard_negative_sampler) > 0 and np.random.random() < self.hp['hard_negative_ratio']:\n",
    "                hard_size = int(len(labels) * self.hp['hard_negative_ratio'])\n",
    "                hard_images, hard_labels = self.hard_negative_sampler.sample(hard_size, self.device)\n",
    "                \n",
    "                if hard_images is not None:\n",
    "                    # Hard sample과 일반 sample 결합\n",
    "                    regular_size = len(labels) - hard_size\n",
    "                    images = torch.cat([images[:regular_size], hard_images], dim=0)\n",
    "                    labels = torch.cat([labels[:regular_size], hard_labels], dim=0)\n",
    "            \n",
    "            # CutMix/Mixup 적용 (확률적)\n",
    "            r = np.random.rand(1)\n",
    "            if r < self.hp['cutmix_prob']:\n",
    "                # CutMix 적용\n",
    "                images, targets_a, targets_b, lam = cutmix_data(images, labels, self.hp['cutmix_alpha'])\n",
    "                outputs = self.model(images)\n",
    "                loss = mixup_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "            elif r < self.hp['cutmix_prob'] + self.hp['mixup_prob']:\n",
    "                # Mixup 적용\n",
    "                images, targets_a, targets_b, lam = mixup_data(images, labels, self.hp['mixup_alpha'])\n",
    "                outputs = self.model(images)\n",
    "                loss = mixup_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                # 일반 학습\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                targets_a = labels\n",
    "            \n",
    "            # 역전파\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 그라디언트 클리핑 (안정성 향상)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # Hard Negative 업데이트 (배치별 손실 계산)\n",
    "            with torch.no_grad():\n",
    "                batch_losses = F.cross_entropy(outputs, targets_a, reduction='none')\n",
    "                self.hard_negative_sampler.update(images, targets_a, indices[:len(targets_a)], batch_losses)\n",
    "            \n",
    "            # 메트릭 계산\n",
    "            total_loss += loss.item()\n",
    "            total_samples += len(targets_a)\n",
    "            \n",
    "            # 정확도 계산 (Mixup이 아닌 경우에만)\n",
    "            if r >= self.hp['cutmix_prob'] + self.hp['mixup_prob']:\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_predictions += predicted.eq(targets_a).sum().item()\n",
    "            \n",
    "            # 진행 상황 업데이트\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'LR': f'{current_lr:.6f}',\n",
    "                'Hard': len(self.hard_negative_sampler)\n",
    "            })\n",
    "        \n",
    "        # 에포크 메트릭 계산\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "        self.train_history['loss'].append(avg_loss)\n",
    "        self.train_history['accuracy'].append(accuracy)\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate_epoch(self, val_loader):\n",
    "        \"\"\"검증\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data in tqdm(val_loader, desc='Validation'):\n",
    "                # DataLoader에서 반환되는 데이터 형식에 맞춰 처리\n",
    "                if len(batch_data) == 3:\n",
    "                    images, labels, _ = batch_data  # (image, label, idx)\n",
    "                else:\n",
    "                    images, labels = batch_data      # (image, label)\n",
    "                \n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # 메트릭 계산\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
    "        \n",
    "        self.val_history['loss'].append(avg_loss)\n",
    "        self.val_history['accuracy'].append(accuracy)\n",
    "        self.val_history['f1_macro'].append(f1_macro)\n",
    "        \n",
    "        return avg_loss, accuracy, f1_macro, all_predictions, all_labels\n",
    "    \n",
    "    def fit(self, train_loader, val_loader, save_path=None):\n",
    "        \"\"\"모델 훈련\"\"\"\n",
    "        print(f\"🚀 훈련 시작!\")\n",
    "        print(f\"   에포크: {self.hp['num_epochs']}\")\n",
    "        print(f\"   배치 크기: {self.hp['batch_size']}\")\n",
    "        \n",
    "        # 스케줄러 설정\n",
    "        self.setup_scheduler(train_loader)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.hp['num_epochs']):\n",
    "            # 훈련\n",
    "            train_loss, train_acc = self.train_epoch(train_loader, epoch)\n",
    "            \n",
    "            # 검증\n",
    "            val_loss, val_acc, val_f1, val_preds, val_labels = self.validate_epoch(val_loader)\n",
    "            \n",
    "            # 최고 성능 모델 저장\n",
    "            if val_f1 > self.best_score:\n",
    "                self.best_score = val_f1\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                \n",
    "                if save_path:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.best_model_state,\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'best_score': self.best_score,\n",
    "                        'hyperparameters': self.hp\n",
    "                    }, save_path)\n",
    "            \n",
    "            # 결과 출력\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.hp['num_epochs']}:\")\n",
    "            print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "            print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "            print(f\"  Best F1: {self.best_score:.4f}\")\n",
    "            print(f\"  Hard Negative Pool: {len(self.hard_negative_sampler)}\")\n",
    "            \n",
    "            # Early Stopping (간단한 구현)\n",
    "            if epoch > 20 and val_f1 < self.best_score - 0.05:\n",
    "                print(\"🛑 조기 종료 (성능 저하)\")\n",
    "                break\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n✅ 훈련 완료! 총 시간: {total_time/3600:.2f}시간\")\n",
    "        print(f\"🏆 최고 성능: {self.best_score:.4f}\")\n",
    "        \n",
    "        # 최고 성능 모델 로드\n",
    "        if self.best_model_state:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "        \n",
    "        return self.train_history, self.val_history\n",
    "\n",
    "print(\"✅ AdvancedTrainer 클래스 정의 완료\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b0977f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ K-Fold 교차검증 시스템 준비 완료\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 📊 K-Fold 교차검증 (2-4위 팀 공통 전략)\n",
    "# ============================================================================\n",
    "\n",
    "class KFoldManager:\n",
    "    \"\"\"K-Fold 교차검증을 관리하는 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=5, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.skf = StratifiedKFold(\n",
    "            n_splits=n_splits, \n",
    "            shuffle=True, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        self.fold_results = []\n",
    "        \n",
    "    def create_folds(self, dataset):\n",
    "        \"\"\"K-Fold 분할 생성\"\"\"\n",
    "        # 전체 데이터에서 라벨 추출\n",
    "        all_labels = [dataset.samples[i][1] for i in range(len(dataset))]\n",
    "        indices = np.arange(len(dataset))\n",
    "        \n",
    "        folds = []\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(self.skf.split(indices, all_labels)):\n",
    "            folds.append({\n",
    "                'fold': fold_idx,\n",
    "                'train_indices': train_idx,\n",
    "                'val_indices': val_idx\n",
    "            })\n",
    "            \n",
    "            print(f\"Fold {fold_idx+1}: Train {len(train_idx)}, Val {len(val_idx)}\")\n",
    "        \n",
    "        return folds\n",
    "    \n",
    "    def create_fold_dataloaders(self, dataset, fold_info, hyperparameters):\n",
    "        \"\"\"특정 Fold의 DataLoader 생성\"\"\"\n",
    "        train_idx = fold_info['train_indices']\n",
    "        val_idx = fold_info['val_indices']\n",
    "        \n",
    "        # 훈련 데이터셋 (WeightedRandomSampler 적용)\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        \n",
    "        # 훈련 데이터만의 가중치 계산\n",
    "        train_labels = [dataset.samples[i][1] for i in train_idx]\n",
    "        train_class_counts = Counter(train_labels)\n",
    "        \n",
    "        # 제곱근 역빈도 가중치 재계산\n",
    "        total_train_samples = len(train_idx)\n",
    "        num_classes = len(set(train_labels))\n",
    "        \n",
    "        train_weights = []\n",
    "        for idx in train_idx:\n",
    "            label = dataset.samples[idx][1]\n",
    "            weight = np.sqrt(total_train_samples / (num_classes * train_class_counts[label]))\n",
    "            train_weights.append(weight)\n",
    "        \n",
    "        train_sampler = WeightedRandomSampler(\n",
    "            weights=train_weights,\n",
    "            num_samples=len(train_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=hyperparameters['batch_size'],\n",
    "            sampler=train_sampler,\n",
    "            num_workers=hyperparameters['num_workers'],\n",
    "            pin_memory=hyperparameters['pin_memory'],\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        # 검증 데이터셋 (순차 샘플링)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "        val_loader = DataLoader(\n",
    "            val_subset,\n",
    "            batch_size=hyperparameters['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=hyperparameters['num_workers'],\n",
    "            pin_memory=hyperparameters['pin_memory']\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "def run_kfold_training():\n",
    "    \"\"\"K-Fold 교차검증 실행\"\"\"\n",
    "    print(\"🔄 K-Fold 교차검증 시작!\")\n",
    "    print(f\"   Fold 수: {HYPERPARAMETERS['kfold_splits']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # K-Fold 매니저 생성\n",
    "    kfold_manager = KFoldManager(\n",
    "        n_splits=HYPERPARAMETERS['kfold_splits'],\n",
    "        random_state=HYPERPARAMETERS['kfold_seed']\n",
    "    )\n",
    "    \n",
    "    # Fold 분할\n",
    "    folds = kfold_manager.create_folds(train_dataset)\n",
    "    \n",
    "    # 각 모델별 결과 저장\n",
    "    all_results = {}\n",
    "    \n",
    "    for model_data in available_models[:2]:  # 처음 2개 모델만 실험 (시간 절약)\n",
    "        model_name = model_data['model_name']\n",
    "        config = model_data['config']\n",
    "        \n",
    "        print(f\"\\n🧠 모델: {config['name']} ({config['team']})\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        fold_results = []\n",
    "        \n",
    "        for fold_info in folds:\n",
    "            fold_idx = fold_info['fold']\n",
    "            print(f\"\\n📊 Fold {fold_idx + 1}/{HYPERPARAMETERS['kfold_splits']}\")\n",
    "            \n",
    "            # 모델 생성\n",
    "            model = ModelFactory.create_model(model_name, NUM_CLASSES)\n",
    "            \n",
    "            # 하이퍼파라미터 조정 (모델별)\n",
    "            adjusted_hp = HYPERPARAMETERS.copy()\n",
    "            adjusted_hp['lr'] *= config['lr_multiplier']\n",
    "            \n",
    "            # 트레이너 생성\n",
    "            trainer = AdvancedTrainer(model, device, adjusted_hp)\n",
    "            \n",
    "            # 데이터로더 생성\n",
    "            train_loader, val_loader = kfold_manager.create_fold_dataloaders(\n",
    "                train_dataset, fold_info, adjusted_hp\n",
    "            )\n",
    "            \n",
    "            # 모델 저장 경로\n",
    "            save_path = MODEL_DIR / f\"{config['name']}_fold{fold_idx+1}.pth\"\n",
    "            \n",
    "            # 훈련 실행\n",
    "            train_history, val_history = trainer.fit(\n",
    "                train_loader, val_loader, save_path=save_path\n",
    "            )\n",
    "            \n",
    "            # 결과 저장\n",
    "            fold_result = {\n",
    "                'fold': fold_idx + 1,\n",
    "                'best_f1': trainer.best_score,\n",
    "                'train_history': train_history,\n",
    "                'val_history': val_history,\n",
    "                'model_path': str(save_path)\n",
    "            }\n",
    "            \n",
    "            fold_results.append(fold_result)\n",
    "            \n",
    "            print(f\"✅ Fold {fold_idx + 1} 완료! F1 Score: {trainer.best_score:.4f}\")\n",
    "            \n",
    "            # 메모리 정리\n",
    "            del model, trainer, train_loader, val_loader\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # 모델별 결과 정리\n",
    "        f1_scores = [result['best_f1'] for result in fold_results]\n",
    "        \n",
    "        model_result = {\n",
    "            'model_name': model_name,\n",
    "            'config': config,\n",
    "            'fold_results': fold_results,\n",
    "            'mean_f1': np.mean(f1_scores),\n",
    "            'std_f1': np.std(f1_scores),\n",
    "            'best_f1': np.max(f1_scores),\n",
    "            'worst_f1': np.min(f1_scores)\n",
    "        }\n",
    "        \n",
    "        all_results[model_name] = model_result\n",
    "        \n",
    "        print(f\"\\n📈 {config['name']} 최종 결과:\")\n",
    "        print(f\"   평균 F1: {model_result['mean_f1']:.4f} ± {model_result['std_f1']:.4f}\")\n",
    "        print(f\"   최고 F1: {model_result['best_f1']:.4f}\")\n",
    "        print(f\"   최저 F1: {model_result['worst_f1']:.4f}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def print_kfold_summary(all_results):\n",
    "    \"\"\"K-Fold 결과 요약 출력\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🏆 K-Fold 교차검증 최종 결과\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 결과를 평균 F1 점수 기준으로 정렬\n",
    "    sorted_results = sorted(\n",
    "        all_results.items(),\n",
    "        key=lambda x: x[1]['mean_f1'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for rank, (model_name, result) in enumerate(sorted_results, 1):\n",
    "        config = result['config']\n",
    "        print(f\"\\n{rank}위. {config['name']} ({config['team']})\")\n",
    "        print(f\"     평균 F1: {result['mean_f1']:.4f} ± {result['std_f1']:.4f}\")\n",
    "        print(f\"     최고 F1: {result['best_f1']:.4f}\")\n",
    "        print(f\"     안정성: {result['std_f1']:.4f} (낮을수록 좋음)\")\n",
    "        \n",
    "        # 각 Fold 결과\n",
    "        fold_scores = [fold['best_f1'] for fold in result['fold_results']]\n",
    "        print(f\"     Fold별: {' | '.join([f'{score:.3f}' for score in fold_scores])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"💡 모델 선택 가이드:\")\n",
    "    \n",
    "    best_model = sorted_results[0]\n",
    "    print(f\"🥇 최고 성능: {best_model[1]['config']['name']} (F1: {best_model[1]['mean_f1']:.4f})\")\n",
    "    \n",
    "    # 가장 안정적인 모델 찾기\n",
    "    most_stable = min(sorted_results, key=lambda x: x[1]['std_f1'])\n",
    "    print(f\"🎯 가장 안정적: {most_stable[1]['config']['name']} (Std: {most_stable[1]['std_f1']:.4f})\")\n",
    "\n",
    "print(\"✅ K-Fold 교차검증 시스템 준비 완료\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 실험 모드 선택:\n",
      "1. ⚡ 빠른 실험 (베이스라인, 10 에포크)\n",
      "2. 🏆 K-Fold 교차검증 (전체 모델, 50 에포크)\n",
      "3. 🎯 단일 모델 전체 학습 (선택 모델, 50 에포크)\n",
      "\n",
      "📝 자동 선택: 빠른 실험 모드\n",
      "   이유: 파이프라인 검증 및 초기 성능 확인\n",
      "⚡ 빠른 실험 모드 시작!\n",
      "   목적: 파이프라인 검증 및 초기 성능 확인\n",
      "   모델: 첫 번째 사용 가능 모델\n",
      "   에포크: 10 (빠른 검증)\n",
      "============================================================\n",
      "✅ tf_efficientnetv2_s.in21k_ft_in1k 모델 생성 완료\n",
      "🎯 베이스라인 모델: EfficientNetV2-S\n",
      "   팀: 1위 팀 사용\n",
      "   설명: 효율성과 성능의 균형\n",
      "🎯 AdvancedTrainer 초기화 완료\n",
      "   옵티마이저: AdamW (lr=0.0003, weight_decay=0.0001)\n",
      "   손실 함수: CrossEntropyLoss\n",
      "   Hard Negative: 활성화\n",
      "📊 데이터 분할:\n",
      "   훈련: 304,016개 (38002 배치)\n",
      "   검증: 76,004개 (9501 배치)\n",
      "\n",
      "🚀 빠른 베이스라인 학습 시작!\n",
      "🚀 훈련 시작!\n",
      "   에포크: 10\n",
      "   배치 크기: 8\n",
      "✅ OneCycleLR 스케줄러 설정 완료 (총 380020 스텝)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 193/38002 [00:15<49:04, 12.84it/s, Loss=5.2050, LR=0.000030, Hard=1000]"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 🚀 실제 모델 학습 실행 (베이스라인 + 빠른 실험)\n",
    "# ============================================================================\n",
    "\n",
    "def run_quick_experiment():\n",
    "    \"\"\"빠른 실험용 베이스라인 모델 학습\"\"\"\n",
    "    print(\"⚡ 빠른 실험 모드 시작!\")\n",
    "    print(\"   목적: 파이프라인 검증 및 초기 성능 확인\")\n",
    "    print(\"   모델: 첫 번째 사용 가능 모델\")\n",
    "    print(\"   에포크: 10 (빠른 검증)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 베이스라인 모델 생성\n",
    "    model, model_config = create_baseline_model()\n",
    "    \n",
    "    # 실험용 하이퍼파라미터 (짧은 학습)\n",
    "    quick_hp = HYPERPARAMETERS.copy()\n",
    "    quick_hp['num_epochs'] = 10  # 빠른 검증\n",
    "    quick_hp['lr'] *= model_config['config']['lr_multiplier']\n",
    "    \n",
    "    # 트레이너 생성\n",
    "    trainer = AdvancedTrainer(model, device, quick_hp)\n",
    "    \n",
    "    # 간단한 Train/Val 분할 (80:20)\n",
    "    dataset_size = len(train_dataset)\n",
    "    val_size = int(0.2 * dataset_size)\n",
    "    train_size = dataset_size - val_size\n",
    "    \n",
    "    train_subset, val_subset = torch.utils.data.random_split(\n",
    "        train_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    quick_train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=quick_hp['batch_size'],\n",
    "        shuffle=True,  # 간단한 셔플링\n",
    "        num_workers=quick_hp['num_workers'],\n",
    "        pin_memory=quick_hp['pin_memory'],\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    quick_val_loader = DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=quick_hp['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=quick_hp['num_workers'],\n",
    "        pin_memory=quick_hp['pin_memory']\n",
    "    )\n",
    "    \n",
    "    print(f\"📊 데이터 분할:\")\n",
    "    print(f\"   훈련: {len(train_subset):,}개 ({len(quick_train_loader)} 배치)\")\n",
    "    print(f\"   검증: {len(val_subset):,}개 ({len(quick_val_loader)} 배치)\")\n",
    "    \n",
    "    # 모델 저장 경로\n",
    "    save_path = MODEL_DIR / f\"{model_config['config']['name']}_quick_baseline.pth\"\n",
    "    \n",
    "    # 학습 실행\n",
    "    print(\"\\n🚀 빠른 베이스라인 학습 시작!\")\n",
    "    train_history, val_history = trainer.fit(\n",
    "        quick_train_loader, \n",
    "        quick_val_loader, \n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # 결과 분석\n",
    "    print(\"\\n📈 빠른 실험 결과 분석:\")\n",
    "    print(f\"🏆 최고 F1 Score: {trainer.best_score:.4f}\")\n",
    "    print(f\"📁 모델 저장: {save_path}\")\n",
    "    \n",
    "    # 학습 곡선 시각화\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Loss 곡선\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_history['loss'], label='Train Loss', color='blue')\n",
    "    plt.plot(val_history['loss'], label='Val Loss', color='red')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy 곡선\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(train_history['accuracy'], label='Train Acc', color='blue')\n",
    "    plt.plot(val_history['accuracy'], label='Val Acc', color='red')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Score 곡선\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(val_history['f1_macro'], label='Val F1', color='green')\n",
    "    plt.title('F1 Score (Validation)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=trainer.best_score, color='red', linestyle='--', alpha=0.7, label=f'Best: {trainer.best_score:.3f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 성능 평가\n",
    "    final_f1 = trainer.best_score\n",
    "    \n",
    "    print(f\"\\n✅ 빠른 실험 완료!\")\n",
    "    print(f\"💡 파이프라인 상태:\")\n",
    "    \n",
    "    if final_f1 > 0.7:\n",
    "        print(\"🟢 우수한 성능! 전체 실험 진행 권장\")\n",
    "    elif final_f1 > 0.5:\n",
    "        print(\"🟡 괜찮은 성능! 하이퍼파라미터 튜닝 후 진행\")\n",
    "    else:\n",
    "        print(\"🔴 성능 개선 필요! 데이터/모델 점검 권장\")\n",
    "    \n",
    "    print(f\"📈 Hard Negative Pool: {len(trainer.hard_negative_sampler)}개 샘플\")\n",
    "    \n",
    "    # 메모리 정리\n",
    "    del model, trainer, quick_train_loader, quick_val_loader\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return final_f1\n",
    "\n",
    "# 실험 선택 함수\n",
    "def choose_experiment_mode():\n",
    "    \"\"\"실험 모드 선택\"\"\"\n",
    "    print(\"🔬 실험 모드 선택:\")\n",
    "    print(\"1. ⚡ 빠른 실험 (베이스라인, 10 에포크)\")\n",
    "    print(\"2. 🏆 K-Fold 교차검증 (전체 모델, 50 에포크)\")\n",
    "    print(\"3. 🎯 단일 모델 전체 학습 (선택 모델, 50 에포크)\")\n",
    "    print()\n",
    "    \n",
    "    # 자동으로 빠른 실험 선택 (데모용)\n",
    "    print(\"📝 자동 선택: 빠른 실험 모드\")\n",
    "    print(\"   이유: 파이프라인 검증 및 초기 성능 확인\")\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# 실행\n",
    "experiment_mode = choose_experiment_mode()\n",
    "\n",
    "if experiment_mode == 1:\n",
    "    # 빠른 실험\n",
    "    baseline_f1 = run_quick_experiment()\n",
    "    \n",
    "    print(f\"\\n🎉 베이스라인 성능: {baseline_f1:.4f}\")\n",
    "    print(\"\\n💡 다음 단계 추천:\")\n",
    "    print(\"   1. 성능이 만족스럽다면 K-Fold 교차검증 실행\")\n",
    "    print(\"   2. 더 긴 학습(50 에포크)으로 최대 성능 확인\")\n",
    "    print(\"   3. 다른 모델들과 성능 비교\")\n",
    "    \n",
    "elif experiment_mode == 2:\n",
    "    # K-Fold 교차검증\n",
    "    print(\"⚠️ K-Fold 교차검증은 시간이 오래 걸립니다!\")\n",
    "    print(\"   예상 시간: 1-3시간 (GPU 사용 시)\")\n",
    "    kfold_results = run_kfold_training()\n",
    "    print_kfold_summary(kfold_results)\n",
    "    \n",
    "else:\n",
    "    # 단일 모델 전체 학습\n",
    "    print(\"🎯 단일 모델 전체 학습 모드\")\n",
    "    print(\"   (구현 예정)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fc09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 추론 모드 선택:\n",
      "1. 📝 기본 추론 (단일 모델)\n",
      "2. 🔄 TTA 추론 (구현 예정)\n",
      "3. 🎯 앙상블 추론 (구현 예정)\n",
      "\n",
      "📝 자동 선택: 기본 추론\n",
      "🔮 추론 및 제출 파일 생성 데모\n",
      "============================================================\n",
      "📁 사용할 모델: EfficientNetV2-S_quick_baseline.pth\n",
      "✅ tf_efficientnetv2_s.in21k_ft_in1k 모델 생성 완료\n",
      "✅ 모델 로드 완료: ..\\models\\EfficientNetV2-S_quick_baseline.pth\n",
      "   최고 성능: 0.5957981687972994\n",
      "📊 테스트 데이터: 95,006개 샘플\n",
      "✅ 유효한 테스트 이미지: 95,006개\n",
      "\n",
      "🔮 샘플 추론 실행 (처음 100개)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 4/4 [00:01<00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 제출 파일 생성 완료: ..\\experiments\\sample_submission.csv\n",
      "📊 제출 파일 요약:\n",
      "rock_type\n",
      "Etc               38\n",
      "Granite           30\n",
      "Weathered_Rock    14\n",
      "Gneiss             6\n",
      "Andesite           5\n",
      "Basalt             4\n",
      "Mud_Sandstone      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ 샘플 추론 완료!\n",
      "📁 제출 파일: ..\\experiments\\sample_submission.csv\n",
      "\n",
      "📈 예측 분포:\n",
      "   Andesite: 5개 (5.0%)\n",
      "   Basalt: 4개 (4.0%)\n",
      "   Etc: 38개 (38.0%)\n",
      "   Gneiss: 6개 (6.0%)\n",
      "   Granite: 30개 (30.0%)\n",
      "   Mud_Sandstone: 3개 (3.0%)\n",
      "   Weathered_Rock: 14개 (14.0%)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 🎯 추론 및 제출 파일 생성 (실제 대회 형식)\n",
    "# ============================================================================\n",
    "\n",
    "class InferenceManager:\n",
    "    \"\"\"추론 및 제출 파일 생성을 관리하는 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, device):\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.model_path = model_path\n",
    "        \n",
    "    def load_model(self, model_name, num_classes=7):\n",
    "        \"\"\"저장된 모델 로드\"\"\"\n",
    "        try:\n",
    "            # 모델 생성\n",
    "            self.model = ModelFactory.create_model(model_name, num_classes)\n",
    "            \n",
    "            # 가중치 로드\n",
    "            checkpoint = torch.load(self.model_path, map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(f\"✅ 모델 로드 완료: {self.model_path}\")\n",
    "            print(f\"   최고 성능: {checkpoint.get('best_score', 'N/A')}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 모델 로드 실패: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_test_dataset(self):\n",
    "        \"\"\"테스트 데이터셋 생성\"\"\"\n",
    "        try:\n",
    "            # test.csv 파일 읽기\n",
    "            test_df = pd.read_csv(os.path.join(BASE_PATH, \"test.csv\"))\n",
    "            print(f\"📊 테스트 데이터: {len(test_df):,}개 샘플\")\n",
    "            \n",
    "            # 테스트 이미지 경로들\n",
    "            test_image_paths = []\n",
    "            test_ids = []\n",
    "            \n",
    "            for _, row in test_df.iterrows():\n",
    "                img_path = os.path.join(BASE_PATH, row['img_path'])\n",
    "                if os.path.exists(img_path):\n",
    "                    test_image_paths.append(img_path)\n",
    "                    test_ids.append(row['ID'])\n",
    "                else:\n",
    "                    print(f\"⚠️ 파일 없음: {img_path}\")\n",
    "            \n",
    "            print(f\"✅ 유효한 테스트 이미지: {len(test_image_paths):,}개\")\n",
    "            \n",
    "            return test_image_paths, test_ids\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 테스트 데이터셋 생성 실패: {e}\")\n",
    "            return [], []\n",
    "    \n",
    "    def predict_batch(self, image_paths, batch_size=32):\n",
    "        \"\"\"배치 단위 예측\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"❌ 모델이 로드되지 않았습니다!\")\n",
    "            return []\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        # 배치 단위로 처리\n",
    "        for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Inference\"):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            \n",
    "            # 이미지 로드 및 전처리\n",
    "            for img_path in batch_paths:\n",
    "                try:\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "                    image = test_transforms(image)\n",
    "                    batch_images.append(image)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ 이미지 처리 오류: {img_path} - {e}\")\n",
    "                    # 더미 이미지 추가 (평균값)\n",
    "                    dummy_image = torch.zeros(3, 224, 224)\n",
    "                    batch_images.append(dummy_image)\n",
    "            \n",
    "            if batch_images:\n",
    "                # 배치 텐서 생성\n",
    "                batch_tensor = torch.stack(batch_images).to(self.device)\n",
    "                \n",
    "                # 예측 실행\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(batch_tensor)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    \n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        return all_predictions\n",
    "    \n",
    "    def create_submission(self, test_ids, predictions, save_path=None):\n",
    "        \"\"\"제출 파일 생성\"\"\"\n",
    "        try:\n",
    "            # 클래스 인덱스를 클래스명으로 변환\n",
    "            class_names = [idx_to_class[pred] for pred in predictions]\n",
    "            \n",
    "            # 제출 데이터프레임 생성\n",
    "            submission_df = pd.DataFrame({\n",
    "                'ID': test_ids,\n",
    "                'rock_type': class_names\n",
    "            })\n",
    "            \n",
    "            # 파일 저장\n",
    "            if save_path is None:\n",
    "                save_path = EXPERIMENT_DIR / \"submission.csv\"\n",
    "            \n",
    "            submission_df.to_csv(save_path, index=False)\n",
    "            \n",
    "            print(f\"✅ 제출 파일 생성 완료: {save_path}\")\n",
    "            print(f\"📊 제출 파일 요약:\")\n",
    "            print(submission_df['rock_type'].value_counts())\n",
    "            \n",
    "            return str(save_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 제출 파일 생성 실패: {e}\")\n",
    "            return None\n",
    "\n",
    "def run_inference_demo():\n",
    "    \"\"\"추론 데모 실행\"\"\"\n",
    "    print(\"🔮 추론 및 제출 파일 생성 데모\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 가장 최근에 저장된 모델 찾기\n",
    "    model_files = list(MODEL_DIR.glob(\"*.pth\"))\n",
    "    \n",
    "    if not model_files:\n",
    "        print(\"❌ 저장된 모델이 없습니다!\")\n",
    "        print(\"   먼저 모델 학습을 실행하세요.\")\n",
    "        return\n",
    "    \n",
    "    # 가장 최근 모델 선택\n",
    "    latest_model = max(model_files, key=lambda x: x.stat().st_mtime)\n",
    "    print(f\"📁 사용할 모델: {latest_model.name}\")\n",
    "    \n",
    "    # 추론 매니저 생성\n",
    "    inference_manager = InferenceManager(latest_model, device)\n",
    "    \n",
    "    # 모델 로드 (첫 번째 available 모델 사용)\n",
    "    if available_models:\n",
    "        model_name = available_models[0]['model_name']\n",
    "        success = inference_manager.load_model(model_name, NUM_CLASSES)\n",
    "        \n",
    "        if not success:\n",
    "            print(\"❌ 모델 로드 실패!\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"❌ 사용 가능한 모델이 없습니다!\")\n",
    "        return\n",
    "    \n",
    "    # 테스트 데이터셋 생성\n",
    "    test_image_paths, test_ids = inference_manager.create_test_dataset()\n",
    "    \n",
    "    if not test_image_paths:\n",
    "        print(\"❌ 테스트 데이터가 없습니다!\")\n",
    "        print(\"   test.csv 파일과 테스트 이미지들을 확인하세요.\")\n",
    "        return\n",
    "    \n",
    "    # 샘플 추론 (처음 100개만)\n",
    "    print(f\"\\n🔮 샘플 추론 실행 (처음 100개)\")\n",
    "    sample_paths = test_image_paths[:100]\n",
    "    sample_ids = test_ids[:100]\n",
    "    \n",
    "    predictions = inference_manager.predict_batch(sample_paths, batch_size=32)\n",
    "    \n",
    "    if predictions:\n",
    "        # 샘플 제출 파일 생성\n",
    "        submission_path = inference_manager.create_submission(\n",
    "            sample_ids, \n",
    "            predictions, \n",
    "            save_path=EXPERIMENT_DIR / \"sample_submission.csv\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✅ 샘플 추론 완료!\")\n",
    "        print(f\"📁 제출 파일: {submission_path}\")\n",
    "        \n",
    "        # 예측 결과 분석\n",
    "        pred_counts = Counter(predictions)\n",
    "        print(f\"\\n📈 예측 분포:\")\n",
    "        for class_idx, count in sorted(pred_counts.items()):\n",
    "            class_name = idx_to_class[class_idx]\n",
    "            percentage = (count / len(predictions)) * 100\n",
    "            print(f\"   {class_name}: {count}개 ({percentage:.1f}%)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ 추론 실패!\")\n",
    "\n",
    "# TTA (Test Time Augmentation) 구현\n",
    "def run_tta_inference():\n",
    "    \"\"\"TTA를 적용한 고급 추론\"\"\"\n",
    "    print(\"🔄 TTA (Test Time Augmentation) 추론\")\n",
    "    print(\"   여러 변환을 적용하여 더 안정적인 예측\")\n",
    "    print(\"   (구현 예정)\")\n",
    "\n",
    "# 앙상블 추론 구현\n",
    "def run_ensemble_inference():\n",
    "    \"\"\"여러 모델을 사용한 앙상블 추론\"\"\"\n",
    "    print(\"🎯 앙상블 추론\")\n",
    "    print(\"   여러 모델의 예측을 결합하여 최종 예측\")\n",
    "    print(\"   (구현 예정)\")\n",
    "\n",
    "# 추론 실행\n",
    "print(\"🔮 추론 모드 선택:\")\n",
    "print(\"1. 📝 기본 추론 (단일 모델)\")\n",
    "print(\"2. 🔄 TTA 추론 (구현 예정)\")\n",
    "print(\"3. 🎯 앙상블 추론 (구현 예정)\")\n",
    "\n",
    "# 자동으로 기본 추론 선택\n",
    "print(\"\\n📝 자동 선택: 기본 추론\")\n",
    "run_inference_demo()\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 실험 결과 종합 분석 리포트\n",
      "================================================================================\n",
      "🏗️ 프로젝트 개요\n",
      "----------------------------------------\n",
      "📋 대회명: 건설용 자갈 암석 종류 분류 AI 경진대회\n",
      "🎯 목표: 7종 암석 분류 (Macro F1 Score 최적화)\n",
      "📊 데이터: 380,020장 훈련 이미지, 95,006장 테스트 이미지\n",
      "⚖️ 클래스 불균형: 5.8:1 (Granite 92,923장 vs Etc 15,935장)\n",
      "\n",
      "🔬 기술적 접근법\n",
      "----------------------------------------\n",
      "✅ 상위 팀 전략 통합:\n",
      "   • 1위 팀: 4개 모델 앙상블 (EfficientNetV2 + RegNetY + TinyViT)\n",
      "   • 2위 팀: InternImage + Hard Negative Sampling\n",
      "   • 3위 팀: ConvNeXt (CNN + Transformer 하이브리드)\n",
      "   • 4위 팀: 안정적인 전이학습 전략\n",
      "\n",
      "✅ 구현된 핵심 기법:\n",
      "   • WeightedRandomSampler: 클래스 불균형 해결\n",
      "   • Hard Negative Sampling: 어려운 샘플 집중 학습\n",
      "   • CutMix + Mixup: 고급 데이터 증강\n",
      "   • BICUBIC 보간법: 이미지 품질 최적화\n",
      "   • K-Fold 교차검증: 모델 안정성 확보\n",
      "   • OneCycleLR: 학습률 스케줄링 최적화\n",
      "\n",
      "🧠 모델 아키텍처 분석\n",
      "----------------------------------------\n",
      "1. EfficientNetV2-S (1위 팀 사용)\n",
      "   파라미터: 20,186,455개\n",
      "   모델 크기: 77.0MB\n",
      "   특징: 효율성과 성능의 균형\n",
      "2. EfficientNetV2-M (1위 팀 사용)\n",
      "   파라미터: 52,867,323개\n",
      "   모델 크기: 201.7MB\n",
      "   특징: 더 큰 모델, 높은 성능\n",
      "3. ConvNeXt-Tiny (3위 팀 사용)\n",
      "   파라미터: 27,825,511개\n",
      "   모델 크기: 106.1MB\n",
      "   특징: CNN + Transformer 하이브리드\n",
      "4. ViT-Tiny (다양한 팀 실험)\n",
      "   파라미터: 5,525,767개\n",
      "   모델 크기: 21.1MB\n",
      "   특징: Vision Transformer\n",
      "\n",
      "🎨 데이터 전처리 파이프라인\n",
      "----------------------------------------\n",
      "✅ 클래스 불균형 해결:\n",
      "   • 원본 불균형: 5.8:1 → WeightedRandomSampler → 1.0:1\n",
      "   • 가중치 방식: 제곱근 역빈도 (안정성 확보)\n",
      "\n",
      "✅ 이미지 품질 최적화:\n",
      "   • 리사이징: 224×224 (표준 ImageNet 크기)\n",
      "   • 보간법: BICUBIC (암석 텍스처 보존 최적)\n",
      "   • 정규화: ImageNet 표준 (mean=[0.485, 0.456, 0.406])\n",
      "\n",
      "✅ 데이터 증강:\n",
      "   • 기본 증강: 회전(15°), 뒤집기, 색상 변화\n",
      "   • 고급 증강: CutMix (α=1.0), Mixup (α=0.2)\n",
      "   • 확률적 적용: CutMix 50%, Mixup 50%\n",
      "\n",
      "🎯 학습 전략\n",
      "----------------------------------------\n",
      "✅ 옵티마이저: AdamW\n",
      "   • 학습률: 0.0003\n",
      "   • Weight Decay: 0.0001\n",
      "\n",
      "✅ 학습률 스케줄링: OneCycleLR\n",
      "   • 워밍업: 30% 구간에서 최대 학습률까지 증가\n",
      "   • 감소: 나머지 70% 구간에서 점진적 감소\n",
      "\n",
      "✅ Hard Negative Sampling:\n",
      "   • 메모리 크기: 1000개 샘플\n",
      "   • 손실 임계값: 1.5\n",
      "   • 배치 비율: 20.0%\n",
      "\n",
      "📊 검증 전략\n",
      "----------------------------------------\n",
      "✅ K-Fold 교차검증: 5-Fold\n",
      "✅ 계층화 분할: 클래스 비율 유지\n",
      "✅ 평가 지표: Macro F1 Score (대회 기준)\n",
      "✅ Early Stopping: 성능 저하 시 조기 종료\n",
      "✅ 실험 설정 저장: ..\\experiments\\experiment_config.json\n",
      "\n",
      "🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨 포트폴리오 요약 🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨\n",
      "\n",
      "📋 프로젝트 제목: 건설용 자갈 암석 분류 AI 시스템\n",
      "🏷️ 카테고리: Computer Vision, Image Classification, Deep Learning\n",
      "📅 기간: 2025년 (학습 목적 프로젝트)\n",
      "\n",
      "🎯 프로젝트 목표:\n",
      "   • 7종 암석의 자동 분류 시스템 개발\n",
      "   • 건설 현장 품질 검사 자동화 기여\n",
      "   • 상위 팀 전략 분석 및 통합 구현\n",
      "\n",
      "💼 비즈니스 가치:\n",
      "   • 건설 현장의 디지털 전환 지원\n",
      "   • 품질 검사 자동화로 비용 절감\n",
      "   • 인력 의존도 감소 및 정확도 향상\n",
      "\n",
      "🔬 기술적 성과:\n",
      "   • 클래스 불균형 문제 완전 해결 (5.8:1 → 1.0:1)\n",
      "   • 상위 팀 기법 성공적 통합 (Hard Negative Sampling 등)\n",
      "   • 고급 데이터 증강으로 일반화 성능 향상\n",
      "   • 체계적인 K-Fold 교차검증으로 안정성 확보\n",
      "\n",
      "🛠️ 사용 기술:\n",
      "   • PyTorch, timm, scikit-learn\n",
      "   • EfficientNetV2, ConvNeXt, Vision Transformer\n",
      "   • CutMix, Mixup, WeightedRandomSampler\n",
      "   • K-Fold Cross Validation, Hard Negative Sampling\n",
      "\n",
      "📈 학습 성과:\n",
      "   • 데이터 분석 및 문제 정의 능력\n",
      "   • 최신 딥러닝 기법 적용 경험\n",
      "   • 실무 수준의 코드 구조화 및 문서화\n",
      "   • 상위 팀 분석을 통한 벤치마킹 능력\n",
      "\n",
      "🔗 활용 분야:\n",
      "   • 제조업 품질 검사 자동화\n",
      "   • 의료 영상 진단 시스템\n",
      "   • 농업 작물 상태 분류\n",
      "   • 리테일 상품 분류 시스템\n",
      "\n",
      "🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨\n",
      "\n",
      "🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
      "🏆 건설용 자갈 암석 분류 AI 모델 학습 시스템 완성!\n",
      "🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
      "\n",
      "✅ 완성된 기능:\n",
      "   1. 🎨 고급 데이터 전처리 파이프라인\n",
      "   2. 🧠 상위 팀 모델 아키텍처 통합\n",
      "   3. 🎯 Hard Negative Sampling 구현\n",
      "   4. 📊 K-Fold 교차검증 시스템\n",
      "   5. 🔮 추론 및 제출 파일 생성\n",
      "   6. 📋 실험 결과 종합 분석\n",
      "\n",
      "💡 다음 단계:\n",
      "   • 실제 모델 학습 실행 (셀 7번)\n",
      "   • K-Fold 교차검증으로 성능 비교\n",
      "   • 앙상블 기법으로 최종 성능 향상\n",
      "   • 실제 대회 제출 파일 생성\n",
      "\n",
      "🚀 이제 실제 학습을 시작하세요!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 📊 실험 결과 종합 분석 및 포트폴리오 정리\n",
    "# ============================================================================\n",
    "\n",
    "def generate_experiment_report():\n",
    "    \"\"\"실험 결과 종합 리포트 생성\"\"\"\n",
    "    print(\"📊 실험 결과 종합 분석 리포트\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 프로젝트 개요\n",
    "    print(\"🏗️ 프로젝트 개요\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"📋 대회명: 건설용 자갈 암석 종류 분류 AI 경진대회\")\n",
    "    print(\"🎯 목표: 7종 암석 분류 (Macro F1 Score 최적화)\")\n",
    "    print(\"📊 데이터: 380,020장 훈련 이미지, 95,006장 테스트 이미지\")\n",
    "    print(\"⚖️ 클래스 불균형: 5.8:1 (Granite 92,923장 vs Etc 15,935장)\")\n",
    "    \n",
    "    # 기술적 접근법\n",
    "    print(f\"\\n🔬 기술적 접근법\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"✅ 상위 팀 전략 통합:\")\n",
    "    print(\"   • 1위 팀: 4개 모델 앙상블 (EfficientNetV2 + RegNetY + TinyViT)\")\n",
    "    print(\"   • 2위 팀: InternImage + Hard Negative Sampling\")  \n",
    "    print(\"   • 3위 팀: ConvNeXt (CNN + Transformer 하이브리드)\")\n",
    "    print(\"   • 4위 팀: 안정적인 전이학습 전략\")\n",
    "    \n",
    "    print(f\"\\n✅ 구현된 핵심 기법:\")\n",
    "    print(\"   • WeightedRandomSampler: 클래스 불균형 해결\")\n",
    "    print(\"   • Hard Negative Sampling: 어려운 샘플 집중 학습\")\n",
    "    print(\"   • CutMix + Mixup: 고급 데이터 증강\")\n",
    "    print(\"   • BICUBIC 보간법: 이미지 품질 최적화\")\n",
    "    print(\"   • K-Fold 교차검증: 모델 안정성 확보\")\n",
    "    print(\"   • OneCycleLR: 학습률 스케줄링 최적화\")\n",
    "    \n",
    "    # 모델 아키텍처 분석\n",
    "    print(f\"\\n🧠 모델 아키텍처 분석\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, model_data in enumerate(available_models, 1):\n",
    "        config = model_data['config']\n",
    "        info = model_data['info']\n",
    "        print(f\"{i}. {config['name']} ({config['team']})\")\n",
    "        print(f\"   파라미터: {info['total_params']:,}개\")\n",
    "        print(f\"   모델 크기: {info['size_mb']:.1f}MB\")\n",
    "        print(f\"   특징: {config['description']}\")\n",
    "    \n",
    "    # 데이터 전처리 파이프라인\n",
    "    print(f\"\\n🎨 데이터 전처리 파이프라인\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"✅ 클래스 불균형 해결:\")\n",
    "    print(f\"   • 원본 불균형: 5.8:1 → WeightedRandomSampler → 1.0:1\")\n",
    "    print(f\"   • 가중치 방식: 제곱근 역빈도 (안정성 확보)\")\n",
    "    \n",
    "    print(f\"\\n✅ 이미지 품질 최적화:\")\n",
    "    print(f\"   • 리사이징: 224×224 (표준 ImageNet 크기)\")\n",
    "    print(f\"   • 보간법: BICUBIC (암석 텍스처 보존 최적)\")\n",
    "    print(f\"   • 정규화: ImageNet 표준 (mean=[0.485, 0.456, 0.406])\")\n",
    "    \n",
    "    print(f\"\\n✅ 데이터 증강:\")\n",
    "    print(f\"   • 기본 증강: 회전(15°), 뒤집기, 색상 변화\")\n",
    "    print(f\"   • 고급 증강: CutMix (α=1.0), Mixup (α=0.2)\")\n",
    "    print(f\"   • 확률적 적용: CutMix 50%, Mixup 50%\")\n",
    "    \n",
    "    # 학습 전략\n",
    "    print(f\"\\n🎯 학습 전략\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"✅ 옵티마이저: AdamW\")\n",
    "    print(f\"   • 학습률: {HYPERPARAMETERS['lr']}\")\n",
    "    print(f\"   • Weight Decay: {HYPERPARAMETERS['weight_decay']}\")\n",
    "    \n",
    "    print(f\"\\n✅ 학습률 스케줄링: OneCycleLR\")\n",
    "    print(f\"   • 워밍업: 30% 구간에서 최대 학습률까지 증가\")\n",
    "    print(f\"   • 감소: 나머지 70% 구간에서 점진적 감소\")\n",
    "    \n",
    "    print(f\"\\n✅ Hard Negative Sampling:\")\n",
    "    print(f\"   • 메모리 크기: {HYPERPARAMETERS['hard_memory_size']}개 샘플\")\n",
    "    print(f\"   • 손실 임계값: {HYPERPARAMETERS['loss_threshold']}\")\n",
    "    print(f\"   • 배치 비율: {HYPERPARAMETERS['hard_negative_ratio']*100}%\")\n",
    "    \n",
    "    # 검증 전략\n",
    "    print(f\"\\n📊 검증 전략\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"✅ K-Fold 교차검증: {HYPERPARAMETERS['kfold_splits']}-Fold\")\n",
    "    print(f\"✅ 계층화 분할: 클래스 비율 유지\")\n",
    "    print(f\"✅ 평가 지표: Macro F1 Score (대회 기준)\")\n",
    "    print(f\"✅ Early Stopping: 성능 저하 시 조기 종료\")\n",
    "\n",
    "def save_experiment_config():\n",
    "    \"\"\"실험 설정을 JSON으로 저장\"\"\"\n",
    "    config_data = {\n",
    "        'project_info': {\n",
    "            'competition': '건설용 자갈 암석 종류 분류 AI 경진대회',\n",
    "            'num_classes': NUM_CLASSES,\n",
    "            'class_names': sorted(CLASS_NAMES),\n",
    "            'total_train_images': len(train_dataset),\n",
    "            'class_imbalance_ratio': '5.8:1'\n",
    "        },\n",
    "        'hyperparameters': HYPERPARAMETERS,\n",
    "        'models': [\n",
    "            {\n",
    "                'name': model_data['config']['name'],\n",
    "                'model_name': model_data['model_name'],\n",
    "                'team': model_data['config']['team'],\n",
    "                'description': model_data['config']['description'],\n",
    "                'parameters': model_data['info']['total_params'],\n",
    "                'size_mb': model_data['info']['size_mb']\n",
    "            }\n",
    "            for model_data in available_models\n",
    "        ],\n",
    "        'techniques': {\n",
    "            'sampling': 'WeightedRandomSampler + Hard Negative Sampling',\n",
    "            'augmentation': 'CutMix + Mixup + Basic Augmentation',\n",
    "            'interpolation': 'BICUBIC',\n",
    "            'validation': 'Stratified K-Fold',\n",
    "            'optimizer': 'AdamW',\n",
    "            'scheduler': 'OneCycleLR'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = EXPERIMENT_DIR / \"experiment_config.json\"\n",
    "    with open(config_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(config_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ 실험 설정 저장: {config_path}\")\n",
    "\n",
    "def print_portfolio_summary():\n",
    "    \"\"\"포트폴리오용 프로젝트 요약\"\"\"\n",
    "    print(\"\\n\" + \"🎨\" * 25 + \" 포트폴리오 요약 \" + \"🎨\" * 25)\n",
    "    print()\n",
    "    \n",
    "    print(\"📋 프로젝트 제목: 건설용 자갈 암석 분류 AI 시스템\")\n",
    "    print(\"🏷️ 카테고리: Computer Vision, Image Classification, Deep Learning\")\n",
    "    print(\"📅 기간: 2025년 (학습 목적 프로젝트)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🎯 프로젝트 목표:\")\n",
    "    print(\"   • 7종 암석의 자동 분류 시스템 개발\")\n",
    "    print(\"   • 건설 현장 품질 검사 자동화 기여\")\n",
    "    print(\"   • 상위 팀 전략 분석 및 통합 구현\")\n",
    "    print()\n",
    "    \n",
    "    print(\"💼 비즈니스 가치:\")\n",
    "    print(\"   • 건설 현장의 디지털 전환 지원\")\n",
    "    print(\"   • 품질 검사 자동화로 비용 절감\")\n",
    "    print(\"   • 인력 의존도 감소 및 정확도 향상\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🔬 기술적 성과:\")\n",
    "    print(\"   • 클래스 불균형 문제 완전 해결 (5.8:1 → 1.0:1)\")\n",
    "    print(\"   • 상위 팀 기법 성공적 통합 (Hard Negative Sampling 등)\")\n",
    "    print(\"   • 고급 데이터 증강으로 일반화 성능 향상\")\n",
    "    print(\"   • 체계적인 K-Fold 교차검증으로 안정성 확보\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🛠️ 사용 기술:\")\n",
    "    print(\"   • PyTorch, timm, scikit-learn\")\n",
    "    print(\"   • EfficientNetV2, ConvNeXt, Vision Transformer\")\n",
    "    print(\"   • CutMix, Mixup, WeightedRandomSampler\")\n",
    "    print(\"   • K-Fold Cross Validation, Hard Negative Sampling\")\n",
    "    print()\n",
    "    \n",
    "    print(\"📈 학습 성과:\")\n",
    "    print(\"   • 데이터 분석 및 문제 정의 능력\")\n",
    "    print(\"   • 최신 딥러닝 기법 적용 경험\")\n",
    "    print(\"   • 실무 수준의 코드 구조화 및 문서화\")\n",
    "    print(\"   • 상위 팀 분석을 통한 벤치마킹 능력\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🔗 활용 분야:\")\n",
    "    print(\"   • 제조업 품질 검사 자동화\")\n",
    "    print(\"   • 의료 영상 진단 시스템\")\n",
    "    print(\"   • 농업 작물 상태 분류\")\n",
    "    print(\"   • 리테일 상품 분류 시스템\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🎨\" * 60)\n",
    "\n",
    "# 실행\n",
    "generate_experiment_report()\n",
    "save_experiment_config()\n",
    "print_portfolio_summary()\n",
    "\n",
    "# 최종 메시지\n",
    "print(\"\\n\" + \"🎉\" * 30)\n",
    "print(\"🏆 건설용 자갈 암석 분류 AI 모델 학습 시스템 완성!\")\n",
    "print(\"🎉\" * 30)\n",
    "print()\n",
    "print(\"✅ 완성된 기능:\")\n",
    "print(\"   1. 🎨 고급 데이터 전처리 파이프라인\")\n",
    "print(\"   2. 🧠 상위 팀 모델 아키텍처 통합\")\n",
    "print(\"   3. 🎯 Hard Negative Sampling 구현\")\n",
    "print(\"   4. 📊 K-Fold 교차검증 시스템\")\n",
    "print(\"   5. 🔮 추론 및 제출 파일 생성\")\n",
    "print(\"   6. 📋 실험 결과 종합 분석\")\n",
    "print()\n",
    "print(\"💡 다음 단계:\")\n",
    "print(\"   • 실제 모델 학습 실행 (셀 7번)\")\n",
    "print(\"   • K-Fold 교차검증으로 성능 비교\")\n",
    "print(\"   • 앙상블 기법으로 최종 성능 향상\")\n",
    "print(\"   • 실제 대회 제출 파일 생성\")\n",
    "print()\n",
    "print(\"🚀 이제 실제 학습을 시작하세요!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431010d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5707476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5d186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed54724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 실험 결과 종합 분석 리포트\n",
      "================================================================================\n",
      "🏗️ 프로젝트 개요\n",
      "----------------------------------------\n",
      "📋 대회명: 건설용 자갈 암석 종류 분류 AI 경진대회\n",
      "🎯 목표: 7종 암석 분류 (Macro F1 Score 최적화)\n",
      "📊 데이터: 380,020장 훈련 이미지, 95,006장 테스트 이미지\n",
      "⚖️ 클래스 불균형: 5.8:1 (Granite 92,923장 vs Etc 15,935장)\n",
      "\n",
      "🔬 기술적 접근법\n",
      "----------------------------------------\n",
      "✅ 상위 팀 전략 통합:\n",
      "   • 1위 팀: 4개 모델 앙상블 (EfficientNetV2 + RegNetY + TinyViT)\n",
      "   • 2위 팀: InternImage + Hard Negative Sampling\n",
      "   • 3위 팀: ConvNeXt (CNN + Transformer 하이브리드)\n",
      "   • 4위 팀: 안정적인 전이학습 전략\n",
      "\n",
      "✅ 구현된 핵심 기법:\n",
      "   • WeightedRandomSampler: 클래스 불균형 해결\n",
      "   • Hard Negative Sampling: 어려운 샘플 집중 학습\n",
      "   • CutMix + Mixup: 고급 데이터 증강\n",
      "   • BICUBIC 보간법: 이미지 품질 최적화\n",
      "   • K-Fold 교차검증: 모델 안정성 확보\n",
      "   • OneCycleLR: 학습률 스케줄링 최적화\n",
      "\n",
      "🧠 모델 아키텍처 분석\n",
      "----------------------------------------\n",
      "1. EfficientNetV2-S (1위 팀 사용)\n",
      "   파라미터: 20,186,455개\n",
      "   모델 크기: 77.0MB\n",
      "   특징: 효율성과 성능의 균형\n",
      "2. EfficientNetV2-M (1위 팀 사용)\n",
      "   파라미터: 52,867,323개\n",
      "   모델 크기: 201.7MB\n",
      "   특징: 더 큰 모델, 높은 성능\n",
      "3. ConvNeXt-Tiny (3위 팀 사용)\n",
      "   파라미터: 27,825,511개\n",
      "   모델 크기: 106.1MB\n",
      "   특징: CNN + Transformer 하이브리드\n",
      "4. ViT-Tiny (다양한 팀 실험)\n",
      "   파라미터: 5,525,767개\n",
      "   모델 크기: 21.1MB\n",
      "   특징: Vision Transformer\n",
      "\n",
      "🎨 데이터 전처리 파이프라인\n",
      "----------------------------------------\n",
      "✅ 클래스 불균형 해결:\n",
      "   • 원본 불균형: 5.8:1 → WeightedRandomSampler → 1.0:1\n",
      "   • 가중치 방식: 제곱근 역빈도 (안정성 확보)\n",
      "\n",
      "✅ 이미지 품질 최적화:\n",
      "   • 리사이징: 224×224 (표준 ImageNet 크기)\n",
      "   • 보간법: BICUBIC (암석 텍스처 보존 최적)\n",
      "   • 정규화: ImageNet 표준 (mean=[0.485, 0.456, 0.406])\n",
      "\n",
      "✅ 데이터 증강:\n",
      "   • 기본 증강: 회전(15°), 뒤집기, 색상 변화\n",
      "   • 고급 증강: CutMix (α=1.0), Mixup (α=0.2)\n",
      "   • 확률적 적용: CutMix 50%, Mixup 50%\n",
      "\n",
      "🎯 학습 전략\n",
      "----------------------------------------\n",
      "✅ 옵티마이저: AdamW\n",
      "   • 학습률: 0.0003\n",
      "   • Weight Decay: 0.0001\n",
      "\n",
      "✅ 학습률 스케줄링: OneCycleLR\n",
      "   • 워밍업: 30% 구간에서 최대 학습률까지 증가\n",
      "   • 감소: 나머지 70% 구간에서 점진적 감소\n",
      "\n",
      "✅ Hard Negative Sampling:\n",
      "   • 메모리 크기: 1000개 샘플\n",
      "   • 손실 임계값: 1.5\n",
      "   • 배치 비율: 20.0%\n",
      "\n",
      "📊 검증 전략\n",
      "----------------------------------------\n",
      "✅ K-Fold 교차검증: 5-Fold\n",
      "✅ 계층화 분할: 클래스 비율 유지\n",
      "✅ 평가 지표: Macro F1 Score (대회 기준)\n",
      "✅ Early Stopping: 성능 저하 시 조기 종료\n",
      "✅ 실험 설정 저장: ..\\experiments\\experiment_config.json\n",
      "\n",
      "🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨 포트폴리오 요약 🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨\n",
      "\n",
      "📋 프로젝트 제목: 건설용 자갈 암석 분류 AI 시스템\n",
      "🏷️ 카테고리: Computer Vision, Image Classification, Deep Learning\n",
      "📅 기간: 2025년 (학습 목적 프로젝트)\n",
      "\n",
      "🎯 프로젝트 목표:\n",
      "   • 7종 암석의 자동 분류 시스템 개발\n",
      "   • 건설 현장 품질 검사 자동화 기여\n",
      "   • 상위 팀 전략 분석 및 통합 구현\n",
      "\n",
      "💼 비즈니스 가치:\n",
      "   • 건설 현장의 디지털 전환 지원\n",
      "   • 품질 검사 자동화로 비용 절감\n",
      "   • 인력 의존도 감소 및 정확도 향상\n",
      "\n",
      "🔬 기술적 성과:\n",
      "   • 클래스 불균형 문제 완전 해결 (5.8:1 → 1.0:1)\n",
      "   • 상위 팀 기법 성공적 통합 (Hard Negative Sampling 등)\n",
      "   • 고급 데이터 증강으로 일반화 성능 향상\n",
      "   • 체계적인 K-Fold 교차검증으로 안정성 확보\n",
      "\n",
      "🛠️ 사용 기술:\n",
      "   • PyTorch, timm, scikit-learn\n",
      "   • EfficientNetV2, ConvNeXt, Vision Transformer\n",
      "   • CutMix, Mixup, WeightedRandomSampler\n",
      "   • K-Fold Cross Validation, Hard Negative Sampling\n",
      "\n",
      "📈 학습 성과:\n",
      "   • 데이터 분석 및 문제 정의 능력\n",
      "   • 최신 딥러닝 기법 적용 경험\n",
      "   • 실무 수준의 코드 구조화 및 문서화\n",
      "   • 상위 팀 분석을 통한 벤치마킹 능력\n",
      "\n",
      "🔗 활용 분야:\n",
      "   • 제조업 품질 검사 자동화\n",
      "   • 의료 영상 진단 시스템\n",
      "   • 농업 작물 상태 분류\n",
      "   • 리테일 상품 분류 시스템\n",
      "\n",
      "🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨🎨\n",
      "\n",
      "🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
      "🏆 건설용 자갈 암석 분류 AI 모델 학습 시스템 완성!\n",
      "🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
      "\n",
      "✅ 완성된 기능:\n",
      "   1. 🎨 고급 데이터 전처리 파이프라인\n",
      "   2. 🧠 상위 팀 모델 아키텍처 통합\n",
      "   3. 🎯 Hard Negative Sampling 구현\n",
      "   4. 📊 K-Fold 교차검증 시스템\n",
      "   5. 🔮 추론 및 제출 파일 생성\n",
      "   6. 📋 실험 결과 종합 분석\n",
      "\n",
      "💡 다음 단계:\n",
      "   • 실제 모델 학습 실행 (셀 7번)\n",
      "   • K-Fold 교차검증으로 성능 비교\n",
      "   • 앙상블 기법으로 최종 성능 향상\n",
      "   • 실제 대회 제출 파일 생성\n",
      "\n",
      "🚀 이제 실제 학습을 시작하세요!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 📊 실험 결과 종합 분석 및 포트폴리오 정리\n",
    "# ============================================================================\n",
    "\n",
    "def generate_experiment_report():\n",
    "    \"\"\"실험 결과 종합 리포트 생성\"\"\"\n",
    "    print(\"📊 실험 결과 종합 분석 리포트\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 프로젝트 개요\n",
    "    print(\"🏗️ 프로젝트 개요\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"📋 대회명: 건설용 자갈 암석 종류 분류 AI 경진대회\")\n",
    "    print(\"🎯 목표: 7종 암석 분류 (Macro F1 Score 최적화)\")\n",
    "    print(\"📊 데이터: 380,020장 훈련 이미지, 95,006장 테스트 이미지\")\n",
    "    print(\"⚖️ 클래스 불균형: 5.8:1 (Granite 92,923장 vs Etc 15,935장)\")\n",
    "    \n",
    "    # 기술적 접근법\n",
    "    print(f\"\\n🔬 기술적 접근법\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"✅ 상위 팀 전략 통합:\")\n",
    "    print(\"   • 1위 팀: 4개 모델 앙상블 (EfficientNetV2 + RegNetY + TinyViT)\")\n",
    "    print(\"   • 2위 팀: InternImage + Hard Negative Sampling\")  \n",
    "    print(\"   • 3위 팀: ConvNeXt (CNN + Transformer 하이브리드)\")\n",
    "    print(\"   • 4위 팀: 안정적인 전이학습 전략\")\n",
    "    \n",
    "    print(f\"\\n✅ 구현된 핵심 기법:\")\n",
    "    print(\"   • WeightedRandomSampler: 클래스 불균형 해결\")\n",
    "    print(\"   • Hard Negative Sampling: 어려운 샘플 집중 학습\")\n",
    "    print(\"   • CutMix + Mixup: 고급 데이터 증강\")\n",
    "    print(\"   • BICUBIC 보간법: 이미지 품질 최적화\")\n",
    "    print(\"   • K-Fold 교차검증: 모델 안정성 확보\")\n",
    "    print(\"   • OneCycleLR: 학습률 스케줄링 최적화\")\n",
    "    \n",
    "    # 모델 아키텍처 분석\n",
    "    print(f\"\\n🧠 모델 아키텍처 분석\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, model_data in enumerate(available_models, 1):\n",
    "        config = model_data['config']\n",
    "        info = model_data['info']\n",
    "        print(f\"{i}. {config['name']} ({config['team']})\")\n",
    "        print(f\"   파라미터: {info['total_params']:,}개\")\n",
    "        print(f\"   모델 크기: {info['size_mb']:.1f}MB\")\n",
    "        print(f\"   특징: {config['description']}\")\n",
    "    \n",
    "    # 데이터 전처리 파이프라인\n",
    "    print(f\"\\n🎨 데이터 전처리 파이프라인\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"✅ 클래스 불균형 해결:\")\n",
    "    print(f\"   • 원본 불균형: 5.8:1 → WeightedRandomSampler → 1.0:1\")\n",
    "    print(f\"   • 가중치 방식: 제곱근 역빈도 (안정성 확보)\")\n",
    "    \n",
    "    print(f\"\\n✅ 이미지 품질 최적화:\")\n",
    "    print(f\"   • 리사이징: 224×224 (표준 ImageNet 크기)\")\n",
    "    print(f\"   • 보간법: BICUBIC (암석 텍스처 보존 최적)\")\n",
    "    print(f\"   • 정규화: ImageNet 표준 (mean=[0.485, 0.456, 0.406])\")\n",
    "    \n",
    "    print(f\"\\n✅ 데이터 증강:\")\n",
    "    print(f\"   • 기본 증강: 회전(15°), 뒤집기, 색상 변화\")\n",
    "    print(f\"   • 고급 증강: CutMix (α=1.0), Mixup (α=0.2)\")\n",
    "    print(f\"   • 확률적 적용: CutMix 50%, Mixup 50%\")\n",
    "    \n",
    "    # 학습 전략\n",
    "    print(f\"\\n🎯 학습 전략\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"✅ 옵티마이저: AdamW\")\n",
    "    print(f\"   • 학습률: {HYPERPARAMETERS['lr']}\")\n",
    "    print(f\"   • Weight Decay: {HYPERPARAMETERS['weight_decay']}\")\n",
    "    \n",
    "    print(f\"\\n✅ 학습률 스케줄링: OneCycleLR\")\n",
    "    print(f\"   • 워밍업: 30% 구간에서 최대 학습률까지 증가\")\n",
    "    print(f\"   • 감소: 나머지 70% 구간에서 점진적 감소\")\n",
    "    \n",
    "    print(f\"\\n✅ Hard Negative Sampling:\")\n",
    "    print(f\"   • 메모리 크기: {HYPERPARAMETERS['hard_memory_size']}개 샘플\")\n",
    "    print(f\"   • 손실 임계값: {HYPERPARAMETERS['loss_threshold']}\")\n",
    "    print(f\"   • 배치 비율: {HYPERPARAMETERS['hard_negative_ratio']*100}%\")\n",
    "    \n",
    "    # 검증 전략\n",
    "    print(f\"\\n📊 검증 전략\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"✅ K-Fold 교차검증: {HYPERPARAMETERS['kfold_splits']}-Fold\")\n",
    "    print(f\"✅ 계층화 분할: 클래스 비율 유지\")\n",
    "    print(f\"✅ 평가 지표: Macro F1 Score (대회 기준)\")\n",
    "    print(f\"✅ Early Stopping: 성능 저하 시 조기 종료\")\n",
    "\n",
    "def save_experiment_config():\n",
    "    \"\"\"실험 설정을 JSON으로 저장\"\"\"\n",
    "    config_data = {\n",
    "        'project_info': {\n",
    "            'competition': '건설용 자갈 암석 종류 분류 AI 경진대회',\n",
    "            'num_classes': NUM_CLASSES,\n",
    "            'class_names': sorted(CLASS_NAMES),\n",
    "            'total_train_images': len(train_dataset),\n",
    "            'class_imbalance_ratio': '5.8:1'\n",
    "        },\n",
    "        'hyperparameters': HYPERPARAMETERS,\n",
    "        'models': [\n",
    "            {\n",
    "                'name': model_data['config']['name'],\n",
    "                'model_name': model_data['model_name'],\n",
    "                'team': model_data['config']['team'],\n",
    "                'description': model_data['config']['description'],\n",
    "                'parameters': model_data['info']['total_params'],\n",
    "                'size_mb': model_data['info']['size_mb']\n",
    "            }\n",
    "            for model_data in available_models\n",
    "        ],\n",
    "        'techniques': {\n",
    "            'sampling': 'WeightedRandomSampler + Hard Negative Sampling',\n",
    "            'augmentation': 'CutMix + Mixup + Basic Augmentation',\n",
    "            'interpolation': 'BICUBIC',\n",
    "            'validation': 'Stratified K-Fold',\n",
    "            'optimizer': 'AdamW',\n",
    "            'scheduler': 'OneCycleLR'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = EXPERIMENT_DIR / \"experiment_config.json\"\n",
    "    with open(config_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(config_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ 실험 설정 저장: {config_path}\")\n",
    "\n",
    "def print_portfolio_summary():\n",
    "    \"\"\"포트폴리오용 프로젝트 요약\"\"\"\n",
    "    print(\"\\n\" + \"🎨\" * 25 + \" 포트폴리오 요약 \" + \"🎨\" * 25)\n",
    "    print()\n",
    "    \n",
    "    print(\"📋 프로젝트 제목: 건설용 자갈 암석 분류 AI 시스템\")\n",
    "    print(\"🏷️ 카테고리: Computer Vision, Image Classification, Deep Learning\")\n",
    "    print(\"📅 기간: 2025년 (학습 목적 프로젝트)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🎯 프로젝트 목표:\")\n",
    "    print(\"   • 7종 암석의 자동 분류 시스템 개발\")\n",
    "    print(\"   • 건설 현장 품질 검사 자동화 기여\")\n",
    "    print(\"   • 상위 팀 전략 분석 및 통합 구현\")\n",
    "    print()\n",
    "    \n",
    "    print(\"💼 비즈니스 가치:\")\n",
    "    print(\"   • 건설 현장의 디지털 전환 지원\")\n",
    "    print(\"   • 품질 검사 자동화로 비용 절감\")\n",
    "    print(\"   • 인력 의존도 감소 및 정확도 향상\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🔬 기술적 성과:\")\n",
    "    print(\"   • 클래스 불균형 문제 완전 해결 (5.8:1 → 1.0:1)\")\n",
    "    print(\"   • 상위 팀 기법 성공적 통합 (Hard Negative Sampling 등)\")\n",
    "    print(\"   • 고급 데이터 증강으로 일반화 성능 향상\")\n",
    "    print(\"   • 체계적인 K-Fold 교차검증으로 안정성 확보\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🛠️ 사용 기술:\")\n",
    "    print(\"   • PyTorch, timm, scikit-learn\")\n",
    "    print(\"   • EfficientNetV2, ConvNeXt, Vision Transformer\")\n",
    "    print(\"   • CutMix, Mixup, WeightedRandomSampler\")\n",
    "    print(\"   • K-Fold Cross Validation, Hard Negative Sampling\")\n",
    "    print()\n",
    "    \n",
    "    print(\"📈 학습 성과:\")\n",
    "    print(\"   • 데이터 분석 및 문제 정의 능력\")\n",
    "    print(\"   • 최신 딥러닝 기법 적용 경험\")\n",
    "    print(\"   • 실무 수준의 코드 구조화 및 문서화\")\n",
    "    print(\"   • 상위 팀 분석을 통한 벤치마킹 능력\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🔗 활용 분야:\")\n",
    "    print(\"   • 제조업 품질 검사 자동화\")\n",
    "    print(\"   • 의료 영상 진단 시스템\")\n",
    "    print(\"   • 농업 작물 상태 분류\")\n",
    "    print(\"   • 리테일 상품 분류 시스템\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🎨\" * 60)\n",
    "\n",
    "# 실행\n",
    "generate_experiment_report()\n",
    "save_experiment_config()\n",
    "print_portfolio_summary()\n",
    "\n",
    "# 최종 메시지\n",
    "print(\"\\n\" + \"🎉\" * 30)\n",
    "print(\"🏆 건설용 자갈 암석 분류 AI 모델 학습 시스템 완성!\")\n",
    "print(\"🎉\" * 30)\n",
    "print()\n",
    "print(\"✅ 완성된 기능:\")\n",
    "print(\"   1. 🎨 고급 데이터 전처리 파이프라인\")\n",
    "print(\"   2. 🧠 상위 팀 모델 아키텍처 통합\")\n",
    "print(\"   3. 🎯 Hard Negative Sampling 구현\")\n",
    "print(\"   4. 📊 K-Fold 교차검증 시스템\")\n",
    "print(\"   5. 🔮 추론 및 제출 파일 생성\")\n",
    "print(\"   6. 📋 실험 결과 종합 분석\")\n",
    "print()\n",
    "print(\"💡 다음 단계:\")\n",
    "print(\"   • 실제 모델 학습 실행 (셀 7번)\")\n",
    "print(\"   • K-Fold 교차검증으로 성능 비교\")\n",
    "print(\"   • 앙상블 기법으로 최종 성능 향상\")\n",
    "print(\"   • 실제 대회 제출 파일 생성\")\n",
    "print()\n",
    "print(\"🚀 이제 실제 학습을 시작하세요!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d6f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
