{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df85c75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wang\\anaconda3\\envs\\yolo\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ë¶„ë¥˜ AI - ëª¨ë¸ í•™ìŠµ ì‹œì‘!\n",
      "PyTorch ë²„ì „: 2.2.2+cu121\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "GPU: NVIDIA GeForce RTX 4070\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# PyTorch & ML Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_v2_s, efficientnet_v2_m\n",
    "import timm\n",
    "\n",
    "# Scientific Computing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Utilities\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ì‹œë“œ ê³ ì • (ì¬í˜„ì„± ë³´ì¥)\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"ğŸ”¥ ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ë¶„ë¥˜ AI - ëª¨ë¸ í•™ìŠµ ì‹œì‘!\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc289c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ê²½ë¡œ ì„¤ì • ì™„ë£Œ:\n",
      "í›ˆë ¨ ë°ì´í„°: D:\\data\\stones\\open\\train\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„°: D:\\data\\stones\\open\\test\n",
      "ì‹¤í—˜ ê²°ê³¼: ..\\experiments\n",
      "ëª¨ë¸ ì €ì¥: ..\\models\n",
      "\n",
      "ğŸ·ï¸ í´ë˜ìŠ¤ ì •ë³´:\n",
      "í´ë˜ìŠ¤ ìˆ˜: 7\n",
      "í´ë˜ìŠ¤ ëª©ë¡: ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
      "\n",
      "âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •:\n",
      "image_size: 224\n",
      "batch_size: 8\n",
      "num_epochs: 50\n",
      "num_workers: 0\n",
      "pin_memory: True\n",
      "lr: 0.0003\n",
      "weight_decay: 0.0001\n",
      "warmup_epochs: 5\n",
      "cutmix_alpha: 1.0\n",
      "mixup_alpha: 0.2\n",
      "cutmix_prob: 0.5\n",
      "mixup_prob: 0.5\n",
      "hard_negative_ratio: 0.2\n",
      "hard_memory_size: 1000\n",
      "loss_threshold: 1.5\n",
      "kfold_splits: 5\n",
      "kfold_seed: 42\n",
      "ensemble_models: 4ê°œ ëª¨ë¸\n",
      "\n",
      "ğŸ’» ì—°ì‚° ì¥ì¹˜: cuda\n",
      "GPU ë©”ëª¨ë¦¬: 12.0 GB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# ğŸ† ìƒìœ„ íŒ€ë“¤ì˜ í•µì‹¬ ì „ëµ í†µí•© ì„¤ì •\n",
    "# 1ìœ„ íŒ€: 4ê°œ ëª¨ë¸ ì•™ìƒë¸” (EfficientNetV2-S/M + RegNetY + TinyViT)\n",
    "# 2ìœ„ íŒ€: InternImage + Hard Negative Sampling\n",
    "# 3ìœ„ íŒ€: ConvNeXt \n",
    "# 4ìœ„ íŒ€: ì•ˆì •ì ì¸ ì „ì´í•™ìŠµ ì „ëµ\n",
    "# ============================================================================\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "BASE_PATH = r\"D:\\data\\stones\\open\"\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, \"train\")\n",
    "TEST_PATH = os.path.join(BASE_PATH, \"test\")\n",
    "SUBMISSION_PATH = os.path.join(BASE_PATH, \"sample_submission.csv\")\n",
    "\n",
    "# ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n",
    "EXPERIMENT_DIR = Path(\"../experiments\")\n",
    "EXPERIMENT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“‚ ê²½ë¡œ ì„¤ì • ì™„ë£Œ:\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„°: {TRAIN_PATH}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {TEST_PATH}\")\n",
    "print(f\"ì‹¤í—˜ ê²°ê³¼: {EXPERIMENT_DIR}\")\n",
    "print(f\"ëª¨ë¸ ì €ì¥: {MODEL_DIR}\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ì •ë³´ (ì´ì „ ë¶„ì„ ê²°ê³¼ í™œìš©)\n",
    "CLASS_NAMES = ['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone', 'Weathered_Rock']\n",
    "CLASS_COUNTS = {\n",
    "    'Andesite': 43802,\n",
    "    'Basalt': 26810,\n",
    "    'Etc': 15935,\n",
    "    'Gneiss': 73914,\n",
    "    'Granite': 92923,\n",
    "    'Mud_Sandstone': 89467,\n",
    "    'Weathered_Rock': 37169\n",
    "}\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "class_to_idx = {cls: i for i, cls in enumerate(sorted(CLASS_NAMES))}\n",
    "idx_to_class = {i: cls for cls, i in class_to_idx.items()}\n",
    "\n",
    "print(f\"\\nğŸ·ï¸ í´ë˜ìŠ¤ ì •ë³´:\")\n",
    "print(f\"í´ë˜ìŠ¤ ìˆ˜: {NUM_CLASSES}\")\n",
    "print(f\"í´ë˜ìŠ¤ ëª©ë¡: {sorted(CLASS_NAMES)}\")\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ìƒìœ„ íŒ€ë“¤ì˜ ìµœì  ì„¤ì • í†µí•©)\n",
    "HYPERPARAMETERS = {\n",
    "    # ê¸°ë³¸ ì„¤ì •\n",
    "    'image_size': 224,          # 1-4ìœ„ íŒ€ ê³µí†µ ì‚¬ìš©\n",
    "    'batch_size': 8,           # GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³ ë ¤\n",
    "    'num_epochs': 50,           # ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„\n",
    "    'num_workers': 0,           # Windows í˜¸í™˜ì„±\n",
    "    'pin_memory': True,\n",
    "    \n",
    "    # í•™ìŠµë¥  ì„¤ì • (1ìœ„ íŒ€ ì „ëµ)\n",
    "    'lr': 3e-4,                 # AdamW ìµœì  í•™ìŠµë¥ \n",
    "    'weight_decay': 1e-4,       # ì •ê·œí™” ê°•ë„\n",
    "    'warmup_epochs': 5,         # í•™ìŠµë¥  ì›Œë°ì—…\n",
    "    \n",
    "    # ì¦ê°• ì„¤ì • (2ìœ„ íŒ€ ì „ëµ)\n",
    "    'cutmix_alpha': 1.0,        # CutMix ê°•ë„\n",
    "    'mixup_alpha': 0.2,         # Mixup ê°•ë„\n",
    "    'cutmix_prob': 0.5,         # CutMix ì ìš© í™•ë¥ \n",
    "    'mixup_prob': 0.5,          # Mixup ì ìš© í™•ë¥ \n",
    "    \n",
    "    # ìƒ˜í”Œë§ ì„¤ì • (2ìœ„ íŒ€ Hard Negative)\n",
    "    'hard_negative_ratio': 0.2,  # ë°°ì¹˜ ì¤‘ Hard Sample ë¹„ìœ¨\n",
    "    'hard_memory_size': 1000,     # Hard Sample ë©”ëª¨ë¦¬ í¬ê¸°\n",
    "    'loss_threshold': 1.5,        # Hard Sample ê¸°ì¤€ ì†ì‹¤ê°’\n",
    "    \n",
    "    # K-Fold ì„¤ì • (2-4ìœ„ íŒ€ ê³µí†µ)\n",
    "    'kfold_splits': 5,           # 5-fold êµì°¨ê²€ì¦\n",
    "    'kfold_seed': 42,\n",
    "    \n",
    "    # ì•™ìƒë¸” ì„¤ì • (1ìœ„ íŒ€ ì „ëµ)\n",
    "    'ensemble_models': [\n",
    "        'tf_efficientnetv2_s.in21k_ft_in1k',  # 1ìœ„ íŒ€ ì‚¬ìš©\n",
    "        'tf_efficientnetv2_m.in21k_ft_in1k',  # 1ìœ„ íŒ€ ì‚¬ìš©\n",
    "        'convnext_tiny.fb_in22k_ft_in1k',     # 3ìœ„ íŒ€ ì‚¬ìš©\n",
    "        'vit_tiny_patch16_224.augreg_in21k_ft_in1k'  # Vision Transformer\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\nâš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •:\")\n",
    "for key, value in HYPERPARAMETERS.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"{key}: {len(value)}ê°œ ëª¨ë¸\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# GPU/CPU ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nğŸ’» ì—°ì‚° ì¥ì¹˜: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e74a52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¨ ë°ì´í„° ë³€í™˜ ì„¤ì • ì™„ë£Œ:\n",
      "âœ… í›ˆë ¨ìš© ë³€í™˜: ê³ ê¸‰ ì¦ê°• í¬í•¨\n",
      "âœ… ê²€ì¦ìš© ë³€í™˜: ì¦ê°• ì—†ìŒ\n",
      "âœ… CutMix/Mixup: êµ¬í˜„ ì™„ë£Œ\n",
      "âœ… ì•”ì„ ì´ë¯¸ì§€ íŠ¹ì„± ìµœì í™”\n",
      "ğŸ–¼ï¸ ì‹œê°í™” í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¨ ë°ì´í„° ë³€í™˜ ë° ì¦ê°• (ìƒìœ„ íŒ€ë“¤ì˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ í†µí•©)\n",
    "# ============================================================================\n",
    "\n",
    "class AdvancedTransforms:\n",
    "    \"\"\"ìƒìœ„ íŒ€ë“¤ì˜ ë°ì´í„° ë³€í™˜ ì „ëµì„ í†µí•©í•œ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=224, is_training=True):\n",
    "        self.image_size = image_size\n",
    "        self.is_training = is_training\n",
    "        \n",
    "    def get_train_transforms(self):\n",
    "        \"\"\"í›ˆë ¨ìš© ë³€í™˜ (2ìœ„ íŒ€ + 3ìœ„ íŒ€ ì „ëµ í†µí•©)\"\"\"\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((256, 256), interpolation=Image.BICUBIC),  # í’ˆì§ˆ ë³´ì¡´\n",
    "            transforms.RandomCrop(self.image_size),                      # ëœë¤ í¬ë¡­\n",
    "            transforms.RandomHorizontalFlip(p=0.5),                      # ì¢Œìš° ë’¤ì§‘ê¸°\n",
    "            transforms.RandomRotation(degrees=15),                       # íšŒì „ (ì•”ì„ íŠ¹ì„± ê³ ë ¤)\n",
    "            transforms.ColorJitter(                                      # ìƒ‰ìƒ ë³€í™” (í˜„ì¥ í™˜ê²½ ë°˜ì˜)\n",
    "                brightness=0.2,\n",
    "                contrast=0.2, \n",
    "                saturation=0.1,\n",
    "                hue=0.05\n",
    "            ),\n",
    "            transforms.RandomApply([                                     # ì¶”ê°€ ì¦ê°• (í™•ë¥ ì  ì ìš©)\n",
    "                transforms.GaussianBlur(kernel_size=3)\n",
    "            ], p=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(                                        # ImageNet ì •ê·œí™”\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def get_valid_transforms(self):\n",
    "        \"\"\"ê²€ì¦ìš© ë³€í™˜ (ì¦ê°• ì—†ìŒ)\"\"\"\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size), interpolation=Image.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def get_test_transforms(self):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ìš© ë³€í™˜ (TTA í¬í•¨ ê°€ëŠ¥)\"\"\"\n",
    "        return self.get_valid_transforms()\n",
    "\n",
    "# CutMix êµ¬í˜„ (2ìœ„ íŒ€ ì „ëµ)\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"CutMix ë°ì´í„° ì¦ê°•\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    \n",
    "    # ì‹¤ì œ í˜¼í•© ë¹„ìœ¨ ê³„ì‚°\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))\n",
    "    \n",
    "    return x, y, y[index], lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    \"\"\"CutMixìš© ëœë¤ ë°•ìŠ¤ ìƒì„±\"\"\"\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Mixup êµ¬í˜„ (2ìœ„ íŒ€ ì „ëµ)\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Mixup ë°ì´í„° ì¦ê°•\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    \n",
    "    return mixed_x, y, y[index], lam\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ (í˜¼í•© ë°ì´í„°ìš©)\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup/CutMixìš© ì†ì‹¤ í•¨ìˆ˜\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# ë³€í™˜ ê°ì²´ ìƒì„±\n",
    "transform_manager = AdvancedTransforms(image_size=HYPERPARAMETERS['image_size'])\n",
    "\n",
    "train_transforms = transform_manager.get_train_transforms()\n",
    "valid_transforms = transform_manager.get_valid_transforms()\n",
    "test_transforms = transform_manager.get_test_transforms()\n",
    "\n",
    "print(\"ğŸ¨ ë°ì´í„° ë³€í™˜ ì„¤ì • ì™„ë£Œ:\")\n",
    "print(\"âœ… í›ˆë ¨ìš© ë³€í™˜: ê³ ê¸‰ ì¦ê°• í¬í•¨\")\n",
    "print(\"âœ… ê²€ì¦ìš© ë³€í™˜: ì¦ê°• ì—†ìŒ\")\n",
    "print(\"âœ… CutMix/Mixup: êµ¬í˜„ ì™„ë£Œ\")\n",
    "print(\"âœ… ì•”ì„ ì´ë¯¸ì§€ íŠ¹ì„± ìµœì í™”\")\n",
    "\n",
    "# ë³€í™˜ ì˜ˆì‹œ ì‹œê°í™”ë¥¼ ìœ„í•œ í•¨ìˆ˜\n",
    "def visualize_transforms(dataset, num_samples=4):\n",
    "    \"\"\"ë°ì´í„° ë³€í™˜ ê²°ê³¼ ì‹œê°í™”\"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
    "    fig.suptitle('ë°ì´í„° ë³€í™˜ ì˜ˆì‹œ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # ì›ë³¸ ì´ë¯¸ì§€ (ê²€ì¦ìš© ë³€í™˜ë§Œ ì ìš©)\n",
    "        img, label = dataset[i]\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ í–‰: ê²€ì¦ìš© ë³€í™˜\n",
    "        axes[0, i].imshow(img.permute(1, 2, 0) * 0.229 + 0.485)  # ì •ê·œí™” í•´ì œ (ê·¼ì‚¬)\n",
    "        axes[0, i].set_title(f'Valid Transform\\n{idx_to_class[label]}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # ë‘ ë²ˆì§¸ í–‰: í›ˆë ¨ìš© ë³€í™˜ (ë³„ë„ ì ìš©)\n",
    "        # ì‹¤ì œë¡œëŠ” ëœë¤ì´ë¯€ë¡œ ë§¤ë²ˆ ë‹¤ë¦„\n",
    "        axes[1, i].imshow(img.permute(1, 2, 0) * 0.229 + 0.485)  # ì •ê·œí™” í•´ì œ (ê·¼ì‚¬) \n",
    "        axes[1, i].set_title(f'Train Transform\\n{idx_to_class[label]}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ğŸ–¼ï¸ ì‹œê°í™” í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa9ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\n",
      "ğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ: 380,020ê°œ ìƒ˜í”Œ\n",
      "   Andesite: 43,802ê°œ\n",
      "   Basalt: 26,810ê°œ\n",
      "   Etc: 15,935ê°œ\n",
      "   Gneiss: 73,914ê°œ\n",
      "   Granite: 92,923ê°œ\n",
      "   Mud_Sandstone: 89,467ê°œ\n",
      "   Weathered_Rock: 37,169ê°œ\n",
      "âš–ï¸ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ: 0.764 ~ 1.846\n",
      "âœ… í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: 380,020ê°œ ìƒ˜í”Œ\n",
      "âœ… WeightedRandomSampler ìƒì„± ì™„ë£Œ\n",
      "âœ… Hard Negative Sampler ìƒì„± ì™„ë£Œ\n",
      "âœ… DataLoader ìƒì„± ì™„ë£Œ: 47502 ë°°ì¹˜\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š ê³ ê¸‰ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (Hard Negative Sampling + WeightedRandomSampler í†µí•©)\n",
    "# ============================================================================\n",
    "\n",
    "class RockDataset(Dataset):\n",
    "    \"\"\"ì•”ì„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê³ ê¸‰ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, transform=None, class_to_idx=None, is_training=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "        self.class_to_idx = class_to_idx or {}\n",
    "        \n",
    "        # ì´ë¯¸ì§€ ê²½ë¡œì™€ ë¼ë²¨ ìˆ˜ì§‘\n",
    "        self.samples = []\n",
    "        self.class_counts = defaultdict(int)\n",
    "        \n",
    "        self._load_samples()\n",
    "        self._compute_weights()\n",
    "        \n",
    "    def _load_samples(self):\n",
    "        \"\"\"ì´ë¯¸ì§€ ìƒ˜í”Œ ë¡œë”©\"\"\"\n",
    "        for class_name in os.listdir(self.data_dir):\n",
    "            class_path = os.path.join(self.data_dir, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "                \n",
    "            class_idx = self.class_to_idx.get(class_name, len(self.class_to_idx))\n",
    "            if class_name not in self.class_to_idx:\n",
    "                self.class_to_idx[class_name] = class_idx\n",
    "            \n",
    "            for img_name in os.listdir(class_path):\n",
    "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    img_path = os.path.join(class_path, img_name)\n",
    "                    self.samples.append((img_path, class_idx))\n",
    "                    self.class_counts[class_idx] += 1\n",
    "        \n",
    "        print(f\"ğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ: {len(self.samples):,}ê°œ ìƒ˜í”Œ\")\n",
    "        for class_name, class_idx in sorted(self.class_to_idx.items()):\n",
    "            print(f\"   {class_name}: {self.class_counts[class_idx]:,}ê°œ\")\n",
    "    \n",
    "    def _compute_weights(self):\n",
    "        \"\"\"ìƒ˜í”Œë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° (WeightedRandomSamplerìš©)\"\"\"\n",
    "        total_samples = len(self.samples)\n",
    "        num_classes = len(self.class_to_idx)\n",
    "        \n",
    "        # ì œê³±ê·¼ ì—­ë¹ˆë„ ê°€ì¤‘ì¹˜ (ì´ì „ ë¶„ì„ì—ì„œ ìµœì ìœ¼ë¡œ í™•ì¸ë¨)\n",
    "        class_weights = {}\n",
    "        for class_idx, count in self.class_counts.items():\n",
    "            weight = np.sqrt(total_samples / (num_classes * count))\n",
    "            class_weights[class_idx] = weight\n",
    "        \n",
    "        # ê° ìƒ˜í”Œì— ê°€ì¤‘ì¹˜ í• ë‹¹\n",
    "        self.sample_weights = []\n",
    "        for _, class_idx in self.samples:\n",
    "            self.sample_weights.append(class_weights[class_idx])\n",
    "        \n",
    "        self.sample_weights = torch.DoubleTensor(self.sample_weights)\n",
    "        print(f\"âš–ï¸ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ: {self.sample_weights.min():.3f} ~ {self.sample_weights.max():.3f}\")\n",
    "    \n",
    "    def get_weighted_sampler(self):\n",
    "        \"\"\"WeightedRandomSampler ë°˜í™˜\"\"\"\n",
    "        return WeightedRandomSampler(\n",
    "            weights=self.sample_weights,\n",
    "            num_samples=len(self.sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            # ì´ë¯¸ì§€ ë¡œë”©\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # ë³€í™˜ ì ìš©\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, label, idx  # idx ì¶”ê°€ (Hard Negative ì¶”ì ìš©)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ì´ë¯¸ì§€ ë¡œë”© ì˜¤ë¥˜: {img_path} - {e}\")\n",
    "            # ë‹¤ë¥¸ ìƒ˜í”Œë¡œ ëŒ€ì²´\n",
    "            return self.__getitem__((idx + 1) % len(self.samples))\n",
    "\n",
    "# Hard Negative Sampler í´ë˜ìŠ¤ (2ìœ„ íŒ€ í•µì‹¬ ê¸°ë²•)\n",
    "class HardNegativeSampler:\n",
    "    \"\"\"Hard Negative Sampleì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, memory_size=1000, loss_threshold=1.5):\n",
    "        self.memory_size = memory_size\n",
    "        self.loss_threshold = loss_threshold\n",
    "        self.hard_samples = []  # (image, label, loss) íŠœí”Œë“¤\n",
    "        self.sample_indices = []  # ì›ë³¸ ë°ì´í„°ì…‹ ì¸ë±ìŠ¤ë“¤\n",
    "        \n",
    "    def update(self, images, labels, indices, losses):\n",
    "        \"\"\"Hard sample ì—…ë°ì´íŠ¸\"\"\"\n",
    "        # ë†’ì€ ì†ì‹¤ì„ ê°€ì§„ ìƒ˜í”Œë“¤ ì°¾ê¸°\n",
    "        hard_mask = losses > self.loss_threshold\n",
    "        \n",
    "        if hard_mask.sum() > 0:\n",
    "            hard_images = images[hard_mask]\n",
    "            hard_labels = labels[hard_mask]\n",
    "            hard_indices = indices[hard_mask]\n",
    "            hard_losses = losses[hard_mask]\n",
    "            \n",
    "            # ìƒˆë¡œìš´ hard sampleë“¤ ì¶”ê°€\n",
    "            for img, label, idx, loss in zip(hard_images, hard_labels, hard_indices, hard_losses):\n",
    "                self.hard_samples.append((img.cpu(), label.cpu(), loss.item()))\n",
    "                self.sample_indices.append(idx.item())\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œ\n",
    "            if len(self.hard_samples) > self.memory_size:\n",
    "                # ì†ì‹¤ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ Nê°œë§Œ ìœ ì§€\n",
    "                sorted_samples = sorted(\n",
    "                    zip(self.hard_samples, self.sample_indices),\n",
    "                    key=lambda x: x[0][2],  # loss ê¸°ì¤€ ì •ë ¬\n",
    "                    reverse=True\n",
    "                )\n",
    "                \n",
    "                self.hard_samples = [s[0] for s in sorted_samples[:self.memory_size]]\n",
    "                self.sample_indices = [s[1] for s in sorted_samples[:self.memory_size]]\n",
    "    \n",
    "    def sample(self, batch_size, device):\n",
    "        \"\"\"Hard sampleë“¤ì—ì„œ ë°°ì¹˜ ìƒ˜í”Œë§\"\"\"\n",
    "        if len(self.hard_samples) == 0:\n",
    "            return None, None\n",
    "        \n",
    "        # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ëœë¤ ìƒ˜í”Œë§\n",
    "        sample_size = min(batch_size, len(self.hard_samples))\n",
    "        sampled_indices = np.random.choice(len(self.hard_samples), sample_size, replace=False)\n",
    "        \n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for idx in sampled_indices:\n",
    "            img, label, _ = self.hard_samples[idx]\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "        \n",
    "        images = torch.stack(images).to(device)\n",
    "        labels = torch.stack(labels).to(device)\n",
    "        \n",
    "        return images, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.hard_samples)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "print(\"ğŸ“Š ë°ì´í„°ì…‹ ìƒì„± ì¤‘...\")\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„°ì…‹\n",
    "train_dataset = RockDataset(\n",
    "    data_dir=TRAIN_PATH,\n",
    "    transform=train_transforms,\n",
    "    class_to_idx=class_to_idx,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„°ì…‹ì€ ë‚˜ì¤‘ì— K-Foldì—ì„œ ë¶„í• \n",
    "print(f\"âœ… í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(train_dataset):,}ê°œ ìƒ˜í”Œ\")\n",
    "\n",
    "# WeightedRandomSampler ìƒì„±\n",
    "weighted_sampler = train_dataset.get_weighted_sampler()\n",
    "print(\"âœ… WeightedRandomSampler ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# Hard Negative Sampler ìƒì„±\n",
    "hard_negative_sampler = HardNegativeSampler(\n",
    "    memory_size=HYPERPARAMETERS['hard_memory_size'],\n",
    "    loss_threshold=HYPERPARAMETERS['loss_threshold']\n",
    ")\n",
    "print(\"âœ… Hard Negative Sampler ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# ê¸°ë³¸ DataLoader ìƒì„± (K-Foldì—ì„œ ì¬êµ¬ì„±ë¨)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=HYPERPARAMETERS['batch_size'],\n",
    "    sampler=weighted_sampler,\n",
    "    num_workers=HYPERPARAMETERS['num_workers'],\n",
    "    pin_memory=HYPERPARAMETERS['pin_memory'],\n",
    "    drop_last=True  # ë°°ì¹˜ í¬ê¸° ì¼ê´€ì„± ìœ ì§€\n",
    ")\n",
    "\n",
    "print(f\"âœ… DataLoader ìƒì„± ì™„ë£Œ: {len(train_loader)} ë°°ì¹˜\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b787924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸ ì¤‘...\n",
      "âœ… tf_efficientnetv2_s.in21k_ft_in1k ëª¨ë¸ ìƒì„± ì™„ë£Œ\n",
      "âœ… EfficientNetV2-S: 20,186,455 íŒŒë¼ë¯¸í„° (77.0MB)\n",
      "âœ… tf_efficientnetv2_m.in21k_ft_in1k ëª¨ë¸ ìƒì„± ì™„ë£Œ\n",
      "âœ… EfficientNetV2-M: 52,867,323 íŒŒë¼ë¯¸í„° (201.7MB)\n",
      "âœ… convnext_tiny.fb_in22k_ft_in1k ëª¨ë¸ ìƒì„± ì™„ë£Œ\n",
      "âœ… ConvNeXt-Tiny: 27,825,511 íŒŒë¼ë¯¸í„° (106.1MB)\n",
      "âœ… vit_tiny_patch16_224.augreg_in21k_ft_in1k ëª¨ë¸ ìƒì„± ì™„ë£Œ\n",
      "âœ… ViT-Tiny: 5,525,767 íŒŒë¼ë¯¸í„° (21.1MB)\n",
      "\n",
      "ğŸ“Š ì´ 4ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥\n",
      "\n",
      "ğŸ† ìƒìœ„ íŒ€ ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ì„:\n",
      "================================================================================\n",
      "1. EfficientNetV2-S\n",
      "   ì‚¬ìš© íŒ€: 1ìœ„ íŒ€ ì‚¬ìš©\n",
      "   íŠ¹ì§•: íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•\n",
      "   íŒŒë¼ë¯¸í„°: 20,186,455ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 77.0MB\n",
      "   í•™ìŠµë¥  ë°°ìˆ˜: 1.0\n",
      "----------------------------------------\n",
      "2. EfficientNetV2-M\n",
      "   ì‚¬ìš© íŒ€: 1ìœ„ íŒ€ ì‚¬ìš©\n",
      "   íŠ¹ì§•: ë” í° ëª¨ë¸, ë†’ì€ ì„±ëŠ¥\n",
      "   íŒŒë¼ë¯¸í„°: 52,867,323ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 201.7MB\n",
      "   í•™ìŠµë¥  ë°°ìˆ˜: 0.8\n",
      "----------------------------------------\n",
      "3. ConvNeXt-Tiny\n",
      "   ì‚¬ìš© íŒ€: 3ìœ„ íŒ€ ì‚¬ìš©\n",
      "   íŠ¹ì§•: CNN + Transformer í•˜ì´ë¸Œë¦¬ë“œ\n",
      "   íŒŒë¼ë¯¸í„°: 27,825,511ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 106.1MB\n",
      "   í•™ìŠµë¥  ë°°ìˆ˜: 1.2\n",
      "----------------------------------------\n",
      "4. ViT-Tiny\n",
      "   ì‚¬ìš© íŒ€: ë‹¤ì–‘í•œ íŒ€ ì‹¤í—˜\n",
      "   íŠ¹ì§•: Vision Transformer\n",
      "   íŒŒë¼ë¯¸í„°: 5,525,767ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 21.1MB\n",
      "   í•™ìŠµë¥  ë°°ìˆ˜: 1.5\n",
      "----------------------------------------\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜ (1-4ìœ„ íŒ€ì˜ ìµœê³  ëª¨ë¸ë“¤ í†µí•©)\n",
    "# ============================================================================\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"ìƒìœ„ íŒ€ë“¤ì´ ì‚¬ìš©í•œ ëª¨ë¸ë“¤ì„ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_model(model_name, num_classes=7, pretrained=True):\n",
    "        \"\"\"ëª¨ë¸ ìƒì„±\"\"\"\n",
    "        try:\n",
    "            if 'efficientnetv2' in model_name:\n",
    "                # 1ìœ„ íŒ€ ì‚¬ìš© ëª¨ë¸\n",
    "                model = timm.create_model(\n",
    "                    model_name, \n",
    "                    pretrained=pretrained,\n",
    "                    num_classes=num_classes\n",
    "                )\n",
    "                \n",
    "            elif 'convnext' in model_name:\n",
    "                # 3ìœ„ íŒ€ ì‚¬ìš© ëª¨ë¸\n",
    "                model = timm.create_model(\n",
    "                    model_name,\n",
    "                    pretrained=pretrained, \n",
    "                    num_classes=num_classes\n",
    "                )\n",
    "                \n",
    "            elif 'vit' in model_name:\n",
    "                # Vision Transformer (ë‹¤ì–‘í•œ íŒ€ì—ì„œ ì‹¤í—˜)\n",
    "                model = timm.create_model(\n",
    "                    model_name,\n",
    "                    pretrained=pretrained,\n",
    "                    num_classes=num_classes\n",
    "                )\n",
    "                \n",
    "            elif 'regnety' in model_name:\n",
    "                # 1ìœ„ íŒ€ ì‚¬ìš© ëª¨ë¸\n",
    "                model = timm.create_model(\n",
    "                    model_name,\n",
    "                    pretrained=pretrained,\n",
    "                    num_classes=num_classes\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}\")\n",
    "            \n",
    "            print(f\"âœ… {model_name} ëª¨ë¸ ìƒì„± ì™„ë£Œ\")\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {model_name} ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            # ëŒ€ì²´ ëª¨ë¸ë¡œ EfficientNetV2-S ì‚¬ìš©\n",
    "            print(\"ğŸ”„ ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©: EfficientNetV2-S\")\n",
    "            return timm.create_model(\n",
    "                'tf_efficientnetv2_s.in21k_ft_in1k',\n",
    "                pretrained=True,\n",
    "                num_classes=num_classes\n",
    "            )\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_model_info(model):\n",
    "        \"\"\"ëª¨ë¸ ì •ë³´ ì¶œë ¥\"\"\"\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params,\n",
    "            'size_mb': total_params * 4 / 1024 / 1024  # ëŒ€ëµì ì¸ í¬ê¸° (float32 ê¸°ì¤€)\n",
    "        }\n",
    "\n",
    "# ëª¨ë¸ë³„ ì„¤ì •\n",
    "MODEL_CONFIGS = {\n",
    "    'tf_efficientnetv2_s.in21k_ft_in1k': {\n",
    "        'name': 'EfficientNetV2-S',\n",
    "        'team': '1ìœ„ íŒ€ ì‚¬ìš©',\n",
    "        'description': 'íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•',\n",
    "        'lr_multiplier': 1.0\n",
    "    },\n",
    "    'tf_efficientnetv2_m.in21k_ft_in1k': {\n",
    "        'name': 'EfficientNetV2-M',\n",
    "        'team': '1ìœ„ íŒ€ ì‚¬ìš©', \n",
    "        'description': 'ë” í° ëª¨ë¸, ë†’ì€ ì„±ëŠ¥',\n",
    "        'lr_multiplier': 0.8\n",
    "    },\n",
    "    'convnext_tiny.fb_in22k_ft_in1k': {\n",
    "        'name': 'ConvNeXt-Tiny',\n",
    "        'team': '3ìœ„ íŒ€ ì‚¬ìš©',\n",
    "        'description': 'CNN + Transformer í•˜ì´ë¸Œë¦¬ë“œ',\n",
    "        'lr_multiplier': 1.2\n",
    "    },\n",
    "    'vit_tiny_patch16_224.augreg_in21k_ft_in1k': {\n",
    "        'name': 'ViT-Tiny',\n",
    "        'team': 'ë‹¤ì–‘í•œ íŒ€ ì‹¤í—˜',\n",
    "        'description': 'Vision Transformer',\n",
    "        'lr_multiplier': 1.5\n",
    "    }\n",
    "}\n",
    "\n",
    "# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸\n",
    "print(\"ğŸ§  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸ ì¤‘...\")\n",
    "available_models = []\n",
    "\n",
    "for model_name in HYPERPARAMETERS['ensemble_models']:\n",
    "    try:\n",
    "        # ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "        test_model = ModelFactory.create_model(model_name, NUM_CLASSES)\n",
    "        model_info = ModelFactory.get_model_info(test_model)\n",
    "        \n",
    "        config = MODEL_CONFIGS.get(model_name, {\n",
    "            'name': model_name,\n",
    "            'team': 'ì‹¤í—˜ìš©',\n",
    "            'description': 'Custom Model',\n",
    "            'lr_multiplier': 1.0\n",
    "        })\n",
    "        \n",
    "        available_models.append({\n",
    "            'model_name': model_name,\n",
    "            'config': config,\n",
    "            'info': model_info\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ… {config['name']}: {model_info['total_params']:,} íŒŒë¼ë¯¸í„° ({model_info['size_mb']:.1f}MB)\")\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        del test_model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name}: ì‚¬ìš© ë¶ˆê°€ - {e}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š ì´ {len(available_models)}ê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ëª¨ë¸ë¡œ ë² ì´ìŠ¤ë¼ì¸ ìƒì„± í•¨ìˆ˜\n",
    "def create_baseline_model():\n",
    "    \"\"\"ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ìƒì„±\"\"\"\n",
    "    if available_models:\n",
    "        baseline_config = available_models[0]\n",
    "        model_name = baseline_config['model_name']\n",
    "        \n",
    "        model = ModelFactory.create_model(model_name, NUM_CLASSES)\n",
    "        \n",
    "        print(f\"ğŸ¯ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸: {baseline_config['config']['name']}\")\n",
    "        print(f\"   íŒ€: {baseline_config['config']['team']}\")\n",
    "        print(f\"   ì„¤ëª…: {baseline_config['config']['description']}\")\n",
    "        \n",
    "        return model, baseline_config\n",
    "    else:\n",
    "        raise RuntimeError(\"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# ëª¨ë¸ ìš”ì•½ ì •ë³´ ì¶œë ¥\n",
    "def print_model_summary():\n",
    "    \"\"\"ëª¨ë¸ ìš”ì•½ ì •ë³´ ì¶œë ¥\"\"\"\n",
    "    print(\"\\nğŸ† ìƒìœ„ íŒ€ ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ì„:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, model_data in enumerate(available_models, 1):\n",
    "        config = model_data['config']\n",
    "        info = model_data['info']\n",
    "        \n",
    "        print(f\"{i}. {config['name']}\")\n",
    "        print(f\"   ì‚¬ìš© íŒ€: {config['team']}\")\n",
    "        print(f\"   íŠ¹ì§•: {config['description']}\")\n",
    "        print(f\"   íŒŒë¼ë¯¸í„°: {info['total_params']:,}ê°œ\")\n",
    "        print(f\"   ëª¨ë¸ í¬ê¸°: {info['size_mb']:.1f}MB\")\n",
    "        print(f\"   í•™ìŠµë¥  ë°°ìˆ˜: {config['lr_multiplier']}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print_model_summary()\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc065bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AdvancedTrainer í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ ê³ ê¸‰ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤ (ëª¨ë“  ìƒìœ„ íŒ€ ê¸°ë²• í†µí•©)\n",
    "# ============================================================================\n",
    "\n",
    "class AdvancedTrainer:\n",
    "    \"\"\"ìƒìœ„ íŒ€ë“¤ì˜ ëª¨ë“  ê¸°ë²•ì„ í†µí•©í•œ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, hyperparameters):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.hp = hyperparameters\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì € ì„¤ì • (1ìœ„ íŒ€ ì „ëµ)\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.hp['lr'],\n",
    "            weight_decay=self.hp['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ìƒìœ„ íŒ€ ê³µí†µ)\n",
    "        self.scheduler = None  # ë‚˜ì¤‘ì— train_loader í¬ê¸°ë¥¼ ì•Œì•„ì•¼ ì„¤ì • ê°€ëŠ¥\n",
    "        \n",
    "        # ì†ì‹¤ í•¨ìˆ˜\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # ë©”íŠ¸ë¦­ ì¶”ì \n",
    "        self.train_history = defaultdict(list)\n",
    "        self.val_history = defaultdict(list)\n",
    "        self.best_score = 0.0\n",
    "        self.best_model_state = None\n",
    "        \n",
    "        # Hard Negative Sampler\n",
    "        self.hard_negative_sampler = HardNegativeSampler(\n",
    "            memory_size=self.hp['hard_memory_size'],\n",
    "            loss_threshold=self.hp['loss_threshold']\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ¯ AdvancedTrainer ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "        print(f\"   ì˜µí‹°ë§ˆì´ì €: AdamW (lr={self.hp['lr']}, weight_decay={self.hp['weight_decay']})\")\n",
    "        print(f\"   ì†ì‹¤ í•¨ìˆ˜: CrossEntropyLoss\")\n",
    "        print(f\"   Hard Negative: í™œì„±í™”\")\n",
    "    \n",
    "    def setup_scheduler(self, train_loader):\n",
    "        \"\"\"ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (OneCycleLR ì‚¬ìš©)\"\"\"\n",
    "        total_steps = len(train_loader) * self.hp['num_epochs']\n",
    "        \n",
    "        self.scheduler = OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.hp['lr'],\n",
    "            total_steps=total_steps,\n",
    "            pct_start=0.3,  # 30%ê¹Œì§€ í•™ìŠµë¥  ì¦ê°€\n",
    "            div_factor=10,  # ì´ˆê¸° í•™ìŠµë¥  = max_lr / div_factor\n",
    "            final_div_factor=100  # ìµœì¢… í•™ìŠµë¥  = max_lr / final_div_factor\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… OneCycleLR ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ (ì´ {total_steps} ìŠ¤í…)\")\n",
    "    \n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        \"\"\"í•œ ì—í¬í¬ í›ˆë ¨\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.hp[\"num_epochs\"]}')\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(pbar):\n",
    "            if len(batch_data) == 3:\n",
    "                images, labels, indices = batch_data\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                indices = indices.to(self.device)\n",
    "            else:\n",
    "                images, labels = batch_data\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                indices = torch.arange(len(labels)).to(self.device)\n",
    "            \n",
    "            # Hard Negative ìƒ˜í”Œë§ ì ìš© (2ìœ„ íŒ€ ì „ëµ)\n",
    "            if len(self.hard_negative_sampler) > 0 and np.random.random() < self.hp['hard_negative_ratio']:\n",
    "                hard_size = int(len(labels) * self.hp['hard_negative_ratio'])\n",
    "                hard_images, hard_labels = self.hard_negative_sampler.sample(hard_size, self.device)\n",
    "                \n",
    "                if hard_images is not None:\n",
    "                    # Hard sampleê³¼ ì¼ë°˜ sample ê²°í•©\n",
    "                    regular_size = len(labels) - hard_size\n",
    "                    images = torch.cat([images[:regular_size], hard_images], dim=0)\n",
    "                    labels = torch.cat([labels[:regular_size], hard_labels], dim=0)\n",
    "            \n",
    "            # CutMix/Mixup ì ìš© (í™•ë¥ ì )\n",
    "            r = np.random.rand(1)\n",
    "            if r < self.hp['cutmix_prob']:\n",
    "                # CutMix ì ìš©\n",
    "                images, targets_a, targets_b, lam = cutmix_data(images, labels, self.hp['cutmix_alpha'])\n",
    "                outputs = self.model(images)\n",
    "                loss = mixup_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "            elif r < self.hp['cutmix_prob'] + self.hp['mixup_prob']:\n",
    "                # Mixup ì ìš©\n",
    "                images, targets_a, targets_b, lam = mixup_data(images, labels, self.hp['mixup_alpha'])\n",
    "                outputs = self.model(images)\n",
    "                loss = mixup_criterion(self.criterion, outputs, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                # ì¼ë°˜ í•™ìŠµ\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                targets_a = labels\n",
    "            \n",
    "            # ì—­ì „íŒŒ\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # ê·¸ë¼ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì•ˆì •ì„± í–¥ìƒ)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # Hard Negative ì—…ë°ì´íŠ¸ (ë°°ì¹˜ë³„ ì†ì‹¤ ê³„ì‚°)\n",
    "            with torch.no_grad():\n",
    "                batch_losses = F.cross_entropy(outputs, targets_a, reduction='none')\n",
    "                self.hard_negative_sampler.update(images, targets_a, indices[:len(targets_a)], batch_losses)\n",
    "            \n",
    "            # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "            total_loss += loss.item()\n",
    "            total_samples += len(targets_a)\n",
    "            \n",
    "            # ì •í™•ë„ ê³„ì‚° (Mixupì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)\n",
    "            if r >= self.hp['cutmix_prob'] + self.hp['mixup_prob']:\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_predictions += predicted.eq(targets_a).sum().item()\n",
    "            \n",
    "            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'LR': f'{current_lr:.6f}',\n",
    "                'Hard': len(self.hard_negative_sampler)\n",
    "            })\n",
    "        \n",
    "        # ì—í¬í¬ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "        self.train_history['loss'].append(avg_loss)\n",
    "        self.train_history['accuracy'].append(accuracy)\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate_epoch(self, val_loader):\n",
    "        \"\"\"ê²€ì¦\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data in tqdm(val_loader, desc='Validation'):\n",
    "                # DataLoaderì—ì„œ ë°˜í™˜ë˜ëŠ” ë°ì´í„° í˜•ì‹ì— ë§ì¶° ì²˜ë¦¬\n",
    "                if len(batch_data) == 3:\n",
    "                    images, labels, _ = batch_data  # (image, label, idx)\n",
    "                else:\n",
    "                    images, labels = batch_data      # (image, label)\n",
    "                \n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
    "        \n",
    "        self.val_history['loss'].append(avg_loss)\n",
    "        self.val_history['accuracy'].append(accuracy)\n",
    "        self.val_history['f1_macro'].append(f1_macro)\n",
    "        \n",
    "        return avg_loss, accuracy, f1_macro, all_predictions, all_labels\n",
    "    \n",
    "    def fit(self, train_loader, val_loader, save_path=None):\n",
    "        \"\"\"ëª¨ë¸ í›ˆë ¨\"\"\"\n",
    "        print(f\"ğŸš€ í›ˆë ¨ ì‹œì‘!\")\n",
    "        print(f\"   ì—í¬í¬: {self.hp['num_epochs']}\")\n",
    "        print(f\"   ë°°ì¹˜ í¬ê¸°: {self.hp['batch_size']}\")\n",
    "        \n",
    "        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n",
    "        self.setup_scheduler(train_loader)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.hp['num_epochs']):\n",
    "            # í›ˆë ¨\n",
    "            train_loss, train_acc = self.train_epoch(train_loader, epoch)\n",
    "            \n",
    "            # ê²€ì¦\n",
    "            val_loss, val_acc, val_f1, val_preds, val_labels = self.validate_epoch(val_loader)\n",
    "            \n",
    "            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "            if val_f1 > self.best_score:\n",
    "                self.best_score = val_f1\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                \n",
    "                if save_path:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.best_model_state,\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'best_score': self.best_score,\n",
    "                        'hyperparameters': self.hp\n",
    "                    }, save_path)\n",
    "            \n",
    "            # ê²°ê³¼ ì¶œë ¥\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.hp['num_epochs']}:\")\n",
    "            print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "            print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "            print(f\"  Best F1: {self.best_score:.4f}\")\n",
    "            print(f\"  Hard Negative Pool: {len(self.hard_negative_sampler)}\")\n",
    "            \n",
    "            # Early Stopping (ê°„ë‹¨í•œ êµ¬í˜„)\n",
    "            if epoch > 20 and val_f1 < self.best_score - 0.05:\n",
    "                print(\"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ (ì„±ëŠ¥ ì €í•˜)\")\n",
    "                break\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nâœ… í›ˆë ¨ ì™„ë£Œ! ì´ ì‹œê°„: {total_time/3600:.2f}ì‹œê°„\")\n",
    "        print(f\"ğŸ† ìµœê³  ì„±ëŠ¥: {self.best_score:.4f}\")\n",
    "        \n",
    "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\n",
    "        if self.best_model_state:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "        \n",
    "        return self.train_history, self.val_history\n",
    "\n",
    "print(\"âœ… AdvancedTrainer í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b0977f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… K-Fold êµì°¨ê²€ì¦ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š K-Fold êµì°¨ê²€ì¦ (2-4ìœ„ íŒ€ ê³µí†µ ì „ëµ)\n",
    "# ============================================================================\n",
    "\n",
    "class KFoldManager:\n",
    "    \"\"\"K-Fold êµì°¨ê²€ì¦ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=5, random_state=42):\n",
    "        self.n_splits = n_splits\n",
    "        self.skf = StratifiedKFold(\n",
    "            n_splits=n_splits, \n",
    "            shuffle=True, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        self.fold_results = []\n",
    "        \n",
    "    def create_folds(self, dataset):\n",
    "        \"\"\"K-Fold ë¶„í•  ìƒì„±\"\"\"\n",
    "        # ì „ì²´ ë°ì´í„°ì—ì„œ ë¼ë²¨ ì¶”ì¶œ\n",
    "        all_labels = [dataset.samples[i][1] for i in range(len(dataset))]\n",
    "        indices = np.arange(len(dataset))\n",
    "        \n",
    "        folds = []\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(self.skf.split(indices, all_labels)):\n",
    "            folds.append({\n",
    "                'fold': fold_idx,\n",
    "                'train_indices': train_idx,\n",
    "                'val_indices': val_idx\n",
    "            })\n",
    "            \n",
    "            print(f\"Fold {fold_idx+1}: Train {len(train_idx)}, Val {len(val_idx)}\")\n",
    "        \n",
    "        return folds\n",
    "    \n",
    "    def create_fold_dataloaders(self, dataset, fold_info, hyperparameters):\n",
    "        \"\"\"íŠ¹ì • Foldì˜ DataLoader ìƒì„±\"\"\"\n",
    "        train_idx = fold_info['train_indices']\n",
    "        val_idx = fold_info['val_indices']\n",
    "        \n",
    "        # í›ˆë ¨ ë°ì´í„°ì…‹ (WeightedRandomSampler ì ìš©)\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        \n",
    "        # í›ˆë ¨ ë°ì´í„°ë§Œì˜ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "        train_labels = [dataset.samples[i][1] for i in train_idx]\n",
    "        train_class_counts = Counter(train_labels)\n",
    "        \n",
    "        # ì œê³±ê·¼ ì—­ë¹ˆë„ ê°€ì¤‘ì¹˜ ì¬ê³„ì‚°\n",
    "        total_train_samples = len(train_idx)\n",
    "        num_classes = len(set(train_labels))\n",
    "        \n",
    "        train_weights = []\n",
    "        for idx in train_idx:\n",
    "            label = dataset.samples[idx][1]\n",
    "            weight = np.sqrt(total_train_samples / (num_classes * train_class_counts[label]))\n",
    "            train_weights.append(weight)\n",
    "        \n",
    "        train_sampler = WeightedRandomSampler(\n",
    "            weights=train_weights,\n",
    "            num_samples=len(train_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=hyperparameters['batch_size'],\n",
    "            sampler=train_sampler,\n",
    "            num_workers=hyperparameters['num_workers'],\n",
    "            pin_memory=hyperparameters['pin_memory'],\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        # ê²€ì¦ ë°ì´í„°ì…‹ (ìˆœì°¨ ìƒ˜í”Œë§)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "        val_loader = DataLoader(\n",
    "            val_subset,\n",
    "            batch_size=hyperparameters['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=hyperparameters['num_workers'],\n",
    "            pin_memory=hyperparameters['pin_memory']\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "def run_kfold_training():\n",
    "    \"\"\"K-Fold êµì°¨ê²€ì¦ ì‹¤í–‰\"\"\"\n",
    "    print(\"ğŸ”„ K-Fold êµì°¨ê²€ì¦ ì‹œì‘!\")\n",
    "    print(f\"   Fold ìˆ˜: {HYPERPARAMETERS['kfold_splits']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # K-Fold ë§¤ë‹ˆì € ìƒì„±\n",
    "    kfold_manager = KFoldManager(\n",
    "        n_splits=HYPERPARAMETERS['kfold_splits'],\n",
    "        random_state=HYPERPARAMETERS['kfold_seed']\n",
    "    )\n",
    "    \n",
    "    # Fold ë¶„í• \n",
    "    folds = kfold_manager.create_folds(train_dataset)\n",
    "    \n",
    "    # ê° ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥\n",
    "    all_results = {}\n",
    "    \n",
    "    for model_data in available_models[:2]:  # ì²˜ìŒ 2ê°œ ëª¨ë¸ë§Œ ì‹¤í—˜ (ì‹œê°„ ì ˆì•½)\n",
    "        model_name = model_data['model_name']\n",
    "        config = model_data['config']\n",
    "        \n",
    "        print(f\"\\nğŸ§  ëª¨ë¸: {config['name']} ({config['team']})\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        fold_results = []\n",
    "        \n",
    "        for fold_info in folds:\n",
    "            fold_idx = fold_info['fold']\n",
    "            print(f\"\\nğŸ“Š Fold {fold_idx + 1}/{HYPERPARAMETERS['kfold_splits']}\")\n",
    "            \n",
    "            # ëª¨ë¸ ìƒì„±\n",
    "            model = ModelFactory.create_model(model_name, NUM_CLASSES)\n",
    "            \n",
    "            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • (ëª¨ë¸ë³„)\n",
    "            adjusted_hp = HYPERPARAMETERS.copy()\n",
    "            adjusted_hp['lr'] *= config['lr_multiplier']\n",
    "            \n",
    "            # íŠ¸ë ˆì´ë„ˆ ìƒì„±\n",
    "            trainer = AdvancedTrainer(model, device, adjusted_hp)\n",
    "            \n",
    "            # ë°ì´í„°ë¡œë” ìƒì„±\n",
    "            train_loader, val_loader = kfold_manager.create_fold_dataloaders(\n",
    "                train_dataset, fold_info, adjusted_hp\n",
    "            )\n",
    "            \n",
    "            # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "            save_path = MODEL_DIR / f\"{config['name']}_fold{fold_idx+1}.pth\"\n",
    "            \n",
    "            # í›ˆë ¨ ì‹¤í–‰\n",
    "            train_history, val_history = trainer.fit(\n",
    "                train_loader, val_loader, save_path=save_path\n",
    "            )\n",
    "            \n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            fold_result = {\n",
    "                'fold': fold_idx + 1,\n",
    "                'best_f1': trainer.best_score,\n",
    "                'train_history': train_history,\n",
    "                'val_history': val_history,\n",
    "                'model_path': str(save_path)\n",
    "            }\n",
    "            \n",
    "            fold_results.append(fold_result)\n",
    "            \n",
    "            print(f\"âœ… Fold {fold_idx + 1} ì™„ë£Œ! F1 Score: {trainer.best_score:.4f}\")\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            del model, trainer, train_loader, val_loader\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # ëª¨ë¸ë³„ ê²°ê³¼ ì •ë¦¬\n",
    "        f1_scores = [result['best_f1'] for result in fold_results]\n",
    "        \n",
    "        model_result = {\n",
    "            'model_name': model_name,\n",
    "            'config': config,\n",
    "            'fold_results': fold_results,\n",
    "            'mean_f1': np.mean(f1_scores),\n",
    "            'std_f1': np.std(f1_scores),\n",
    "            'best_f1': np.max(f1_scores),\n",
    "            'worst_f1': np.min(f1_scores)\n",
    "        }\n",
    "        \n",
    "        all_results[model_name] = model_result\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ {config['name']} ìµœì¢… ê²°ê³¼:\")\n",
    "        print(f\"   í‰ê·  F1: {model_result['mean_f1']:.4f} Â± {model_result['std_f1']:.4f}\")\n",
    "        print(f\"   ìµœê³  F1: {model_result['best_f1']:.4f}\")\n",
    "        print(f\"   ìµœì € F1: {model_result['worst_f1']:.4f}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def print_kfold_summary(all_results):\n",
    "    \"\"\"K-Fold ê²°ê³¼ ìš”ì•½ ì¶œë ¥\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ† K-Fold êµì°¨ê²€ì¦ ìµœì¢… ê²°ê³¼\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ê²°ê³¼ë¥¼ í‰ê·  F1 ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "    sorted_results = sorted(\n",
    "        all_results.items(),\n",
    "        key=lambda x: x[1]['mean_f1'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for rank, (model_name, result) in enumerate(sorted_results, 1):\n",
    "        config = result['config']\n",
    "        print(f\"\\n{rank}ìœ„. {config['name']} ({config['team']})\")\n",
    "        print(f\"     í‰ê·  F1: {result['mean_f1']:.4f} Â± {result['std_f1']:.4f}\")\n",
    "        print(f\"     ìµœê³  F1: {result['best_f1']:.4f}\")\n",
    "        print(f\"     ì•ˆì •ì„±: {result['std_f1']:.4f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)\")\n",
    "        \n",
    "        # ê° Fold ê²°ê³¼\n",
    "        fold_scores = [fold['best_f1'] for fold in result['fold_results']]\n",
    "        print(f\"     Foldë³„: {' | '.join([f'{score:.3f}' for score in fold_scores])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ’¡ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ:\")\n",
    "    \n",
    "    best_model = sorted_results[0]\n",
    "    print(f\"ğŸ¥‡ ìµœê³  ì„±ëŠ¥: {best_model[1]['config']['name']} (F1: {best_model[1]['mean_f1']:.4f})\")\n",
    "    \n",
    "    # ê°€ì¥ ì•ˆì •ì ì¸ ëª¨ë¸ ì°¾ê¸°\n",
    "    most_stable = min(sorted_results, key=lambda x: x[1]['std_f1'])\n",
    "    print(f\"ğŸ¯ ê°€ì¥ ì•ˆì •ì : {most_stable[1]['config']['name']} (Std: {most_stable[1]['std_f1']:.4f})\")\n",
    "\n",
    "print(\"âœ… K-Fold êµì°¨ê²€ì¦ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ ì‹¤í—˜ ëª¨ë“œ ì„ íƒ:\n",
      "1. âš¡ ë¹ ë¥¸ ì‹¤í—˜ (ë² ì´ìŠ¤ë¼ì¸, 10 ì—í¬í¬)\n",
      "2. ğŸ† K-Fold êµì°¨ê²€ì¦ (ì „ì²´ ëª¨ë¸, 50 ì—í¬í¬)\n",
      "3. ğŸ¯ ë‹¨ì¼ ëª¨ë¸ ì „ì²´ í•™ìŠµ (ì„ íƒ ëª¨ë¸, 50 ì—í¬í¬)\n",
      "\n",
      "ğŸ“ ìë™ ì„ íƒ: ë¹ ë¥¸ ì‹¤í—˜ ëª¨ë“œ\n",
      "   ì´ìœ : íŒŒì´í”„ë¼ì¸ ê²€ì¦ ë° ì´ˆê¸° ì„±ëŠ¥ í™•ì¸\n",
      "âš¡ ë¹ ë¥¸ ì‹¤í—˜ ëª¨ë“œ ì‹œì‘!\n",
      "   ëª©ì : íŒŒì´í”„ë¼ì¸ ê²€ì¦ ë° ì´ˆê¸° ì„±ëŠ¥ í™•ì¸\n",
      "   ëª¨ë¸: ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸\n",
      "   ì—í¬í¬: 10 (ë¹ ë¥¸ ê²€ì¦)\n",
      "============================================================\n",
      "âœ… tf_efficientnetv2_s.in21k_ft_in1k ëª¨ë¸ ìƒì„± ì™„ë£Œ\n",
      "ğŸ¯ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸: EfficientNetV2-S\n",
      "   íŒ€: 1ìœ„ íŒ€ ì‚¬ìš©\n",
      "   ì„¤ëª…: íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•\n",
      "ğŸ¯ AdvancedTrainer ì´ˆê¸°í™” ì™„ë£Œ\n",
      "   ì˜µí‹°ë§ˆì´ì €: AdamW (lr=0.0003, weight_decay=0.0001)\n",
      "   ì†ì‹¤ í•¨ìˆ˜: CrossEntropyLoss\n",
      "   Hard Negative: í™œì„±í™”\n",
      "ğŸ“Š ë°ì´í„° ë¶„í• :\n",
      "   í›ˆë ¨: 304,016ê°œ (38002 ë°°ì¹˜)\n",
      "   ê²€ì¦: 76,004ê°œ (9501 ë°°ì¹˜)\n",
      "\n",
      "ğŸš€ ë¹ ë¥¸ ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ ì‹œì‘!\n",
      "ğŸš€ í›ˆë ¨ ì‹œì‘!\n",
      "   ì—í¬í¬: 10\n",
      "   ë°°ì¹˜ í¬ê¸°: 8\n",
      "âœ… OneCycleLR ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ (ì´ 380020 ìŠ¤í…)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|          | 193/38002 [00:15<49:04, 12.84it/s, Loss=5.2050, LR=0.000030, Hard=1000]"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸš€ ì‹¤ì œ ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ (ë² ì´ìŠ¤ë¼ì¸ + ë¹ ë¥¸ ì‹¤í—˜)\n",
    "# ============================================================================\n",
    "\n",
    "def run_quick_experiment():\n",
    "    \"\"\"ë¹ ë¥¸ ì‹¤í—˜ìš© ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "    print(\"âš¡ ë¹ ë¥¸ ì‹¤í—˜ ëª¨ë“œ ì‹œì‘!\")\n",
    "    print(\"   ëª©ì : íŒŒì´í”„ë¼ì¸ ê²€ì¦ ë° ì´ˆê¸° ì„±ëŠ¥ í™•ì¸\")\n",
    "    print(\"   ëª¨ë¸: ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸\")\n",
    "    print(\"   ì—í¬í¬: 10 (ë¹ ë¥¸ ê²€ì¦)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ìƒì„±\n",
    "    model, model_config = create_baseline_model()\n",
    "    \n",
    "    # ì‹¤í—˜ìš© í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì§§ì€ í•™ìŠµ)\n",
    "    quick_hp = HYPERPARAMETERS.copy()\n",
    "    quick_hp['num_epochs'] = 10  # ë¹ ë¥¸ ê²€ì¦\n",
    "    quick_hp['lr'] *= model_config['config']['lr_multiplier']\n",
    "    \n",
    "    # íŠ¸ë ˆì´ë„ˆ ìƒì„±\n",
    "    trainer = AdvancedTrainer(model, device, quick_hp)\n",
    "    \n",
    "    # ê°„ë‹¨í•œ Train/Val ë¶„í•  (80:20)\n",
    "    dataset_size = len(train_dataset)\n",
    "    val_size = int(0.2 * dataset_size)\n",
    "    train_size = dataset_size - val_size\n",
    "    \n",
    "    train_subset, val_subset = torch.utils.data.random_split(\n",
    "        train_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # DataLoader ìƒì„±\n",
    "    quick_train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=quick_hp['batch_size'],\n",
    "        shuffle=True,  # ê°„ë‹¨í•œ ì…”í”Œë§\n",
    "        num_workers=quick_hp['num_workers'],\n",
    "        pin_memory=quick_hp['pin_memory'],\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    quick_val_loader = DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=quick_hp['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=quick_hp['num_workers'],\n",
    "        pin_memory=quick_hp['pin_memory']\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š ë°ì´í„° ë¶„í• :\")\n",
    "    print(f\"   í›ˆë ¨: {len(train_subset):,}ê°œ ({len(quick_train_loader)} ë°°ì¹˜)\")\n",
    "    print(f\"   ê²€ì¦: {len(val_subset):,}ê°œ ({len(quick_val_loader)} ë°°ì¹˜)\")\n",
    "    \n",
    "    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "    save_path = MODEL_DIR / f\"{model_config['config']['name']}_quick_baseline.pth\"\n",
    "    \n",
    "    # í•™ìŠµ ì‹¤í–‰\n",
    "    print(\"\\nğŸš€ ë¹ ë¥¸ ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ ì‹œì‘!\")\n",
    "    train_history, val_history = trainer.fit(\n",
    "        quick_train_loader, \n",
    "        quick_val_loader, \n",
    "        save_path=save_path\n",
    "    )\n",
    "    \n",
    "    # ê²°ê³¼ ë¶„ì„\n",
    "    print(\"\\nğŸ“ˆ ë¹ ë¥¸ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„:\")\n",
    "    print(f\"ğŸ† ìµœê³  F1 Score: {trainer.best_score:.4f}\")\n",
    "    print(f\"ğŸ“ ëª¨ë¸ ì €ì¥: {save_path}\")\n",
    "    \n",
    "    # í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Loss ê³¡ì„ \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_history['loss'], label='Train Loss', color='blue')\n",
    "    plt.plot(val_history['loss'], label='Val Loss', color='red')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy ê³¡ì„ \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(train_history['accuracy'], label='Train Acc', color='blue')\n",
    "    plt.plot(val_history['accuracy'], label='Val Acc', color='red')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Score ê³¡ì„ \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(val_history['f1_macro'], label='Val F1', color='green')\n",
    "    plt.title('F1 Score (Validation)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=trainer.best_score, color='red', linestyle='--', alpha=0.7, label=f'Best: {trainer.best_score:.3f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ì„±ëŠ¥ í‰ê°€\n",
    "    final_f1 = trainer.best_score\n",
    "    \n",
    "    print(f\"\\nâœ… ë¹ ë¥¸ ì‹¤í—˜ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ’¡ íŒŒì´í”„ë¼ì¸ ìƒíƒœ:\")\n",
    "    \n",
    "    if final_f1 > 0.7:\n",
    "        print(\"ğŸŸ¢ ìš°ìˆ˜í•œ ì„±ëŠ¥! ì „ì²´ ì‹¤í—˜ ì§„í–‰ ê¶Œì¥\")\n",
    "    elif final_f1 > 0.5:\n",
    "        print(\"ğŸŸ¡ ê´œì°®ì€ ì„±ëŠ¥! í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í›„ ì§„í–‰\")\n",
    "    else:\n",
    "        print(\"ğŸ”´ ì„±ëŠ¥ ê°œì„  í•„ìš”! ë°ì´í„°/ëª¨ë¸ ì ê²€ ê¶Œì¥\")\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Hard Negative Pool: {len(trainer.hard_negative_sampler)}ê°œ ìƒ˜í”Œ\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    del model, trainer, quick_train_loader, quick_val_loader\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return final_f1\n",
    "\n",
    "# ì‹¤í—˜ ì„ íƒ í•¨ìˆ˜\n",
    "def choose_experiment_mode():\n",
    "    \"\"\"ì‹¤í—˜ ëª¨ë“œ ì„ íƒ\"\"\"\n",
    "    print(\"ğŸ”¬ ì‹¤í—˜ ëª¨ë“œ ì„ íƒ:\")\n",
    "    print(\"1. âš¡ ë¹ ë¥¸ ì‹¤í—˜ (ë² ì´ìŠ¤ë¼ì¸, 10 ì—í¬í¬)\")\n",
    "    print(\"2. ğŸ† K-Fold êµì°¨ê²€ì¦ (ì „ì²´ ëª¨ë¸, 50 ì—í¬í¬)\")\n",
    "    print(\"3. ğŸ¯ ë‹¨ì¼ ëª¨ë¸ ì „ì²´ í•™ìŠµ (ì„ íƒ ëª¨ë¸, 50 ì—í¬í¬)\")\n",
    "    print()\n",
    "    \n",
    "    # ìë™ìœ¼ë¡œ ë¹ ë¥¸ ì‹¤í—˜ ì„ íƒ (ë°ëª¨ìš©)\n",
    "    print(\"ğŸ“ ìë™ ì„ íƒ: ë¹ ë¥¸ ì‹¤í—˜ ëª¨ë“œ\")\n",
    "    print(\"   ì´ìœ : íŒŒì´í”„ë¼ì¸ ê²€ì¦ ë° ì´ˆê¸° ì„±ëŠ¥ í™•ì¸\")\n",
    "    \n",
    "    return 1\n",
    "\n",
    "# ì‹¤í–‰\n",
    "experiment_mode = choose_experiment_mode()\n",
    "\n",
    "if experiment_mode == 1:\n",
    "    # ë¹ ë¥¸ ì‹¤í—˜\n",
    "    baseline_f1 = run_quick_experiment()\n",
    "    \n",
    "    print(f\"\\nğŸ‰ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥: {baseline_f1:.4f}\")\n",
    "    print(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ì¶”ì²œ:\")\n",
    "    print(\"   1. ì„±ëŠ¥ì´ ë§Œì¡±ìŠ¤ëŸ½ë‹¤ë©´ K-Fold êµì°¨ê²€ì¦ ì‹¤í–‰\")\n",
    "    print(\"   2. ë” ê¸´ í•™ìŠµ(50 ì—í¬í¬)ìœ¼ë¡œ ìµœëŒ€ ì„±ëŠ¥ í™•ì¸\")\n",
    "    print(\"   3. ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ì„±ëŠ¥ ë¹„êµ\")\n",
    "    \n",
    "elif experiment_mode == 2:\n",
    "    # K-Fold êµì°¨ê²€ì¦\n",
    "    print(\"âš ï¸ K-Fold êµì°¨ê²€ì¦ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤!\")\n",
    "    print(\"   ì˜ˆìƒ ì‹œê°„: 1-3ì‹œê°„ (GPU ì‚¬ìš© ì‹œ)\")\n",
    "    kfold_results = run_kfold_training()\n",
    "    print_kfold_summary(kfold_results)\n",
    "    \n",
    "else:\n",
    "    # ë‹¨ì¼ ëª¨ë¸ ì „ì²´ í•™ìŠµ\n",
    "    print(\"ğŸ¯ ë‹¨ì¼ ëª¨ë¸ ì „ì²´ í•™ìŠµ ëª¨ë“œ\")\n",
    "    print(\"   (êµ¬í˜„ ì˜ˆì •)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fc09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”® ì¶”ë¡  ëª¨ë“œ ì„ íƒ:\n",
      "1. ğŸ“ ê¸°ë³¸ ì¶”ë¡  (ë‹¨ì¼ ëª¨ë¸)\n",
      "2. ğŸ”„ TTA ì¶”ë¡  (êµ¬í˜„ ì˜ˆì •)\n",
      "3. ğŸ¯ ì•™ìƒë¸” ì¶”ë¡  (êµ¬í˜„ ì˜ˆì •)\n",
      "\n",
      "ğŸ“ ìë™ ì„ íƒ: ê¸°ë³¸ ì¶”ë¡ \n",
      "ğŸ”® ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± ë°ëª¨\n",
      "============================================================\n",
      "ğŸ“ ì‚¬ìš©í•  ëª¨ë¸: EfficientNetV2-S_quick_baseline.pth\n",
      "âœ… tf_efficientnetv2_s.in21k_ft_in1k ëª¨ë¸ ìƒì„± ì™„ë£Œ\n",
      "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: ..\\models\\EfficientNetV2-S_quick_baseline.pth\n",
      "   ìµœê³  ì„±ëŠ¥: 0.5957981687972994\n",
      "ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 95,006ê°œ ìƒ˜í”Œ\n",
      "âœ… ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: 95,006ê°œ\n",
      "\n",
      "ğŸ”® ìƒ˜í”Œ ì¶”ë¡  ì‹¤í–‰ (ì²˜ìŒ 100ê°œ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: ..\\experiments\\sample_submission.csv\n",
      "ğŸ“Š ì œì¶œ íŒŒì¼ ìš”ì•½:\n",
      "rock_type\n",
      "Etc               38\n",
      "Granite           30\n",
      "Weathered_Rock    14\n",
      "Gneiss             6\n",
      "Andesite           5\n",
      "Basalt             4\n",
      "Mud_Sandstone      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… ìƒ˜í”Œ ì¶”ë¡  ì™„ë£Œ!\n",
      "ğŸ“ ì œì¶œ íŒŒì¼: ..\\experiments\\sample_submission.csv\n",
      "\n",
      "ğŸ“ˆ ì˜ˆì¸¡ ë¶„í¬:\n",
      "   Andesite: 5ê°œ (5.0%)\n",
      "   Basalt: 4ê°œ (4.0%)\n",
      "   Etc: 38ê°œ (38.0%)\n",
      "   Gneiss: 6ê°œ (6.0%)\n",
      "   Granite: 30ê°œ (30.0%)\n",
      "   Mud_Sandstone: 3ê°œ (3.0%)\n",
      "   Weathered_Rock: 14ê°œ (14.0%)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ¯ ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± (ì‹¤ì œ ëŒ€íšŒ í˜•ì‹)\n",
    "# ============================================================================\n",
    "\n",
    "class InferenceManager:\n",
    "    \"\"\"ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, device):\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.model_path = model_path\n",
    "        \n",
    "    def load_model(self, model_name, num_classes=7):\n",
    "        \"\"\"ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            # ëª¨ë¸ ìƒì„±\n",
    "            self.model = ModelFactory.create_model(model_name, num_classes)\n",
    "            \n",
    "            # ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "            checkpoint = torch.load(self.model_path, map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}\")\n",
    "            print(f\"   ìµœê³  ì„±ëŠ¥: {checkpoint.get('best_score', 'N/A')}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_test_dataset(self):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "        try:\n",
    "            # test.csv íŒŒì¼ ì½ê¸°\n",
    "            test_df = pd.read_csv(os.path.join(BASE_PATH, \"test.csv\"))\n",
    "            print(f\"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ ìƒ˜í”Œ\")\n",
    "            \n",
    "            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œë“¤\n",
    "            test_image_paths = []\n",
    "            test_ids = []\n",
    "            \n",
    "            for _, row in test_df.iterrows():\n",
    "                img_path = os.path.join(BASE_PATH, row['img_path'])\n",
    "                if os.path.exists(img_path):\n",
    "                    test_image_paths.append(img_path)\n",
    "                    test_ids.append(row['ID'])\n",
    "                else:\n",
    "                    print(f\"âš ï¸ íŒŒì¼ ì—†ìŒ: {img_path}\")\n",
    "            \n",
    "            print(f\"âœ… ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(test_image_paths):,}ê°œ\")\n",
    "            \n",
    "            return test_image_paths, test_ids\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            return [], []\n",
    "    \n",
    "    def predict_batch(self, image_paths, batch_size=32):\n",
    "        \"\"\"ë°°ì¹˜ ë‹¨ìœ„ ì˜ˆì¸¡\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!\")\n",
    "            return []\n",
    "        \n",
    "        all_predictions = []\n",
    "        \n",
    "        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬\n",
    "        for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Inference\"):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "            for img_path in batch_paths:\n",
    "                try:\n",
    "                    image = Image.open(img_path).convert('RGB')\n",
    "                    image = test_transforms(image)\n",
    "                    batch_images.append(image)\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {img_path} - {e}\")\n",
    "                    # ë”ë¯¸ ì´ë¯¸ì§€ ì¶”ê°€ (í‰ê· ê°’)\n",
    "                    dummy_image = torch.zeros(3, 224, 224)\n",
    "                    batch_images.append(dummy_image)\n",
    "            \n",
    "            if batch_images:\n",
    "                # ë°°ì¹˜ í…ì„œ ìƒì„±\n",
    "                batch_tensor = torch.stack(batch_images).to(self.device)\n",
    "                \n",
    "                # ì˜ˆì¸¡ ì‹¤í–‰\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(batch_tensor)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    \n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        return all_predictions\n",
    "    \n",
    "    def create_submission(self, test_ids, predictions, save_path=None):\n",
    "        \"\"\"ì œì¶œ íŒŒì¼ ìƒì„±\"\"\"\n",
    "        try:\n",
    "            # í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¥¼ í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ë³€í™˜\n",
    "            class_names = [idx_to_class[pred] for pred in predictions]\n",
    "            \n",
    "            # ì œì¶œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "            submission_df = pd.DataFrame({\n",
    "                'ID': test_ids,\n",
    "                'rock_type': class_names\n",
    "            })\n",
    "            \n",
    "            # íŒŒì¼ ì €ì¥\n",
    "            if save_path is None:\n",
    "                save_path = EXPERIMENT_DIR / \"submission.csv\"\n",
    "            \n",
    "            submission_df.to_csv(save_path, index=False)\n",
    "            \n",
    "            print(f\"âœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: {save_path}\")\n",
    "            print(f\"ğŸ“Š ì œì¶œ íŒŒì¼ ìš”ì•½:\")\n",
    "            print(submission_df['rock_type'].value_counts())\n",
    "            \n",
    "            return str(save_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì œì¶œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "\n",
    "def run_inference_demo():\n",
    "    \"\"\"ì¶”ë¡  ë°ëª¨ ì‹¤í–‰\"\"\"\n",
    "    print(\"ğŸ”® ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± ë°ëª¨\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ê°€ì¥ ìµœê·¼ì— ì €ì¥ëœ ëª¨ë¸ ì°¾ê¸°\n",
    "    model_files = list(MODEL_DIR.glob(\"*.pth\"))\n",
    "    \n",
    "    if not model_files:\n",
    "        print(\"âŒ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "        print(\"   ë¨¼ì € ëª¨ë¸ í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "    \n",
    "    # ê°€ì¥ ìµœê·¼ ëª¨ë¸ ì„ íƒ\n",
    "    latest_model = max(model_files, key=lambda x: x.stat().st_mtime)\n",
    "    print(f\"ğŸ“ ì‚¬ìš©í•  ëª¨ë¸: {latest_model.name}\")\n",
    "    \n",
    "    # ì¶”ë¡  ë§¤ë‹ˆì € ìƒì„±\n",
    "    inference_manager = InferenceManager(latest_model, device)\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ (ì²« ë²ˆì§¸ available ëª¨ë¸ ì‚¬ìš©)\n",
    "    if available_models:\n",
    "        model_name = available_models[0]['model_name']\n",
    "        success = inference_manager.load_model(model_name, NUM_CLASSES)\n",
    "        \n",
    "        if not success:\n",
    "            print(\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨!\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "        return\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    test_image_paths, test_ids = inference_manager.create_test_dataset()\n",
    "    \n",
    "    if not test_image_paths:\n",
    "        print(\"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "        print(\"   test.csv íŒŒì¼ê³¼ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "    \n",
    "    # ìƒ˜í”Œ ì¶”ë¡  (ì²˜ìŒ 100ê°œë§Œ)\n",
    "    print(f\"\\nğŸ”® ìƒ˜í”Œ ì¶”ë¡  ì‹¤í–‰ (ì²˜ìŒ 100ê°œ)\")\n",
    "    sample_paths = test_image_paths[:100]\n",
    "    sample_ids = test_ids[:100]\n",
    "    \n",
    "    predictions = inference_manager.predict_batch(sample_paths, batch_size=32)\n",
    "    \n",
    "    if predictions:\n",
    "        # ìƒ˜í”Œ ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "        submission_path = inference_manager.create_submission(\n",
    "            sample_ids, \n",
    "            predictions, \n",
    "            save_path=EXPERIMENT_DIR / \"sample_submission.csv\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nâœ… ìƒ˜í”Œ ì¶”ë¡  ì™„ë£Œ!\")\n",
    "        print(f\"ğŸ“ ì œì¶œ íŒŒì¼: {submission_path}\")\n",
    "        \n",
    "        # ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„\n",
    "        pred_counts = Counter(predictions)\n",
    "        print(f\"\\nğŸ“ˆ ì˜ˆì¸¡ ë¶„í¬:\")\n",
    "        for class_idx, count in sorted(pred_counts.items()):\n",
    "            class_name = idx_to_class[class_idx]\n",
    "            percentage = (count / len(predictions)) * 100\n",
    "            print(f\"   {class_name}: {count}ê°œ ({percentage:.1f}%)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ ì¶”ë¡  ì‹¤íŒ¨!\")\n",
    "\n",
    "# TTA (Test Time Augmentation) êµ¬í˜„\n",
    "def run_tta_inference():\n",
    "    \"\"\"TTAë¥¼ ì ìš©í•œ ê³ ê¸‰ ì¶”ë¡ \"\"\"\n",
    "    print(\"ğŸ”„ TTA (Test Time Augmentation) ì¶”ë¡ \")\n",
    "    print(\"   ì—¬ëŸ¬ ë³€í™˜ì„ ì ìš©í•˜ì—¬ ë” ì•ˆì •ì ì¸ ì˜ˆì¸¡\")\n",
    "    print(\"   (êµ¬í˜„ ì˜ˆì •)\")\n",
    "\n",
    "# ì•™ìƒë¸” ì¶”ë¡  êµ¬í˜„\n",
    "def run_ensemble_inference():\n",
    "    \"\"\"ì—¬ëŸ¬ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì•™ìƒë¸” ì¶”ë¡ \"\"\"\n",
    "    print(\"ğŸ¯ ì•™ìƒë¸” ì¶”ë¡ \")\n",
    "    print(\"   ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡\")\n",
    "    print(\"   (êµ¬í˜„ ì˜ˆì •)\")\n",
    "\n",
    "# ì¶”ë¡  ì‹¤í–‰\n",
    "print(\"ğŸ”® ì¶”ë¡  ëª¨ë“œ ì„ íƒ:\")\n",
    "print(\"1. ğŸ“ ê¸°ë³¸ ì¶”ë¡  (ë‹¨ì¼ ëª¨ë¸)\")\n",
    "print(\"2. ğŸ”„ TTA ì¶”ë¡  (êµ¬í˜„ ì˜ˆì •)\")\n",
    "print(\"3. ğŸ¯ ì•™ìƒë¸” ì¶”ë¡  (êµ¬í˜„ ì˜ˆì •)\")\n",
    "\n",
    "# ìë™ìœ¼ë¡œ ê¸°ë³¸ ì¶”ë¡  ì„ íƒ\n",
    "print(\"\\nğŸ“ ìë™ ì„ íƒ: ê¸°ë³¸ ì¶”ë¡ \")\n",
    "run_inference_demo()\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸\n",
      "================================================================================\n",
      "ğŸ—ï¸ í”„ë¡œì íŠ¸ ê°œìš”\n",
      "----------------------------------------\n",
      "ğŸ“‹ ëŒ€íšŒëª…: ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ì¢…ë¥˜ ë¶„ë¥˜ AI ê²½ì§„ëŒ€íšŒ\n",
      "ğŸ¯ ëª©í‘œ: 7ì¢… ì•”ì„ ë¶„ë¥˜ (Macro F1 Score ìµœì í™”)\n",
      "ğŸ“Š ë°ì´í„°: 380,020ì¥ í›ˆë ¨ ì´ë¯¸ì§€, 95,006ì¥ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€\n",
      "âš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•: 5.8:1 (Granite 92,923ì¥ vs Etc 15,935ì¥)\n",
      "\n",
      "ğŸ”¬ ê¸°ìˆ ì  ì ‘ê·¼ë²•\n",
      "----------------------------------------\n",
      "âœ… ìƒìœ„ íŒ€ ì „ëµ í†µí•©:\n",
      "   â€¢ 1ìœ„ íŒ€: 4ê°œ ëª¨ë¸ ì•™ìƒë¸” (EfficientNetV2 + RegNetY + TinyViT)\n",
      "   â€¢ 2ìœ„ íŒ€: InternImage + Hard Negative Sampling\n",
      "   â€¢ 3ìœ„ íŒ€: ConvNeXt (CNN + Transformer í•˜ì´ë¸Œë¦¬ë“œ)\n",
      "   â€¢ 4ìœ„ íŒ€: ì•ˆì •ì ì¸ ì „ì´í•™ìŠµ ì „ëµ\n",
      "\n",
      "âœ… êµ¬í˜„ëœ í•µì‹¬ ê¸°ë²•:\n",
      "   â€¢ WeightedRandomSampler: í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\n",
      "   â€¢ Hard Negative Sampling: ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘ í•™ìŠµ\n",
      "   â€¢ CutMix + Mixup: ê³ ê¸‰ ë°ì´í„° ì¦ê°•\n",
      "   â€¢ BICUBIC ë³´ê°„ë²•: ì´ë¯¸ì§€ í’ˆì§ˆ ìµœì í™”\n",
      "   â€¢ K-Fold êµì°¨ê²€ì¦: ëª¨ë¸ ì•ˆì •ì„± í™•ë³´\n",
      "   â€¢ OneCycleLR: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ìµœì í™”\n",
      "\n",
      "ğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ì„\n",
      "----------------------------------------\n",
      "1. EfficientNetV2-S (1ìœ„ íŒ€ ì‚¬ìš©)\n",
      "   íŒŒë¼ë¯¸í„°: 20,186,455ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 77.0MB\n",
      "   íŠ¹ì§•: íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•\n",
      "2. EfficientNetV2-M (1ìœ„ íŒ€ ì‚¬ìš©)\n",
      "   íŒŒë¼ë¯¸í„°: 52,867,323ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 201.7MB\n",
      "   íŠ¹ì§•: ë” í° ëª¨ë¸, ë†’ì€ ì„±ëŠ¥\n",
      "3. ConvNeXt-Tiny (3ìœ„ íŒ€ ì‚¬ìš©)\n",
      "   íŒŒë¼ë¯¸í„°: 27,825,511ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 106.1MB\n",
      "   íŠ¹ì§•: CNN + Transformer í•˜ì´ë¸Œë¦¬ë“œ\n",
      "4. ViT-Tiny (ë‹¤ì–‘í•œ íŒ€ ì‹¤í—˜)\n",
      "   íŒŒë¼ë¯¸í„°: 5,525,767ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 21.1MB\n",
      "   íŠ¹ì§•: Vision Transformer\n",
      "\n",
      "ğŸ¨ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
      "----------------------------------------\n",
      "âœ… í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°:\n",
      "   â€¢ ì›ë³¸ ë¶ˆê· í˜•: 5.8:1 â†’ WeightedRandomSampler â†’ 1.0:1\n",
      "   â€¢ ê°€ì¤‘ì¹˜ ë°©ì‹: ì œê³±ê·¼ ì—­ë¹ˆë„ (ì•ˆì •ì„± í™•ë³´)\n",
      "\n",
      "âœ… ì´ë¯¸ì§€ í’ˆì§ˆ ìµœì í™”:\n",
      "   â€¢ ë¦¬ì‚¬ì´ì§•: 224Ã—224 (í‘œì¤€ ImageNet í¬ê¸°)\n",
      "   â€¢ ë³´ê°„ë²•: BICUBIC (ì•”ì„ í…ìŠ¤ì²˜ ë³´ì¡´ ìµœì )\n",
      "   â€¢ ì •ê·œí™”: ImageNet í‘œì¤€ (mean=[0.485, 0.456, 0.406])\n",
      "\n",
      "âœ… ë°ì´í„° ì¦ê°•:\n",
      "   â€¢ ê¸°ë³¸ ì¦ê°•: íšŒì „(15Â°), ë’¤ì§‘ê¸°, ìƒ‰ìƒ ë³€í™”\n",
      "   â€¢ ê³ ê¸‰ ì¦ê°•: CutMix (Î±=1.0), Mixup (Î±=0.2)\n",
      "   â€¢ í™•ë¥ ì  ì ìš©: CutMix 50%, Mixup 50%\n",
      "\n",
      "ğŸ¯ í•™ìŠµ ì „ëµ\n",
      "----------------------------------------\n",
      "âœ… ì˜µí‹°ë§ˆì´ì €: AdamW\n",
      "   â€¢ í•™ìŠµë¥ : 0.0003\n",
      "   â€¢ Weight Decay: 0.0001\n",
      "\n",
      "âœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§: OneCycleLR\n",
      "   â€¢ ì›Œë°ì—…: 30% êµ¬ê°„ì—ì„œ ìµœëŒ€ í•™ìŠµë¥ ê¹Œì§€ ì¦ê°€\n",
      "   â€¢ ê°ì†Œ: ë‚˜ë¨¸ì§€ 70% êµ¬ê°„ì—ì„œ ì ì§„ì  ê°ì†Œ\n",
      "\n",
      "âœ… Hard Negative Sampling:\n",
      "   â€¢ ë©”ëª¨ë¦¬ í¬ê¸°: 1000ê°œ ìƒ˜í”Œ\n",
      "   â€¢ ì†ì‹¤ ì„ê³„ê°’: 1.5\n",
      "   â€¢ ë°°ì¹˜ ë¹„ìœ¨: 20.0%\n",
      "\n",
      "ğŸ“Š ê²€ì¦ ì „ëµ\n",
      "----------------------------------------\n",
      "âœ… K-Fold êµì°¨ê²€ì¦: 5-Fold\n",
      "âœ… ê³„ì¸µí™” ë¶„í• : í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€\n",
      "âœ… í‰ê°€ ì§€í‘œ: Macro F1 Score (ëŒ€íšŒ ê¸°ì¤€)\n",
      "âœ… Early Stopping: ì„±ëŠ¥ ì €í•˜ ì‹œ ì¡°ê¸° ì¢…ë£Œ\n",
      "âœ… ì‹¤í—˜ ì„¤ì • ì €ì¥: ..\\experiments\\experiment_config.json\n",
      "\n",
      "ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ í¬íŠ¸í´ë¦¬ì˜¤ ìš”ì•½ ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨\n",
      "\n",
      "ğŸ“‹ í”„ë¡œì íŠ¸ ì œëª©: ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ë¶„ë¥˜ AI ì‹œìŠ¤í…œ\n",
      "ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: Computer Vision, Image Classification, Deep Learning\n",
      "ğŸ“… ê¸°ê°„: 2025ë…„ (í•™ìŠµ ëª©ì  í”„ë¡œì íŠ¸)\n",
      "\n",
      "ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ:\n",
      "   â€¢ 7ì¢… ì•”ì„ì˜ ìë™ ë¶„ë¥˜ ì‹œìŠ¤í…œ ê°œë°œ\n",
      "   â€¢ ê±´ì„¤ í˜„ì¥ í’ˆì§ˆ ê²€ì‚¬ ìë™í™” ê¸°ì—¬\n",
      "   â€¢ ìƒìœ„ íŒ€ ì „ëµ ë¶„ì„ ë° í†µí•© êµ¬í˜„\n",
      "\n",
      "ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜:\n",
      "   â€¢ ê±´ì„¤ í˜„ì¥ì˜ ë””ì§€í„¸ ì „í™˜ ì§€ì›\n",
      "   â€¢ í’ˆì§ˆ ê²€ì‚¬ ìë™í™”ë¡œ ë¹„ìš© ì ˆê°\n",
      "   â€¢ ì¸ë ¥ ì˜ì¡´ë„ ê°ì†Œ ë° ì •í™•ë„ í–¥ìƒ\n",
      "\n",
      "ğŸ”¬ ê¸°ìˆ ì  ì„±ê³¼:\n",
      "   â€¢ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ ì™„ì „ í•´ê²° (5.8:1 â†’ 1.0:1)\n",
      "   â€¢ ìƒìœ„ íŒ€ ê¸°ë²• ì„±ê³µì  í†µí•© (Hard Negative Sampling ë“±)\n",
      "   â€¢ ê³ ê¸‰ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n",
      "   â€¢ ì²´ê³„ì ì¸ K-Fold êµì°¨ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´\n",
      "\n",
      "ğŸ› ï¸ ì‚¬ìš© ê¸°ìˆ :\n",
      "   â€¢ PyTorch, timm, scikit-learn\n",
      "   â€¢ EfficientNetV2, ConvNeXt, Vision Transformer\n",
      "   â€¢ CutMix, Mixup, WeightedRandomSampler\n",
      "   â€¢ K-Fold Cross Validation, Hard Negative Sampling\n",
      "\n",
      "ğŸ“ˆ í•™ìŠµ ì„±ê³¼:\n",
      "   â€¢ ë°ì´í„° ë¶„ì„ ë° ë¬¸ì œ ì •ì˜ ëŠ¥ë ¥\n",
      "   â€¢ ìµœì‹  ë”¥ëŸ¬ë‹ ê¸°ë²• ì ìš© ê²½í—˜\n",
      "   â€¢ ì‹¤ë¬´ ìˆ˜ì¤€ì˜ ì½”ë“œ êµ¬ì¡°í™” ë° ë¬¸ì„œí™”\n",
      "   â€¢ ìƒìœ„ íŒ€ ë¶„ì„ì„ í†µí•œ ë²¤ì¹˜ë§ˆí‚¹ ëŠ¥ë ¥\n",
      "\n",
      "ğŸ”— í™œìš© ë¶„ì•¼:\n",
      "   â€¢ ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬ ìë™í™”\n",
      "   â€¢ ì˜ë£Œ ì˜ìƒ ì§„ë‹¨ ì‹œìŠ¤í…œ\n",
      "   â€¢ ë†ì—… ì‘ë¬¼ ìƒíƒœ ë¶„ë¥˜\n",
      "   â€¢ ë¦¬í…Œì¼ ìƒí’ˆ ë¶„ë¥˜ ì‹œìŠ¤í…œ\n",
      "\n",
      "ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨\n",
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
      "ğŸ† ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ë¶„ë¥˜ AI ëª¨ë¸ í•™ìŠµ ì‹œìŠ¤í…œ ì™„ì„±!\n",
      "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
      "\n",
      "âœ… ì™„ì„±ëœ ê¸°ëŠ¥:\n",
      "   1. ğŸ¨ ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
      "   2. ğŸ§  ìƒìœ„ íŒ€ ëª¨ë¸ ì•„í‚¤í…ì²˜ í†µí•©\n",
      "   3. ğŸ¯ Hard Negative Sampling êµ¬í˜„\n",
      "   4. ğŸ“Š K-Fold êµì°¨ê²€ì¦ ì‹œìŠ¤í…œ\n",
      "   5. ğŸ”® ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "   6. ğŸ“‹ ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„\n",
      "\n",
      "ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\n",
      "   â€¢ ì‹¤ì œ ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ (ì…€ 7ë²ˆ)\n",
      "   â€¢ K-Fold êµì°¨ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ ë¹„êµ\n",
      "   â€¢ ì•™ìƒë¸” ê¸°ë²•ìœ¼ë¡œ ìµœì¢… ì„±ëŠ¥ í–¥ìƒ\n",
      "   â€¢ ì‹¤ì œ ëŒ€íšŒ ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "\n",
      "ğŸš€ ì´ì œ ì‹¤ì œ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„ ë° í¬íŠ¸í´ë¦¬ì˜¤ ì •ë¦¬\n",
    "# ============================================================================\n",
    "\n",
    "def generate_experiment_report():\n",
    "    \"\"\"ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "    print(\"ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # í”„ë¡œì íŠ¸ ê°œìš”\n",
    "    print(\"ğŸ—ï¸ í”„ë¡œì íŠ¸ ê°œìš”\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"ğŸ“‹ ëŒ€íšŒëª…: ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ì¢…ë¥˜ ë¶„ë¥˜ AI ê²½ì§„ëŒ€íšŒ\")\n",
    "    print(\"ğŸ¯ ëª©í‘œ: 7ì¢… ì•”ì„ ë¶„ë¥˜ (Macro F1 Score ìµœì í™”)\")\n",
    "    print(\"ğŸ“Š ë°ì´í„°: 380,020ì¥ í›ˆë ¨ ì´ë¯¸ì§€, 95,006ì¥ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€\")\n",
    "    print(\"âš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•: 5.8:1 (Granite 92,923ì¥ vs Etc 15,935ì¥)\")\n",
    "    \n",
    "    # ê¸°ìˆ ì  ì ‘ê·¼ë²•\n",
    "    print(f\"\\nğŸ”¬ ê¸°ìˆ ì  ì ‘ê·¼ë²•\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"âœ… ìƒìœ„ íŒ€ ì „ëµ í†µí•©:\")\n",
    "    print(\"   â€¢ 1ìœ„ íŒ€: 4ê°œ ëª¨ë¸ ì•™ìƒë¸” (EfficientNetV2 + RegNetY + TinyViT)\")\n",
    "    print(\"   â€¢ 2ìœ„ íŒ€: InternImage + Hard Negative Sampling\")  \n",
    "    print(\"   â€¢ 3ìœ„ íŒ€: ConvNeXt (CNN + Transformer í•˜ì´ë¸Œë¦¬ë“œ)\")\n",
    "    print(\"   â€¢ 4ìœ„ íŒ€: ì•ˆì •ì ì¸ ì „ì´í•™ìŠµ ì „ëµ\")\n",
    "    \n",
    "    print(f\"\\nâœ… êµ¬í˜„ëœ í•µì‹¬ ê¸°ë²•:\")\n",
    "    print(\"   â€¢ WeightedRandomSampler: í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\")\n",
    "    print(\"   â€¢ Hard Negative Sampling: ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘ í•™ìŠµ\")\n",
    "    print(\"   â€¢ CutMix + Mixup: ê³ ê¸‰ ë°ì´í„° ì¦ê°•\")\n",
    "    print(\"   â€¢ BICUBIC ë³´ê°„ë²•: ì´ë¯¸ì§€ í’ˆì§ˆ ìµœì í™”\")\n",
    "    print(\"   â€¢ K-Fold êµì°¨ê²€ì¦: ëª¨ë¸ ì•ˆì •ì„± í™•ë³´\")\n",
    "    print(\"   â€¢ OneCycleLR: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ìµœì í™”\")\n",
    "    \n",
    "    # ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ì„\n",
    "    print(f\"\\nğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ì„\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, model_data in enumerate(available_models, 1):\n",
    "        config = model_data['config']\n",
    "        info = model_data['info']\n",
    "        print(f\"{i}. {config['name']} ({config['team']})\")\n",
    "        print(f\"   íŒŒë¼ë¯¸í„°: {info['total_params']:,}ê°œ\")\n",
    "        print(f\"   ëª¨ë¸ í¬ê¸°: {info['size_mb']:.1f}MB\")\n",
    "        print(f\"   íŠ¹ì§•: {config['description']}\")\n",
    "    \n",
    "    # ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "    print(f\"\\nğŸ¨ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"âœ… í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°:\")\n",
    "    print(f\"   â€¢ ì›ë³¸ ë¶ˆê· í˜•: 5.8:1 â†’ WeightedRandomSampler â†’ 1.0:1\")\n",
    "    print(f\"   â€¢ ê°€ì¤‘ì¹˜ ë°©ì‹: ì œê³±ê·¼ ì—­ë¹ˆë„ (ì•ˆì •ì„± í™•ë³´)\")\n",
    "    \n",
    "    print(f\"\\nâœ… ì´ë¯¸ì§€ í’ˆì§ˆ ìµœì í™”:\")\n",
    "    print(f\"   â€¢ ë¦¬ì‚¬ì´ì§•: 224Ã—224 (í‘œì¤€ ImageNet í¬ê¸°)\")\n",
    "    print(f\"   â€¢ ë³´ê°„ë²•: BICUBIC (ì•”ì„ í…ìŠ¤ì²˜ ë³´ì¡´ ìµœì )\")\n",
    "    print(f\"   â€¢ ì •ê·œí™”: ImageNet í‘œì¤€ (mean=[0.485, 0.456, 0.406])\")\n",
    "    \n",
    "    print(f\"\\nâœ… ë°ì´í„° ì¦ê°•:\")\n",
    "    print(f\"   â€¢ ê¸°ë³¸ ì¦ê°•: íšŒì „(15Â°), ë’¤ì§‘ê¸°, ìƒ‰ìƒ ë³€í™”\")\n",
    "    print(f\"   â€¢ ê³ ê¸‰ ì¦ê°•: CutMix (Î±=1.0), Mixup (Î±=0.2)\")\n",
    "    print(f\"   â€¢ í™•ë¥ ì  ì ìš©: CutMix 50%, Mixup 50%\")\n",
    "    \n",
    "    # í•™ìŠµ ì „ëµ\n",
    "    print(f\"\\nğŸ¯ í•™ìŠµ ì „ëµ\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"âœ… ì˜µí‹°ë§ˆì´ì €: AdamW\")\n",
    "    print(f\"   â€¢ í•™ìŠµë¥ : {HYPERPARAMETERS['lr']}\")\n",
    "    print(f\"   â€¢ Weight Decay: {HYPERPARAMETERS['weight_decay']}\")\n",
    "    \n",
    "    print(f\"\\nâœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§: OneCycleLR\")\n",
    "    print(f\"   â€¢ ì›Œë°ì—…: 30% êµ¬ê°„ì—ì„œ ìµœëŒ€ í•™ìŠµë¥ ê¹Œì§€ ì¦ê°€\")\n",
    "    print(f\"   â€¢ ê°ì†Œ: ë‚˜ë¨¸ì§€ 70% êµ¬ê°„ì—ì„œ ì ì§„ì  ê°ì†Œ\")\n",
    "    \n",
    "    print(f\"\\nâœ… Hard Negative Sampling:\")\n",
    "    print(f\"   â€¢ ë©”ëª¨ë¦¬ í¬ê¸°: {HYPERPARAMETERS['hard_memory_size']}ê°œ ìƒ˜í”Œ\")\n",
    "    print(f\"   â€¢ ì†ì‹¤ ì„ê³„ê°’: {HYPERPARAMETERS['loss_threshold']}\")\n",
    "    print(f\"   â€¢ ë°°ì¹˜ ë¹„ìœ¨: {HYPERPARAMETERS['hard_negative_ratio']*100}%\")\n",
    "    \n",
    "    # ê²€ì¦ ì „ëµ\n",
    "    print(f\"\\nğŸ“Š ê²€ì¦ ì „ëµ\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"âœ… K-Fold êµì°¨ê²€ì¦: {HYPERPARAMETERS['kfold_splits']}-Fold\")\n",
    "    print(f\"âœ… ê³„ì¸µí™” ë¶„í• : í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€\")\n",
    "    print(f\"âœ… í‰ê°€ ì§€í‘œ: Macro F1 Score (ëŒ€íšŒ ê¸°ì¤€)\")\n",
    "    print(f\"âœ… Early Stopping: ì„±ëŠ¥ ì €í•˜ ì‹œ ì¡°ê¸° ì¢…ë£Œ\")\n",
    "\n",
    "def save_experiment_config():\n",
    "    \"\"\"ì‹¤í—˜ ì„¤ì •ì„ JSONìœ¼ë¡œ ì €ì¥\"\"\"\n",
    "    config_data = {\n",
    "        'project_info': {\n",
    "            'competition': 'ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ì¢…ë¥˜ ë¶„ë¥˜ AI ê²½ì§„ëŒ€íšŒ',\n",
    "            'num_classes': NUM_CLASSES,\n",
    "            'class_names': sorted(CLASS_NAMES),\n",
    "            'total_train_images': len(train_dataset),\n",
    "            'class_imbalance_ratio': '5.8:1'\n",
    "        },\n",
    "        'hyperparameters': HYPERPARAMETERS,\n",
    "        'models': [\n",
    "            {\n",
    "                'name': model_data['config']['name'],\n",
    "                'model_name': model_data['model_name'],\n",
    "                'team': model_data['config']['team'],\n",
    "                'description': model_data['config']['description'],\n",
    "                'parameters': model_data['info']['total_params'],\n",
    "                'size_mb': model_data['info']['size_mb']\n",
    "            }\n",
    "            for model_data in available_models\n",
    "        ],\n",
    "        'techniques': {\n",
    "            'sampling': 'WeightedRandomSampler + Hard Negative Sampling',\n",
    "            'augmentation': 'CutMix + Mixup + Basic Augmentation',\n",
    "            'interpolation': 'BICUBIC',\n",
    "            'validation': 'Stratified K-Fold',\n",
    "            'optimizer': 'AdamW',\n",
    "            'scheduler': 'OneCycleLR'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = EXPERIMENT_DIR / \"experiment_config.json\"\n",
    "    with open(config_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(config_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… ì‹¤í—˜ ì„¤ì • ì €ì¥: {config_path}\")\n",
    "\n",
    "def print_portfolio_summary():\n",
    "    \"\"\"í¬íŠ¸í´ë¦¬ì˜¤ìš© í”„ë¡œì íŠ¸ ìš”ì•½\"\"\"\n",
    "    print(\"\\n\" + \"ğŸ¨\" * 25 + \" í¬íŠ¸í´ë¦¬ì˜¤ ìš”ì•½ \" + \"ğŸ¨\" * 25)\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ“‹ í”„ë¡œì íŠ¸ ì œëª©: ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ë¶„ë¥˜ AI ì‹œìŠ¤í…œ\")\n",
    "    print(\"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: Computer Vision, Image Classification, Deep Learning\")\n",
    "    print(\"ğŸ“… ê¸°ê°„: 2025ë…„ (í•™ìŠµ ëª©ì  í”„ë¡œì íŠ¸)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ:\")\n",
    "    print(\"   â€¢ 7ì¢… ì•”ì„ì˜ ìë™ ë¶„ë¥˜ ì‹œìŠ¤í…œ ê°œë°œ\")\n",
    "    print(\"   â€¢ ê±´ì„¤ í˜„ì¥ í’ˆì§ˆ ê²€ì‚¬ ìë™í™” ê¸°ì—¬\")\n",
    "    print(\"   â€¢ ìƒìœ„ íŒ€ ì „ëµ ë¶„ì„ ë° í†µí•© êµ¬í˜„\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜:\")\n",
    "    print(\"   â€¢ ê±´ì„¤ í˜„ì¥ì˜ ë””ì§€í„¸ ì „í™˜ ì§€ì›\")\n",
    "    print(\"   â€¢ í’ˆì§ˆ ê²€ì‚¬ ìë™í™”ë¡œ ë¹„ìš© ì ˆê°\")\n",
    "    print(\"   â€¢ ì¸ë ¥ ì˜ì¡´ë„ ê°ì†Œ ë° ì •í™•ë„ í–¥ìƒ\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ”¬ ê¸°ìˆ ì  ì„±ê³¼:\")\n",
    "    print(\"   â€¢ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ ì™„ì „ í•´ê²° (5.8:1 â†’ 1.0:1)\")\n",
    "    print(\"   â€¢ ìƒìœ„ íŒ€ ê¸°ë²• ì„±ê³µì  í†µí•© (Hard Negative Sampling ë“±)\")\n",
    "    print(\"   â€¢ ê³ ê¸‰ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\")\n",
    "    print(\"   â€¢ ì²´ê³„ì ì¸ K-Fold êµì°¨ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ› ï¸ ì‚¬ìš© ê¸°ìˆ :\")\n",
    "    print(\"   â€¢ PyTorch, timm, scikit-learn\")\n",
    "    print(\"   â€¢ EfficientNetV2, ConvNeXt, Vision Transformer\")\n",
    "    print(\"   â€¢ CutMix, Mixup, WeightedRandomSampler\")\n",
    "    print(\"   â€¢ K-Fold Cross Validation, Hard Negative Sampling\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ“ˆ í•™ìŠµ ì„±ê³¼:\")\n",
    "    print(\"   â€¢ ë°ì´í„° ë¶„ì„ ë° ë¬¸ì œ ì •ì˜ ëŠ¥ë ¥\")\n",
    "    print(\"   â€¢ ìµœì‹  ë”¥ëŸ¬ë‹ ê¸°ë²• ì ìš© ê²½í—˜\")\n",
    "    print(\"   â€¢ ì‹¤ë¬´ ìˆ˜ì¤€ì˜ ì½”ë“œ êµ¬ì¡°í™” ë° ë¬¸ì„œí™”\")\n",
    "    print(\"   â€¢ ìƒìœ„ íŒ€ ë¶„ì„ì„ í†µí•œ ë²¤ì¹˜ë§ˆí‚¹ ëŠ¥ë ¥\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ”— í™œìš© ë¶„ì•¼:\")\n",
    "    print(\"   â€¢ ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬ ìë™í™”\")\n",
    "    print(\"   â€¢ ì˜ë£Œ ì˜ìƒ ì§„ë‹¨ ì‹œìŠ¤í…œ\")\n",
    "    print(\"   â€¢ ë†ì—… ì‘ë¬¼ ìƒíƒœ ë¶„ë¥˜\")\n",
    "    print(\"   â€¢ ë¦¬í…Œì¼ ìƒí’ˆ ë¶„ë¥˜ ì‹œìŠ¤í…œ\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ¨\" * 60)\n",
    "\n",
    "# ì‹¤í–‰\n",
    "generate_experiment_report()\n",
    "save_experiment_config()\n",
    "print_portfolio_summary()\n",
    "\n",
    "# ìµœì¢… ë©”ì‹œì§€\n",
    "print(\"\\n\" + \"ğŸ‰\" * 30)\n",
    "print(\"ğŸ† ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ë¶„ë¥˜ AI ëª¨ë¸ í•™ìŠµ ì‹œìŠ¤í…œ ì™„ì„±!\")\n",
    "print(\"ğŸ‰\" * 30)\n",
    "print()\n",
    "print(\"âœ… ì™„ì„±ëœ ê¸°ëŠ¥:\")\n",
    "print(\"   1. ğŸ¨ ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\")\n",
    "print(\"   2. ğŸ§  ìƒìœ„ íŒ€ ëª¨ë¸ ì•„í‚¤í…ì²˜ í†µí•©\")\n",
    "print(\"   3. ğŸ¯ Hard Negative Sampling êµ¬í˜„\")\n",
    "print(\"   4. ğŸ“Š K-Fold êµì°¨ê²€ì¦ ì‹œìŠ¤í…œ\")\n",
    "print(\"   5. ğŸ”® ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"   6. ğŸ“‹ ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„\")\n",
    "print()\n",
    "print(\"ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"   â€¢ ì‹¤ì œ ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ (ì…€ 7ë²ˆ)\")\n",
    "print(\"   â€¢ K-Fold êµì°¨ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"   â€¢ ì•™ìƒë¸” ê¸°ë²•ìœ¼ë¡œ ìµœì¢… ì„±ëŠ¥ í–¥ìƒ\")\n",
    "print(\"   â€¢ ì‹¤ì œ ëŒ€íšŒ ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print()\n",
    "print(\"ğŸš€ ì´ì œ ì‹¤ì œ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431010d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5707476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5d186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed54724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸\n",
      "================================================================================\n",
      "ğŸ—ï¸ í”„ë¡œì íŠ¸ ê°œìš”\n",
      "----------------------------------------\n",
      "ğŸ“‹ ëŒ€íšŒëª…: ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ì¢…ë¥˜ ë¶„ë¥˜ AI ê²½ì§„ëŒ€íšŒ\n",
      "ğŸ¯ ëª©í‘œ: 7ì¢… ì•”ì„ ë¶„ë¥˜ (Macro F1 Score ìµœì í™”)\n",
      "ğŸ“Š ë°ì´í„°: 380,020ì¥ í›ˆë ¨ ì´ë¯¸ì§€, 95,006ì¥ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€\n",
      "âš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•: 5.8:1 (Granite 92,923ì¥ vs Etc 15,935ì¥)\n",
      "\n",
      "ğŸ”¬ ê¸°ìˆ ì  ì ‘ê·¼ë²•\n",
      "----------------------------------------\n",
      "âœ… ìƒìœ„ íŒ€ ì „ëµ í†µí•©:\n",
      "   â€¢ 1ìœ„ íŒ€: 4ê°œ ëª¨ë¸ ì•™ìƒë¸” (EfficientNetV2 + RegNetY + TinyViT)\n",
      "   â€¢ 2ìœ„ íŒ€: InternImage + Hard Negative Sampling\n",
      "   â€¢ 3ìœ„ íŒ€: ConvNeXt (CNN + Transformer í•˜ì´ë¸Œë¦¬ë“œ)\n",
      "   â€¢ 4ìœ„ íŒ€: ì•ˆì •ì ì¸ ì „ì´í•™ìŠµ ì „ëµ\n",
      "\n",
      "âœ… êµ¬í˜„ëœ í•µì‹¬ ê¸°ë²•:\n",
      "   â€¢ WeightedRandomSampler: í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\n",
      "   â€¢ Hard Negative Sampling: ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘ í•™ìŠµ\n",
      "   â€¢ CutMix + Mixup: ê³ ê¸‰ ë°ì´í„° ì¦ê°•\n",
      "   â€¢ BICUBIC ë³´ê°„ë²•: ì´ë¯¸ì§€ í’ˆì§ˆ ìµœì í™”\n",
      "   â€¢ K-Fold êµì°¨ê²€ì¦: ëª¨ë¸ ì•ˆì •ì„± í™•ë³´\n",
      "   â€¢ OneCycleLR: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ìµœì í™”\n",
      "\n",
      "ğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ì„\n",
      "----------------------------------------\n",
      "1. EfficientNetV2-S (1ìœ„ íŒ€ ì‚¬ìš©)\n",
      "   íŒŒë¼ë¯¸í„°: 20,186,455ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 77.0MB\n",
      "   íŠ¹ì§•: íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•\n",
      "2. EfficientNetV2-M (1ìœ„ íŒ€ ì‚¬ìš©)\n",
      "   íŒŒë¼ë¯¸í„°: 52,867,323ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 201.7MB\n",
      "   íŠ¹ì§•: ë” í° ëª¨ë¸, ë†’ì€ ì„±ëŠ¥\n",
      "3. ConvNeXt-Tiny (3ìœ„ íŒ€ ì‚¬ìš©)\n",
      "   íŒŒë¼ë¯¸í„°: 27,825,511ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 106.1MB\n",
      "   íŠ¹ì§•: CNN + Transformer í•˜ì´ë¸Œë¦¬ë“œ\n",
      "4. ViT-Tiny (ë‹¤ì–‘í•œ íŒ€ ì‹¤í—˜)\n",
      "   íŒŒë¼ë¯¸í„°: 5,525,767ê°œ\n",
      "   ëª¨ë¸ í¬ê¸°: 21.1MB\n",
      "   íŠ¹ì§•: Vision Transformer\n",
      "\n",
      "ğŸ¨ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
      "----------------------------------------\n",
      "âœ… í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°:\n",
      "   â€¢ ì›ë³¸ ë¶ˆê· í˜•: 5.8:1 â†’ WeightedRandomSampler â†’ 1.0:1\n",
      "   â€¢ ê°€ì¤‘ì¹˜ ë°©ì‹: ì œê³±ê·¼ ì—­ë¹ˆë„ (ì•ˆì •ì„± í™•ë³´)\n",
      "\n",
      "âœ… ì´ë¯¸ì§€ í’ˆì§ˆ ìµœì í™”:\n",
      "   â€¢ ë¦¬ì‚¬ì´ì§•: 224Ã—224 (í‘œì¤€ ImageNet í¬ê¸°)\n",
      "   â€¢ ë³´ê°„ë²•: BICUBIC (ì•”ì„ í…ìŠ¤ì²˜ ë³´ì¡´ ìµœì )\n",
      "   â€¢ ì •ê·œí™”: ImageNet í‘œì¤€ (mean=[0.485, 0.456, 0.406])\n",
      "\n",
      "âœ… ë°ì´í„° ì¦ê°•:\n",
      "   â€¢ ê¸°ë³¸ ì¦ê°•: íšŒì „(15Â°), ë’¤ì§‘ê¸°, ìƒ‰ìƒ ë³€í™”\n",
      "   â€¢ ê³ ê¸‰ ì¦ê°•: CutMix (Î±=1.0), Mixup (Î±=0.2)\n",
      "   â€¢ í™•ë¥ ì  ì ìš©: CutMix 50%, Mixup 50%\n",
      "\n",
      "ğŸ¯ í•™ìŠµ ì „ëµ\n",
      "----------------------------------------\n",
      "âœ… ì˜µí‹°ë§ˆì´ì €: AdamW\n",
      "   â€¢ í•™ìŠµë¥ : 0.0003\n",
      "   â€¢ Weight Decay: 0.0001\n",
      "\n",
      "âœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§: OneCycleLR\n",
      "   â€¢ ì›Œë°ì—…: 30% êµ¬ê°„ì—ì„œ ìµœëŒ€ í•™ìŠµë¥ ê¹Œì§€ ì¦ê°€\n",
      "   â€¢ ê°ì†Œ: ë‚˜ë¨¸ì§€ 70% êµ¬ê°„ì—ì„œ ì ì§„ì  ê°ì†Œ\n",
      "\n",
      "âœ… Hard Negative Sampling:\n",
      "   â€¢ ë©”ëª¨ë¦¬ í¬ê¸°: 1000ê°œ ìƒ˜í”Œ\n",
      "   â€¢ ì†ì‹¤ ì„ê³„ê°’: 1.5\n",
      "   â€¢ ë°°ì¹˜ ë¹„ìœ¨: 20.0%\n",
      "\n",
      "ğŸ“Š ê²€ì¦ ì „ëµ\n",
      "----------------------------------------\n",
      "âœ… K-Fold êµì°¨ê²€ì¦: 5-Fold\n",
      "âœ… ê³„ì¸µí™” ë¶„í• : í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€\n",
      "âœ… í‰ê°€ ì§€í‘œ: Macro F1 Score (ëŒ€íšŒ ê¸°ì¤€)\n",
      "âœ… Early Stopping: ì„±ëŠ¥ ì €í•˜ ì‹œ ì¡°ê¸° ì¢…ë£Œ\n",
      "âœ… ì‹¤í—˜ ì„¤ì • ì €ì¥: ..\\experiments\\experiment_config.json\n",
      "\n",
      "ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ í¬íŠ¸í´ë¦¬ì˜¤ ìš”ì•½ ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨\n",
      "\n",
      "ğŸ“‹ í”„ë¡œì íŠ¸ ì œëª©: ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ë¶„ë¥˜ AI ì‹œìŠ¤í…œ\n",
      "ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: Computer Vision, Image Classification, Deep Learning\n",
      "ğŸ“… ê¸°ê°„: 2025ë…„ (í•™ìŠµ ëª©ì  í”„ë¡œì íŠ¸)\n",
      "\n",
      "ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ:\n",
      "   â€¢ 7ì¢… ì•”ì„ì˜ ìë™ ë¶„ë¥˜ ì‹œìŠ¤í…œ ê°œë°œ\n",
      "   â€¢ ê±´ì„¤ í˜„ì¥ í’ˆì§ˆ ê²€ì‚¬ ìë™í™” ê¸°ì—¬\n",
      "   â€¢ ìƒìœ„ íŒ€ ì „ëµ ë¶„ì„ ë° í†µí•© êµ¬í˜„\n",
      "\n",
      "ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜:\n",
      "   â€¢ ê±´ì„¤ í˜„ì¥ì˜ ë””ì§€í„¸ ì „í™˜ ì§€ì›\n",
      "   â€¢ í’ˆì§ˆ ê²€ì‚¬ ìë™í™”ë¡œ ë¹„ìš© ì ˆê°\n",
      "   â€¢ ì¸ë ¥ ì˜ì¡´ë„ ê°ì†Œ ë° ì •í™•ë„ í–¥ìƒ\n",
      "\n",
      "ğŸ”¬ ê¸°ìˆ ì  ì„±ê³¼:\n",
      "   â€¢ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ ì™„ì „ í•´ê²° (5.8:1 â†’ 1.0:1)\n",
      "   â€¢ ìƒìœ„ íŒ€ ê¸°ë²• ì„±ê³µì  í†µí•© (Hard Negative Sampling ë“±)\n",
      "   â€¢ ê³ ê¸‰ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n",
      "   â€¢ ì²´ê³„ì ì¸ K-Fold êµì°¨ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´\n",
      "\n",
      "ğŸ› ï¸ ì‚¬ìš© ê¸°ìˆ :\n",
      "   â€¢ PyTorch, timm, scikit-learn\n",
      "   â€¢ EfficientNetV2, ConvNeXt, Vision Transformer\n",
      "   â€¢ CutMix, Mixup, WeightedRandomSampler\n",
      "   â€¢ K-Fold Cross Validation, Hard Negative Sampling\n",
      "\n",
      "ğŸ“ˆ í•™ìŠµ ì„±ê³¼:\n",
      "   â€¢ ë°ì´í„° ë¶„ì„ ë° ë¬¸ì œ ì •ì˜ ëŠ¥ë ¥\n",
      "   â€¢ ìµœì‹  ë”¥ëŸ¬ë‹ ê¸°ë²• ì ìš© ê²½í—˜\n",
      "   â€¢ ì‹¤ë¬´ ìˆ˜ì¤€ì˜ ì½”ë“œ êµ¬ì¡°í™” ë° ë¬¸ì„œí™”\n",
      "   â€¢ ìƒìœ„ íŒ€ ë¶„ì„ì„ í†µí•œ ë²¤ì¹˜ë§ˆí‚¹ ëŠ¥ë ¥\n",
      "\n",
      "ğŸ”— í™œìš© ë¶„ì•¼:\n",
      "   â€¢ ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬ ìë™í™”\n",
      "   â€¢ ì˜ë£Œ ì˜ìƒ ì§„ë‹¨ ì‹œìŠ¤í…œ\n",
      "   â€¢ ë†ì—… ì‘ë¬¼ ìƒíƒœ ë¶„ë¥˜\n",
      "   â€¢ ë¦¬í…Œì¼ ìƒí’ˆ ë¶„ë¥˜ ì‹œìŠ¤í…œ\n",
      "\n",
      "ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨ğŸ¨\n",
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
      "ğŸ† ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ë¶„ë¥˜ AI ëª¨ë¸ í•™ìŠµ ì‹œìŠ¤í…œ ì™„ì„±!\n",
      "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
      "\n",
      "âœ… ì™„ì„±ëœ ê¸°ëŠ¥:\n",
      "   1. ğŸ¨ ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
      "   2. ğŸ§  ìƒìœ„ íŒ€ ëª¨ë¸ ì•„í‚¤í…ì²˜ í†µí•©\n",
      "   3. ğŸ¯ Hard Negative Sampling êµ¬í˜„\n",
      "   4. ğŸ“Š K-Fold êµì°¨ê²€ì¦ ì‹œìŠ¤í…œ\n",
      "   5. ğŸ”® ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "   6. ğŸ“‹ ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„\n",
      "\n",
      "ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\n",
      "   â€¢ ì‹¤ì œ ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ (ì…€ 7ë²ˆ)\n",
      "   â€¢ K-Fold êµì°¨ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ ë¹„êµ\n",
      "   â€¢ ì•™ìƒë¸” ê¸°ë²•ìœ¼ë¡œ ìµœì¢… ì„±ëŠ¥ í–¥ìƒ\n",
      "   â€¢ ì‹¤ì œ ëŒ€íšŒ ì œì¶œ íŒŒì¼ ìƒì„±\n",
      "\n",
      "ğŸš€ ì´ì œ ì‹¤ì œ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„ ë° í¬íŠ¸í´ë¦¬ì˜¤ ì •ë¦¬\n",
    "# ============================================================================\n",
    "\n",
    "def generate_experiment_report():\n",
    "    \"\"\"ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "    print(\"ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # í”„ë¡œì íŠ¸ ê°œìš”\n",
    "    print(\"ğŸ—ï¸ í”„ë¡œì íŠ¸ ê°œìš”\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"ğŸ“‹ ëŒ€íšŒëª…: ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ì¢…ë¥˜ ë¶„ë¥˜ AI ê²½ì§„ëŒ€íšŒ\")\n",
    "    print(\"ğŸ¯ ëª©í‘œ: 7ì¢… ì•”ì„ ë¶„ë¥˜ (Macro F1 Score ìµœì í™”)\")\n",
    "    print(\"ğŸ“Š ë°ì´í„°: 380,020ì¥ í›ˆë ¨ ì´ë¯¸ì§€, 95,006ì¥ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€\")\n",
    "    print(\"âš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•: 5.8:1 (Granite 92,923ì¥ vs Etc 15,935ì¥)\")\n",
    "    \n",
    "    # ê¸°ìˆ ì  ì ‘ê·¼ë²•\n",
    "    print(f\"\\nğŸ”¬ ê¸°ìˆ ì  ì ‘ê·¼ë²•\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"âœ… ìƒìœ„ íŒ€ ì „ëµ í†µí•©:\")\n",
    "    print(\"   â€¢ 1ìœ„ íŒ€: 4ê°œ ëª¨ë¸ ì•™ìƒë¸” (EfficientNetV2 + RegNetY + TinyViT)\")\n",
    "    print(\"   â€¢ 2ìœ„ íŒ€: InternImage + Hard Negative Sampling\")  \n",
    "    print(\"   â€¢ 3ìœ„ íŒ€: ConvNeXt (CNN + Transformer í•˜ì´ë¸Œë¦¬ë“œ)\")\n",
    "    print(\"   â€¢ 4ìœ„ íŒ€: ì•ˆì •ì ì¸ ì „ì´í•™ìŠµ ì „ëµ\")\n",
    "    \n",
    "    print(f\"\\nâœ… êµ¬í˜„ëœ í•µì‹¬ ê¸°ë²•:\")\n",
    "    print(\"   â€¢ WeightedRandomSampler: í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\")\n",
    "    print(\"   â€¢ Hard Negative Sampling: ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘ í•™ìŠµ\")\n",
    "    print(\"   â€¢ CutMix + Mixup: ê³ ê¸‰ ë°ì´í„° ì¦ê°•\")\n",
    "    print(\"   â€¢ BICUBIC ë³´ê°„ë²•: ì´ë¯¸ì§€ í’ˆì§ˆ ìµœì í™”\")\n",
    "    print(\"   â€¢ K-Fold êµì°¨ê²€ì¦: ëª¨ë¸ ì•ˆì •ì„± í™•ë³´\")\n",
    "    print(\"   â€¢ OneCycleLR: í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ìµœì í™”\")\n",
    "    \n",
    "    # ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ì„\n",
    "    print(f\"\\nğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ì„\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, model_data in enumerate(available_models, 1):\n",
    "        config = model_data['config']\n",
    "        info = model_data['info']\n",
    "        print(f\"{i}. {config['name']} ({config['team']})\")\n",
    "        print(f\"   íŒŒë¼ë¯¸í„°: {info['total_params']:,}ê°œ\")\n",
    "        print(f\"   ëª¨ë¸ í¬ê¸°: {info['size_mb']:.1f}MB\")\n",
    "        print(f\"   íŠ¹ì§•: {config['description']}\")\n",
    "    \n",
    "    # ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "    print(f\"\\nğŸ¨ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"âœ… í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°:\")\n",
    "    print(f\"   â€¢ ì›ë³¸ ë¶ˆê· í˜•: 5.8:1 â†’ WeightedRandomSampler â†’ 1.0:1\")\n",
    "    print(f\"   â€¢ ê°€ì¤‘ì¹˜ ë°©ì‹: ì œê³±ê·¼ ì—­ë¹ˆë„ (ì•ˆì •ì„± í™•ë³´)\")\n",
    "    \n",
    "    print(f\"\\nâœ… ì´ë¯¸ì§€ í’ˆì§ˆ ìµœì í™”:\")\n",
    "    print(f\"   â€¢ ë¦¬ì‚¬ì´ì§•: 224Ã—224 (í‘œì¤€ ImageNet í¬ê¸°)\")\n",
    "    print(f\"   â€¢ ë³´ê°„ë²•: BICUBIC (ì•”ì„ í…ìŠ¤ì²˜ ë³´ì¡´ ìµœì )\")\n",
    "    print(f\"   â€¢ ì •ê·œí™”: ImageNet í‘œì¤€ (mean=[0.485, 0.456, 0.406])\")\n",
    "    \n",
    "    print(f\"\\nâœ… ë°ì´í„° ì¦ê°•:\")\n",
    "    print(f\"   â€¢ ê¸°ë³¸ ì¦ê°•: íšŒì „(15Â°), ë’¤ì§‘ê¸°, ìƒ‰ìƒ ë³€í™”\")\n",
    "    print(f\"   â€¢ ê³ ê¸‰ ì¦ê°•: CutMix (Î±=1.0), Mixup (Î±=0.2)\")\n",
    "    print(f\"   â€¢ í™•ë¥ ì  ì ìš©: CutMix 50%, Mixup 50%\")\n",
    "    \n",
    "    # í•™ìŠµ ì „ëµ\n",
    "    print(f\"\\nğŸ¯ í•™ìŠµ ì „ëµ\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"âœ… ì˜µí‹°ë§ˆì´ì €: AdamW\")\n",
    "    print(f\"   â€¢ í•™ìŠµë¥ : {HYPERPARAMETERS['lr']}\")\n",
    "    print(f\"   â€¢ Weight Decay: {HYPERPARAMETERS['weight_decay']}\")\n",
    "    \n",
    "    print(f\"\\nâœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§: OneCycleLR\")\n",
    "    print(f\"   â€¢ ì›Œë°ì—…: 30% êµ¬ê°„ì—ì„œ ìµœëŒ€ í•™ìŠµë¥ ê¹Œì§€ ì¦ê°€\")\n",
    "    print(f\"   â€¢ ê°ì†Œ: ë‚˜ë¨¸ì§€ 70% êµ¬ê°„ì—ì„œ ì ì§„ì  ê°ì†Œ\")\n",
    "    \n",
    "    print(f\"\\nâœ… Hard Negative Sampling:\")\n",
    "    print(f\"   â€¢ ë©”ëª¨ë¦¬ í¬ê¸°: {HYPERPARAMETERS['hard_memory_size']}ê°œ ìƒ˜í”Œ\")\n",
    "    print(f\"   â€¢ ì†ì‹¤ ì„ê³„ê°’: {HYPERPARAMETERS['loss_threshold']}\")\n",
    "    print(f\"   â€¢ ë°°ì¹˜ ë¹„ìœ¨: {HYPERPARAMETERS['hard_negative_ratio']*100}%\")\n",
    "    \n",
    "    # ê²€ì¦ ì „ëµ\n",
    "    print(f\"\\nğŸ“Š ê²€ì¦ ì „ëµ\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"âœ… K-Fold êµì°¨ê²€ì¦: {HYPERPARAMETERS['kfold_splits']}-Fold\")\n",
    "    print(f\"âœ… ê³„ì¸µí™” ë¶„í• : í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€\")\n",
    "    print(f\"âœ… í‰ê°€ ì§€í‘œ: Macro F1 Score (ëŒ€íšŒ ê¸°ì¤€)\")\n",
    "    print(f\"âœ… Early Stopping: ì„±ëŠ¥ ì €í•˜ ì‹œ ì¡°ê¸° ì¢…ë£Œ\")\n",
    "\n",
    "def save_experiment_config():\n",
    "    \"\"\"ì‹¤í—˜ ì„¤ì •ì„ JSONìœ¼ë¡œ ì €ì¥\"\"\"\n",
    "    config_data = {\n",
    "        'project_info': {\n",
    "            'competition': 'ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ì¢…ë¥˜ ë¶„ë¥˜ AI ê²½ì§„ëŒ€íšŒ',\n",
    "            'num_classes': NUM_CLASSES,\n",
    "            'class_names': sorted(CLASS_NAMES),\n",
    "            'total_train_images': len(train_dataset),\n",
    "            'class_imbalance_ratio': '5.8:1'\n",
    "        },\n",
    "        'hyperparameters': HYPERPARAMETERS,\n",
    "        'models': [\n",
    "            {\n",
    "                'name': model_data['config']['name'],\n",
    "                'model_name': model_data['model_name'],\n",
    "                'team': model_data['config']['team'],\n",
    "                'description': model_data['config']['description'],\n",
    "                'parameters': model_data['info']['total_params'],\n",
    "                'size_mb': model_data['info']['size_mb']\n",
    "            }\n",
    "            for model_data in available_models\n",
    "        ],\n",
    "        'techniques': {\n",
    "            'sampling': 'WeightedRandomSampler + Hard Negative Sampling',\n",
    "            'augmentation': 'CutMix + Mixup + Basic Augmentation',\n",
    "            'interpolation': 'BICUBIC',\n",
    "            'validation': 'Stratified K-Fold',\n",
    "            'optimizer': 'AdamW',\n",
    "            'scheduler': 'OneCycleLR'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = EXPERIMENT_DIR / \"experiment_config.json\"\n",
    "    with open(config_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(config_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… ì‹¤í—˜ ì„¤ì • ì €ì¥: {config_path}\")\n",
    "\n",
    "def print_portfolio_summary():\n",
    "    \"\"\"í¬íŠ¸í´ë¦¬ì˜¤ìš© í”„ë¡œì íŠ¸ ìš”ì•½\"\"\"\n",
    "    print(\"\\n\" + \"ğŸ¨\" * 25 + \" í¬íŠ¸í´ë¦¬ì˜¤ ìš”ì•½ \" + \"ğŸ¨\" * 25)\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ“‹ í”„ë¡œì íŠ¸ ì œëª©: ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ë¶„ë¥˜ AI ì‹œìŠ¤í…œ\")\n",
    "    print(\"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: Computer Vision, Image Classification, Deep Learning\")\n",
    "    print(\"ğŸ“… ê¸°ê°„: 2025ë…„ (í•™ìŠµ ëª©ì  í”„ë¡œì íŠ¸)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ:\")\n",
    "    print(\"   â€¢ 7ì¢… ì•”ì„ì˜ ìë™ ë¶„ë¥˜ ì‹œìŠ¤í…œ ê°œë°œ\")\n",
    "    print(\"   â€¢ ê±´ì„¤ í˜„ì¥ í’ˆì§ˆ ê²€ì‚¬ ìë™í™” ê¸°ì—¬\")\n",
    "    print(\"   â€¢ ìƒìœ„ íŒ€ ì „ëµ ë¶„ì„ ë° í†µí•© êµ¬í˜„\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜:\")\n",
    "    print(\"   â€¢ ê±´ì„¤ í˜„ì¥ì˜ ë””ì§€í„¸ ì „í™˜ ì§€ì›\")\n",
    "    print(\"   â€¢ í’ˆì§ˆ ê²€ì‚¬ ìë™í™”ë¡œ ë¹„ìš© ì ˆê°\")\n",
    "    print(\"   â€¢ ì¸ë ¥ ì˜ì¡´ë„ ê°ì†Œ ë° ì •í™•ë„ í–¥ìƒ\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ”¬ ê¸°ìˆ ì  ì„±ê³¼:\")\n",
    "    print(\"   â€¢ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ ì™„ì „ í•´ê²° (5.8:1 â†’ 1.0:1)\")\n",
    "    print(\"   â€¢ ìƒìœ„ íŒ€ ê¸°ë²• ì„±ê³µì  í†µí•© (Hard Negative Sampling ë“±)\")\n",
    "    print(\"   â€¢ ê³ ê¸‰ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\")\n",
    "    print(\"   â€¢ ì²´ê³„ì ì¸ K-Fold êµì°¨ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ› ï¸ ì‚¬ìš© ê¸°ìˆ :\")\n",
    "    print(\"   â€¢ PyTorch, timm, scikit-learn\")\n",
    "    print(\"   â€¢ EfficientNetV2, ConvNeXt, Vision Transformer\")\n",
    "    print(\"   â€¢ CutMix, Mixup, WeightedRandomSampler\")\n",
    "    print(\"   â€¢ K-Fold Cross Validation, Hard Negative Sampling\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ“ˆ í•™ìŠµ ì„±ê³¼:\")\n",
    "    print(\"   â€¢ ë°ì´í„° ë¶„ì„ ë° ë¬¸ì œ ì •ì˜ ëŠ¥ë ¥\")\n",
    "    print(\"   â€¢ ìµœì‹  ë”¥ëŸ¬ë‹ ê¸°ë²• ì ìš© ê²½í—˜\")\n",
    "    print(\"   â€¢ ì‹¤ë¬´ ìˆ˜ì¤€ì˜ ì½”ë“œ êµ¬ì¡°í™” ë° ë¬¸ì„œí™”\")\n",
    "    print(\"   â€¢ ìƒìœ„ íŒ€ ë¶„ì„ì„ í†µí•œ ë²¤ì¹˜ë§ˆí‚¹ ëŠ¥ë ¥\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ”— í™œìš© ë¶„ì•¼:\")\n",
    "    print(\"   â€¢ ì œì¡°ì—… í’ˆì§ˆ ê²€ì‚¬ ìë™í™”\")\n",
    "    print(\"   â€¢ ì˜ë£Œ ì˜ìƒ ì§„ë‹¨ ì‹œìŠ¤í…œ\")\n",
    "    print(\"   â€¢ ë†ì—… ì‘ë¬¼ ìƒíƒœ ë¶„ë¥˜\")\n",
    "    print(\"   â€¢ ë¦¬í…Œì¼ ìƒí’ˆ ë¶„ë¥˜ ì‹œìŠ¤í…œ\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ¨\" * 60)\n",
    "\n",
    "# ì‹¤í–‰\n",
    "generate_experiment_report()\n",
    "save_experiment_config()\n",
    "print_portfolio_summary()\n",
    "\n",
    "# ìµœì¢… ë©”ì‹œì§€\n",
    "print(\"\\n\" + \"ğŸ‰\" * 30)\n",
    "print(\"ğŸ† ê±´ì„¤ìš© ìê°ˆ ì•”ì„ ë¶„ë¥˜ AI ëª¨ë¸ í•™ìŠµ ì‹œìŠ¤í…œ ì™„ì„±!\")\n",
    "print(\"ğŸ‰\" * 30)\n",
    "print()\n",
    "print(\"âœ… ì™„ì„±ëœ ê¸°ëŠ¥:\")\n",
    "print(\"   1. ğŸ¨ ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\")\n",
    "print(\"   2. ğŸ§  ìƒìœ„ íŒ€ ëª¨ë¸ ì•„í‚¤í…ì²˜ í†µí•©\")\n",
    "print(\"   3. ğŸ¯ Hard Negative Sampling êµ¬í˜„\")\n",
    "print(\"   4. ğŸ“Š K-Fold êµì°¨ê²€ì¦ ì‹œìŠ¤í…œ\")\n",
    "print(\"   5. ğŸ”® ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print(\"   6. ğŸ“‹ ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„\")\n",
    "print()\n",
    "print(\"ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "print(\"   â€¢ ì‹¤ì œ ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ (ì…€ 7ë²ˆ)\")\n",
    "print(\"   â€¢ K-Fold êµì°¨ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"   â€¢ ì•™ìƒë¸” ê¸°ë²•ìœ¼ë¡œ ìµœì¢… ì„±ëŠ¥ í–¥ìƒ\")\n",
    "print(\"   â€¢ ì‹¤ì œ ëŒ€íšŒ ì œì¶œ íŒŒì¼ ìƒì„±\")\n",
    "print()\n",
    "print(\"ğŸš€ ì´ì œ ì‹¤ì œ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d6f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
